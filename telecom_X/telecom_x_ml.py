# -*- coding: utf-8 -*-
"""Telecom_X_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HpHi8aTw83ivYPo8MFqnWrj4_bFYRGwi

## 1. Importación de librerías y configuración

En esta sección se importan las librerías necesarias para el análisis de datos, visualización y manejo de advertencias. Además, se configura el estilo de los gráficos para asegurar una presentación visual profesional y se suprimen advertencias irrelevantes para mantener el notebook limpio.
"""

# Importación de librerías necesarias para el análisis de datos y visualización
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import requests

# Configuración de estilo para gráficos y supresión de advertencias irrelevantes
sns.set(style="whitegrid", palette="muted")
warnings.filterwarnings('ignore')

"""## 2. Carga, limpieza y transformación de los datos

Se cargan los datos desde una fuente externa, se normalizan y renombran las columnas para mayor claridad. Posteriormente, se transforman variables categóricas a formato numérico binario, se crean nuevas variables útiles para el análisis y se eliminan registros con valores nulos en las columnas clave. Este proceso garantiza que los datos estén listos para un análisis exploratorio robusto.
"""

# Carga de datos desde un archivo JSON alojado en GitHub
url = "https://raw.githubusercontent.com/alura-cursos/challenge2-data-science-LATAM/refs/heads/main/TelecomX_Data.json"
response = requests.get(url)
response.raise_for_status()
json_data = response.json()

# Normalización y conversión a DataFrame
df = pd.json_normalize(json_data, sep='_')

# Renombrado de columnas para mayor claridad y consistencia
def clean_column_names(col):
    for prefix in ['customer_', 'internet_', 'phone_', 'account_']:
        if col.startswith(prefix):
            return col.replace(prefix, '')
    return col

df.columns = [clean_column_names(col).lower() for col in df.columns]

# Conversión de variables categóricas a formato numérico binario
binary_columns = [
    'churn', 'partner', 'dependents', 'onlinesecurity', 'onlinebackup',
    'deviceprotection', 'techsupport', 'streamingtv', 'streamingmovies',
    'paperlessbilling', 'phoneservice', 'multiplelines'
]
for col in binary_columns:
    df[col] = df[col].replace({'Yes': 1, 'No': 0})

# Conversión de la columna 'charges_total' a numérico, manejando errores
df['charges_total'] = pd.to_numeric(df['charges_total'], errors='coerce')

# Creación de una nueva columna para el cargo diario promedio
df['charges_daily'] = df['charges_monthly'] / 30

# Eliminación de filas con valores nulos en 'charges_total' para análisis numéricos
df_cleaned = df.dropna(subset=['charges_total']).copy()

"""## 3. Análisis exploratorio: Distribución de Churn

En esta sección se visualiza la distribución de clientes que han abandonado la empresa (churn) frente a los que permanecen. Se utilizan gráficos de barras y de pastel para comprender la proporción y cantidad de clientes en cada grupo, lo que permite dimensionar el problema de la evasión.
"""

plt.figure(figsize=(6, 4))
df['churn'] = pd.to_numeric(df['churn'].replace({'Yes': 1, 'No': 0}), errors='coerce')
df.dropna(subset=['churn'], inplace=True)
churn_counts = df['churn'].value_counts().sort_index()
sns.barplot(x=churn_counts.index, y=churn_counts.values, palette=['#3498db', '#e74c3c'])
plt.title('Distribución de Churn (0: No, 1: Sí)')
plt.xlabel('Churn')
plt.ylabel('Número de Clientes')
plt.xticks([0, 1], ['No Churn', 'Churn'])
plt.show()

# Gráfico de pastel para proporción de churn
plt.figure(figsize=(6, 6))
plt.pie(churn_counts, labels=['No Churn', 'Churn'], autopct='%1.1f%%', startangle=90, colors=['#3498db', '#e74c3c'])
plt.title('Proporción de Clientes con y sin Churn')
plt.axis('equal')
plt.show()

"""## 4. Análisis exploratorio: Variables categóricas

Aquí se analiza la relación entre la evasión de clientes (churn) y diferentes variables categóricas, como género, tipo de contrato, servicios contratados y método de pago. Se emplean gráficos de barras segmentados para identificar patrones y diferencias en la tasa de churn según cada categoría.
"""

# Análisis de la relación entre churn y variables categóricas
categorical_cols = [
    'gender', 'partner', 'dependents', 'phoneservice', 'multiplelines',
    'internetservice', 'onlinesecurity', 'onlinebackup', 'deviceprotection',
    'techsupport', 'streamingtv', 'streamingmovies', 'contract',
    'paperlessbilling', 'paymentmethod'
]

for col in categorical_cols:
    plt.figure(figsize=(8, 5))
    sns.countplot(data=df, x=col, hue='churn', palette=['#3498db', '#e74c3c'])
    plt.title(f'Distribución de Churn por {col.capitalize()}')
    plt.xlabel(col.capitalize())
    plt.ylabel('Número de Clientes')
    plt.legend(title='Churn', labels=['No Churn', 'Churn'])
    plt.tight_layout()
    plt.show()

"""## 5. Análisis exploratorio: Variables numéricas

En esta sección se explora cómo las variables numéricas, como la antigüedad del cliente y los cargos mensuales o totales, se relacionan con la evasión. Se utilizan boxplots e histogramas para comparar la distribución de estas variables entre clientes que permanecen y los que han abandonado la empresa, identificando posibles factores de riesgo.
"""

# Análisis de la relación entre churn y variables numéricas
numeric_cols = ['tenure', 'charges_monthly', 'charges_daily', 'charges_total']

for col in numeric_cols:
    if col in df_cleaned.columns:
        # Este bloque de código ahora está indentado correctamente
        plt.figure(figsize=(8, 5))
        sns.boxplot(data=df_cleaned, x='churn', y=col, palette=['#3498db', '#e74c3c'])
        plt.title(f'Distribución de {col.replace("_", " ").capitalize()} por Churn')
        plt.xlabel('Churn (0: No, 1: Sí)')
        plt.ylabel(col.replace('_', ' ').capitalize())
        plt.xticks([0, 1], ['No Churn', 'Churn'])
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(8, 5))
        sns.histplot(data=df_cleaned, x=col, hue='churn', multiple='stack', kde=True, palette=['#3498db', '#e74c3c'])
        plt.title(f'Histograma de {col.replace("_", " ").capitalize()} por Churn')
        plt.xlabel(col.replace('_', ' ').capitalize())
        plt.ylabel('Frecuencia')
        plt.legend(title='Churn', labels=['No Churn', 'Churn'])
        plt.tight_layout()
        plt.show()

"""# 📄 Informe Final: Análisis de Evasión de Clientes (Churn) en TelecomX

## 1. Introducción

El objetivo de este análisis es comprender los factores que influyen en la evasión de clientes (churn) en la empresa TelecomX. El churn representa la pérdida de clientes, un problema crítico para las empresas de telecomunicaciones, ya que retener clientes existentes suele ser más rentable que adquirir nuevos. Identificar patrones y variables asociadas al churn permite diseñar estrategias efectivas para reducir la fuga de clientes y mejorar la rentabilidad.

## 2. Limpieza y Tratamiento de Datos

- **Importación:** Se importaron los datos desde un archivo JSON alojado en GitHub.
- **Normalización:** Se normalizaron los datos y se renombraron las columnas para mayor claridad.
- **Conversión de variables:** Se transformaron variables categóricas (como 'Yes'/'No') a valores binarios (1/0) para facilitar el análisis.
- **Creación de variables:** Se creó la variable `charges_daily` para analizar el gasto diario promedio.
- **Manejo de valores nulos:** Se identificaron y trataron valores nulos, especialmente en la columna `charges_total`, convirtiéndola a formato numérico y eliminando filas con datos faltantes para ciertos análisis.

## 3. Análisis Exploratorio de Datos

- **Distribución de Churn:** Se observó que aproximadamente el 26% de los clientes han abandonado la empresa.
- **Variables categóricas:** Se analizaron variables como género, tipo de contrato, servicios contratados, y métodos de pago. Se identificaron diferencias notables en la tasa de churn según el tipo de contrato y la presencia de servicios adicionales (seguridad en línea, soporte técnico, etc.).
- **Variables numéricas:** Se exploró la relación entre el churn y variables como la antigüedad del cliente (`tenure`) y los cargos mensuales/totales. Los clientes con menor antigüedad y cargos mensuales más altos tienden a presentar mayor churn.

## 4. Conclusiones e Insights

- **Antigüedad:** Los clientes con menor tiempo en la empresa son más propensos a abandonar el servicio.
- **Tipo de contrato:** Los contratos mensuales presentan una tasa de churn significativamente mayor que los contratos a largo plazo.
- **Servicios adicionales:** La ausencia de servicios como seguridad en línea, respaldo o soporte técnico está asociada a una mayor evasión.
- **Cargos mensuales:** Los clientes con cargos mensuales elevados tienden a abandonar más, posiblemente por percibir un menor valor por el costo.

## 5. Recomendaciones

- **Fomentar contratos a largo plazo:** Ofrecer incentivos para que los clientes opten por contratos anuales o bianuales puede reducir el churn.
- **Promocionar servicios adicionales:** Incluir servicios de seguridad, respaldo y soporte técnico en los paquetes básicos o a precios promocionales puede aumentar la retención.
- **Estrategias para nuevos clientes:** Implementar programas de bienvenida y seguimiento personalizado durante los primeros meses puede disminuir la fuga temprana.
- **Revisión de precios:** Analizar la estructura de precios para clientes con cargos mensuales altos y ofrecer descuentos o beneficios adicionales.
- **Alertas tempranas:** Desarrollar modelos predictivos para identificar clientes en riesgo y aplicar acciones preventivas.

# ADICIONALES

### 6. Análisis de correlación entre variables

En esta sección se exploran las correlaciones entre diferentes variables del dataset para identificar los factores que tienen mayor relación con la evasión de clientes (churn). Se analiza, por ejemplo, la relación entre el gasto diario y la evasión, así como el impacto de la cantidad de servicios contratados en la probabilidad de churn. Este análisis es fundamental para obtener insights que permitan construir modelos predictivos más robustos.
"""

# Cálculo de la matriz de correlación para las variables numéricas relevantes
df_cleaned['churn'] = pd.to_numeric(df_cleaned['churn'], errors='coerce')
df_cleaned.dropna(subset=['churn'], inplace=True)


correlation_matrix = df_cleaned[['churn', 'charges_daily', 'tenure', 'charges_monthly', 'charges_total']].corr()

# Visualización de la matriz de correlación
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de correlación entre variables numéricas')
plt.show()

# Análisis específico: relación entre el gasto diario y la evasión de clientes
plt.figure(figsize=(7, 5))
sns.boxplot(x='churn', y='charges_daily', data=df_cleaned, palette=['#3498db', '#e74c3c'])
plt.title('Relación entre el gasto diario y la evasión de clientes (churn)')
plt.xlabel('Churn (0: No, 1: Sí)')
plt.ylabel('Gasto diario promedio')
plt.xticks([0, 1], ['No Churn', 'Churn'])
plt.show()

# Análisis adicional: cantidad de servicios contratados y churn
# Se cuenta el número de servicios adicionales contratados por cada cliente
service_cols = [
    'onlinesecurity', 'onlinebackup', 'deviceprotection',
    'techsupport', 'streamingtv', 'streamingmovies'
]

for col in service_cols:
    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')

df_cleaned['num_services'] = df_cleaned[service_cols].sum(axis=1)

plt.figure(figsize=(7, 5))
sns.boxplot(x='churn', y='num_services', data=df_cleaned, palette=['#3498db', '#e74c3c'])
plt.title('Cantidad de servicios contratados vs. churn')
plt.xlabel('Churn (0: No, 1: Sí)')
plt.ylabel('Cantidad de servicios contratados')
plt.xticks([0, 1], ['No Churn', 'Churn'])
plt.show()

# Gráfico de dispersión: servicios contratados vs. gasto diario, coloreado por churn
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x='num_services', y='charges_daily', hue='churn',
    data=df_cleaned, palette=['#3498db', '#e74c3c'], alpha=0.6
)
plt.title('Servicios contratados vs. gasto diario, según churn')
plt.xlabel('Cantidad de servicios contratados')
plt.ylabel('Gasto diario promedio')
plt.legend(title='Churn', labels=['No Churn', 'Churn'])
plt.show()

"""**Conclusión:**  
Se observa que los clientes con menor cantidad de servicios contratados y mayor gasto diario presentan una mayor tasa de evasión. Además, la matriz de correlación permite identificar qué variables están más asociadas al churn, lo que es clave para futuros modelos predictivos.

# TELECOM PARTE 2

## Eliminacion de columnas irrelevantes para el modelo predictivo
"""

# Eliminar columnas que no aportan valor predictivo
columns_to_drop = [
    'customerid',  # Identificador único - no aporta valor predictivo
]

# Verificar qué columnas existen antes de eliminar
existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
print(f"Columnas que se eliminarán: {existing_columns_to_drop}")

# Eliminar las columnas
df = df.drop(columns=existing_columns_to_drop)

print(f"\nDataset después de eliminar columnas irrelevantes:")
print(f"Forma: {df.shape}")
print(f"Columnas restantes: {df.columns.tolist()}")

"""## Limpieza adiccional de datos"""

# Limpiar valores problemáticos en la columna churn (strings vacíos)
print(f"Valores únicos en churn antes de limpieza: {df['churn'].unique()}")
print(f"Tipos de datos en churn: {df['churn'].dtype}")

# Convertir strings vacíos a NaN y luego eliminar esas filas
df['churn'] = df['churn'].replace('', np.nan)
initial_rows = df.shape[0]
df = df.dropna(subset=['churn']).copy()
final_rows = df.shape[0]

print(f"\nValores únicos en churn después de limpieza: {df['churn'].unique()}")
print(f"Filas eliminadas: {initial_rows - final_rows}")
print(f"Dataset después de limpieza: {df.shape}")

# Verificar distribución final de churn
churn_distribution = df['churn'].value_counts()
print(f"\nDistribución final de Churn:")
print(f"No Churn (0): {churn_distribution[0]} ({churn_distribution[0]/len(df)*100:.1f}%)")
print(f"Churn (1): {churn_distribution[1]} ({churn_distribution[1]/len(df)*100:.1f}%)")

"""## Resumen del preprocesamiento para machine learning"""

print("="*60)
print("RESUMEN DEL PREPROCESAMIENTO PARA MACHINE LEARNING")
print("="*60)

print(f"\n📊 INFORMACIÓN DEL DATASET:")
print(f"   • Filas: {df.shape[0]:,}")
print(f"   • Columnas: {df.shape[1]}")
print(f"   • Target variable: 'churn'")

print(f"\n🗑️ COLUMNAS ELIMINADAS:")
print(f"   • customerid (identificador único sin valor predictivo)")

print(f"\n🧹 LIMPIEZA REALIZADA:")
print(f"   • Eliminados registros con valores vacíos en 'churn'")
print(f"   • Transformación Yes/No → 1/0 en variables categóricas")

print(f"\n📈 DISTRIBUCIÓN DE CHURN:")
churn_counts = df['churn'].value_counts()
for value, count in churn_counts.items():
    label = "No Churn" if value == 0 else "Churn"
    percentage = count/len(df)*100
    print(f"   • {label}: {count:,} ({percentage:.1f}%)")

print(f"\n📋 COLUMNAS FINALES:")
categorical_features = ['gender', 'internetservice', 'contract', 'paymentmethod']
binary_features = ['seniorcitizen', 'partner', 'dependents', 'phoneservice',
                  'multiplelines', 'onlinesecurity', 'onlinebackup',
                  'deviceprotection', 'techsupport', 'streamingtv',
                  'streamingmovies', 'paperlessbilling']
numerical_features = ['tenure', 'charges_monthly', 'charges_total', 'charges_daily']

print(f"   • Categóricas: {len(categorical_features)} - {categorical_features}")
print(f"   • Binarias: {len(binary_features)} - {binary_features}")
print(f"   • Numéricas: {len(numerical_features)} - {numerical_features}")
print(f"   • Target: churn")

print(f"\n✅ DATASET LISTO PARA MACHINE LEARNING")
print("="*60)

"""## Analisis de variables categoricas para one-hot encoding"""

# Función auxiliar para analizar variables categóricas
def analyze_categorical_variables(df):
    """Analiza y clasifica variables categóricas del dataset"""

    print("="*60)
    print("ANÁLISIS DE VARIABLES CATEGÓRICAS")
    print("="*60)

    categorical_vars = []
    binary_vars = []
    numerical_vars = []

    for col in df.columns:
        if col == 'churn':  # Skip target variable
            continue

        unique_count = df[col].nunique()
        unique_values = df[col].unique()

        print(f"\n📊 {col.upper()}:")
        print(f"   • Tipo: {df[col].dtype}")
        print(f"   • Valores únicos: {unique_count}")
        print(f"   • Valores: {list(unique_values)}")

        # Clasificar variable
        if df[col].dtype == 'object':
            categorical_vars.append(col)
            print(f"   • Clasificación: CATEGÓRICA (necesita One-Hot Encoding)")
        elif unique_count == 2 and set(unique_values).issubset({0, 1}):
            binary_vars.append(col)
            print(f"   • Clasificación: BINARIA (ya codificada)")
        else:
            numerical_vars.append(col)
            print(f"   • Clasificación: NUMÉRICA")

    print(f"\n{'='*60}")
    print("RESUMEN DE CLASIFICACIÓN:")
    print(f"   🔤 Variables categóricas (One-Hot): {len(categorical_vars)} - {categorical_vars}")
    print(f"   🔘 Variables binarias: {len(binary_vars)} - {binary_vars}")
    print(f"   🔢 Variables numéricas: {len(numerical_vars)} - {numerical_vars}")
    print(f"   🎯 Variable target: churn")
    print("="*60)

    return categorical_vars, binary_vars, numerical_vars

# Ejecutar análisis
categorical_vars, binary_vars, numerical_vars = analyze_categorical_variables(df)

"""## One-hot encoding de variables categoricas"""

def identify_categorical_variables(df):
    """Identifica variables categóricas que necesitan One-Hot Encoding"""

    print("="*60)
    print("ANÁLISIS DE VARIABLES CATEGÓRICAS REALES")
    print("="*60)

    categorical_vars = []
    binary_vars = []
    numerical_vars = []

    for col in df.columns:
        if col == 'churn':  # Skip target variable
            continue

        unique_count = df[col].nunique()
        unique_values = df[col].unique()

        print(f"\n📊 {col.upper()}:")
        print(f"   • Tipo: {df[col].dtype}")
        print(f"   • Valores únicos: {unique_count}")
        print(f"   • Valores: {list(unique_values)[:10]}")  # Mostrar máximo 10 valores

        # Clasificar variable
        if df[col].dtype == 'object' and unique_count > 2:
            categorical_vars.append(col)
            print(f"   • Clasificación: CATEGÓRICA (necesita One-Hot Encoding)")
        elif df[col].dtype == 'object' and unique_count == 2:
            # Variables de texto binarias que necesitan codificación manual
            categorical_vars.append(col)
            print(f"   • Clasificación: BINARIA TEXTO (necesita codificación)")
        elif unique_count == 2 and set(unique_values).issubset({0, 1}):
            binary_vars.append(col)
            print(f"   • Clasificación: BINARIA (ya codificada)")
        else:
            numerical_vars.append(col)
            print(f"   • Clasificación: NUMÉRICA")

    print(f"\n{'='*60}")
    print("RESUMEN DE CLASIFICACIÓN:")
    print(f"   🔤 Variables categóricas (One-Hot): {len(categorical_vars)} - {categorical_vars}")
    print(f"   🔘 Variables binarias: {len(binary_vars)} - {binary_vars}")
    print(f"   🔢 Variables numéricas: {len(numerical_vars)} - {numerical_vars}")
    print(f"   🎯 Variable target: churn")
    print("="*60)

    return categorical_vars, binary_vars, numerical_vars

# Ejecutar análisis
categorical_vars, binary_vars, numerical_vars = identify_categorical_variables(df)

"""## Verificacion del One-hot encoding"""

if categorical_vars:
    print(f"\n🔄 APLICANDO ONE-HOT ENCODING A: {categorical_vars}")

    # Hacer una copia del dataset original para comparación
    df_original = df.copy()

    # Aplicar One-Hot Encoding usando pandas get_dummies
    df_encoded = pd.get_dummies(df,
                               columns=categorical_vars,
                               drop_first=True,  # Evita multicolinealidad
                               dtype=int)        # Asegura que sean integers (0,1)

    print(f"✅ One-Hot Encoding completado!")
    print(f"   • Dataset original: {df_original.shape}")
    print(f"   • Dataset codificado: {df_encoded.shape}")
    print(f"   • Nuevas columnas creadas: {df_encoded.shape[1] - df_original.shape[1]}")

    # Mostrar las nuevas columnas creadas
    original_cols = set(df_original.columns)
    new_cols = set(df_encoded.columns)
    created_cols = sorted(list(new_cols - original_cols))

    print(f"\n📋 NUEVAS COLUMNAS CREADAS ({len(created_cols)}):")
    for i, col in enumerate(created_cols, 1):
        print(f"   {i:2d}. {col}")

    # Actualizar el DataFrame principal
    df = df_encoded.copy()

else:
    print("ℹ️ No se encontraron variables categóricas que requieran One-Hot Encoding")
    print("   Todas las variables ya están en formato numérico")

print(f"\n🎯 DATASET FINAL PREPARADO:")
print(f"   • Forma: {df.shape}")
print(f"   • Total de features: {df.shape[1] - 1} (excluyendo target)")

"""## Preparacion final del dataset para machine learning"""

# Convertir charges_total a numérico (por si no se hizo antes)
df['charges_total'] = pd.to_numeric(df['charges_total'], errors='coerce')

# Verificar que no hay variables categóricas sin codificar
remaining_categorical = []
for col in df.columns:
    if col != 'churn' and df[col].dtype == 'object':
        remaining_categorical.append(col)

if remaining_categorical:
    print(f"⚠️  Variables categóricas sin codificar: {remaining_categorical}")
else:
    print("✅ Todas las variables categóricas han sido codificadas correctamente")

# Separar features (X) y target (y)
X = df.drop('churn', axis=1)
y = df['churn']

print("\n🎯 DATASET FINAL PARA MACHINE LEARNING")
print("="*60)
print(f"📊 INFORMACIÓN DEL DATASET:")
print(f"   • Total de registros: {df.shape[0]:,}")
print(f"   • Total de features (X): {X.shape[1]}")
print(f"   • Variable target (y): churn")
print(f"   • Distribución de churn:")
print(f"     - No Churn (0): {(y == 0).sum():,} ({(y == 0).mean()*100:.1f}%)")
print(f"     - Churn (1): {(y == 1).sum():,} ({(y == 1).mean()*100:.1f}%)")

print(f"\n📋 TIPOS DE FEATURES:")
feature_summary = {
    'Numéricas originales': ['tenure', 'charges_monthly', 'charges_total', 'charges_daily'],
    'Binarias originales': [col for col in X.columns if col in binary_vars],
    'One-Hot Encoded': [col for col in X.columns if any(col.startswith(f"{cat}_") for cat in categorical_vars)]
}

for feature_type, cols in feature_summary.items():
    actual_cols = [col for col in cols if col in X.columns]
    print(f"   • {feature_type}: {len(actual_cols)} columnas")

print(f"\n📊 TIPOS DE DATOS EN EL DATASET FINAL:")
data_types = df.dtypes.value_counts()
for dtype, count in data_types.items():
    print(f"   • {dtype}: {count} columnas")

print(f"\n✅ DATASET COMPLETAMENTE PREPARADO PARA MACHINE LEARNING")
print("="*60)

# Mostrar primeras filas del dataset final (opcional)
print(f"\n🔍 MUESTRA DEL DATASET CODIFICADO:")
print(df.head())

"""## Analisis de balance de clases"""

def analyze_class_balance(df, target_column='churn'):
    """
    Analiza el balance de clases en el dataset para evaluar desbalance
    y proporcionar recomendaciones para el modelado
    """
    print("="*70)
    print("📊 ANÁLISIS DE BALANCE DE CLASES")
    print("="*70)

    # Conteo absoluto y proporciones usando value_counts()
    class_counts = df[target_column].value_counts().sort_index()
    class_proportions = df[target_column].value_counts(normalize=True).sort_index()

    print(f"\n📈 CONTEOS ABSOLUTOS:")
    print(f"   • No Churn (0): {class_counts[0]:,} clientes")
    print(f"   • Churn (1): {class_counts[1]:,} clientes")
    print(f"   • Total: {class_counts.sum():,} clientes")

    print(f"\n📊 PROPORCIONES:")
    print(f"   • No Churn (0): {class_proportions[0]:.3f} ({class_proportions[0]*100:.1f}%)")
    print(f"   • Churn (1): {class_proportions[1]:.3f} ({class_proportions[1]*100:.1f}%)")

    # Ratio de desbalance
    ratio = class_counts[0] / class_counts[1]
    print(f"\n⚖️ RATIO DE DESBALANCE:")
    print(f"   • No Churn : Churn = {ratio:.2f} : 1")
    print(f"   • Por cada cliente que cancela, {ratio:.1f} permanecen activos")

    # Evaluación del desbalance
    minority_percentage = min(class_proportions) * 100

    print(f"\n🎯 EVALUACIÓN DEL DESBALANCE:")
    if minority_percentage >= 40:
        balance_status = "BALANCEADO"
        impact = "BAJO"
        color = "🟢"
    elif minority_percentage >= 20:
        balance_status = "MODERADAMENTE DESBALANCEADO"
        impact = "MEDIO"
        color = "🟡"
    else:
        balance_status = "SEVERAMENTE DESBALANCEADO"
        impact = "ALTO"
        color = "🔴"

    print(f"   • Estado: {color} {balance_status}")
    print(f"   • Clase minoritaria: {minority_percentage:.1f}%")
    print(f"   • Impacto potencial en modelos: {impact}")

    # Recomendaciones específicas
    print(f"\n💡 RECOMENDACIONES PARA MODELADO:")
    if minority_percentage >= 40:
        print("   ✅ El dataset está bien balanceado")
        print("   ✅ Puedes usar métricas estándar (accuracy, precision, recall)")
        print("   ✅ No requiere técnicas especiales de balanceo")
    elif minority_percentage >= 20:
        print("   ⚠️ Considera usar técnicas de balanceo:")
        print("     - Ajustar pesos de clase (class_weight='balanced')")
        print("     - Usar métricas como F1-score, AUC-ROC")
        print("     - Validar con stratified sampling")
        print("     - Considerar threshold tuning")
    else:
        print("   🚨 Es crítico aplicar técnicas de balanceo:")
        print("     - SMOTE (Synthetic Minority Oversampling)")
        print("     - Random undersampling de clase mayoritaria")
        print("     - Ensemble methods (Random Forest con balanced weights)")
        print("     - Métricas: precision, recall, F1-score (NO accuracy)")

    print("="*70)
    return class_counts, class_proportions, ratio

# Ejecutar análisis de balance de clases
class_counts, class_proportions, balance_ratio = analyze_class_balance(df)

"""## Visualizacion del balance de clases"""

# Crear visualizaciones del balance de clases
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Gráfico de barras - Conteo absoluto
axes[0].bar(['No Churn', 'Churn'], class_counts.values,
           color=['skyblue', 'salmon'], alpha=0.8, edgecolor='black')
axes[0].set_title('Distribución Absoluta de Clases', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Número de Clientes')
axes[0].grid(axis='y', alpha=0.3)
for i, v in enumerate(class_counts.values):
    axes[0].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold', fontsize=12)

# Gráfico circular - Proporciones
axes[1].pie(class_counts.values, labels=['No Churn', 'Churn'],
           autopct='%1.1f%%', startangle=90,
           colors=['skyblue', 'salmon'],
           explode=(0.05, 0.05))  # Separar un poco las secciones
axes[1].set_title('Proporción de Clases', fontsize=14, fontweight='bold')

# Gráfico de ratio - Desbalance
ratio_data = [balance_ratio, 1]
bars = axes[2].bar(['No Churn', 'Churn'], ratio_data,
                   color=['skyblue', 'salmon'], alpha=0.8, edgecolor='black')
axes[2].set_title('Ratio de Desbalance', fontsize=14, fontweight='bold')
axes[2].set_ylabel('Ratio (relativo a Churn)')
axes[2].grid(axis='y', alpha=0.3)
axes[2].text(0, balance_ratio + 0.1, f'{balance_ratio:.1f}:1',
            ha='center', fontweight='bold', fontsize=12)
axes[2].text(1, 1 + 0.1, '1:1', ha='center', fontweight='bold', fontsize=12)

plt.tight_layout()
plt.show()

"""## Impacto en modelos de machine learning"""

print("🤖 IMPACTO DEL DESBALANCE EN MODELOS ML")
print("="*60)

# Análisis de impacto específico
minority_pct = min(class_proportions) * 100

print(f"\n📊 SITUACIÓN ACTUAL:")
print(f"   • Clase minoritaria (Churn): {minority_pct:.1f}%")
print(f"   • Ratio de desbalance: {balance_ratio:.1f}:1")

print(f"\n⚠️ RIESGOS POTENCIALES:")
print("   • Los modelos pueden tener sesgo hacia la clase mayoritaria")
print("   • Accuracy puede ser engañosa (un modelo que prediga siempre 'No Churn' tendría 73.5% accuracy)")
print("   • Menor sensibilidad para detectar churn (falsos negativos)")
print("   • Impacto en métricas de negocio (clientes perdidos no detectados)")

print(f"\n✅ ESTRATEGIAS RECOMENDADAS:")
print("   1. 🎯 MÉTRICAS APROPIADAS:")
print("      - Precision, Recall, F1-score para clase minoritaria")
print("      - AUC-ROC y AUC-PR (Precision-Recall)")
print("      - Confusion Matrix detallada")
print("      - Evitar accuracy como métrica principal")

print("\n   2. 🔧 TÉCNICAS DE BALANCEO:")
print("      - class_weight='balanced' en algoritmos")
print("      - SMOTE para oversampling inteligente")
print("      - Random undersampling de clase mayoritaria")
print("      - Ensemble methods con balanced weights")

print("\n   3. 📊 VALIDACIÓN:")
print("      - Stratified K-Fold Cross-Validation")
print("      - Validar en conjunto de prueba balanceado")
print("      - Análisis de curvas ROC y Precision-Recall")

print("\n   4. 🎛️ OPTIMIZACIÓN:")
print("      - Threshold tuning para optimizar recall")
print("      - Cost-sensitive learning")
print("      - Ensemble de modelos con diferentes enfoques")

print(f"\n💼 IMPACTO EN NEGOCIO:")
print("   • Costo de falsos negativos: perder clientes valiosos")
print("   • Costo de falsos positivos: campañas de retención innecesarias")
print("   • Recomendación: priorizar recall para detectar más churn")
print("="*60)

"""## Analisis de escalas de variables numericas"""

def analyze_numerical_scales(df):
    """
    Analiza las escalas de las variables numéricas para evaluar
    la necesidad de normalización/estandarización
    """
    print("="*80)
    print("📏 ANÁLISIS DE ESCALAS DE VARIABLES NUMÉRICAS")
    print("="*80)

    # Identificar variables numéricas (excluyendo binarias)
    numerical_vars = []
    for col in df.columns:
        if col == 'churn':  # Skip target
            continue
        if df[col].dtype in ['int64', 'float64'] and df[col].nunique() > 2:
            numerical_vars.append(col)

    print(f"\n🔢 VARIABLES NUMÉRICAS IDENTIFICADAS: {numerical_vars}")

    # Análisis detallado de cada variable
    scale_analysis = {}

    for var in numerical_vars:
        if var in df.columns:
            stats = {
                'min': df[var].min(),
                'max': df[var].max(),
                'mean': df[var].mean(),
                'std': df[var].std(),
                'range': df[var].max() - df[var].min()
            }
            scale_analysis[var] = stats

            print(f"\n📊 {var.upper()}:")
            print(f"   • Min: {stats['min']:,.2f}")
            print(f"   • Max: {stats['max']:,.2f}")
            print(f"   • Mean: {stats['mean']:,.2f}")
            print(f"   • Std: {stats['std']:,.2f}")
            print(f"   • Rango: {stats['range']:,.2f}")

    # Comparación de escalas
    print(f"\n⚖️ COMPARACIÓN DE ESCALAS:")
    ranges = [(var, data['range']) for var, data in scale_analysis.items()]
    ranges.sort(key=lambda x: x[1], reverse=True)

    for i, (var, range_val) in enumerate(ranges, 1):
        print(f"   {i}. {var}: {range_val:,.2f}")

    # Determinar necesidad de escalado
    max_range = max([data['range'] for data in scale_analysis.values()])
    min_range = min([data['range'] for data in scale_analysis.values()])
    scale_ratio = max_range / min_range if min_range > 0 else float('inf')

    print(f"\n🎯 EVALUACIÓN DE NECESIDAD DE ESCALADO:")
    print(f"   • Ratio entre escalas: {scale_ratio:,.1f}:1")

    if scale_ratio > 100:
        need_scaling = "CRÍTICA"
        color = "🔴"
    elif scale_ratio > 10:
        need_scaling = "ALTA"
        color = "🟡"
    else:
        need_scaling = "BAJA"
        color = "🟢"

    print(f"   • Necesidad de escalado: {color} {need_scaling}")

    print(f"\n📚 RECOMENDACIONES POR TIPO DE MODELO:")
    print("   🎯 MODELOS SENSIBLES A LA ESCALA (requieren normalización):")
    print("     - KNN (K-Nearest Neighbors)")
    print("     - SVM (Support Vector Machine)")
    print("     - Regresión Logística")
    print("     - Redes Neuronales")
    print("     - K-Means Clustering")

    print("\n   🌳 MODELOS NO SENSIBLES A LA ESCALA (no requieren normalización):")
    print("     - Decision Trees")
    print("     - Random Forest")
    print("     - XGBoost/LightGBM")
    print("     - Gradient Boosting")

    print("="*80)
    return numerical_vars, scale_analysis

# Ejecutar análisis de escalas
numerical_vars, scale_analysis = analyze_numerical_scales(df)

"""## Implementacion de metodos de normalizacion/estandarizacion"""

# =============================================================================
# IMPLEMENTACIÓN DE MÉTODOS DE NORMALIZACIÓN/ESTANDARIZACIÓN
# =============================================================================

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split

def implement_scaling_methods(df, numerical_vars):
    """
    Implementa diferentes métodos de escalado para variables numéricas
    """
    print("🔧 IMPLEMENTACIÓN DE MÉTODOS DE ESCALADO")
    print("="*60)

    # Separar features y target
    X = df.drop('churn', axis=1)
    y = df['churn']

    # División inicial train/test (estratificada para balance de clases)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"📊 DIVISIÓN DEL DATASET:")
    print(f"   • Train: {X_train.shape[0]:,} muestras")
    print(f"   • Test: {X_test.shape[0]:,} muestras")
    print(f"   • Features: {X_train.shape[1]}")

    # Crear diferentes escaladores
    scalers = {
        'StandardScaler': StandardScaler(),      # Media=0, Std=1 (Z-score)
        'MinMaxScaler': MinMaxScaler(),          # Rango [0,1]
        'RobustScaler': RobustScaler()           # Mediana y IQR (robusto a outliers)
    }

    # Aplicar cada método de escalado
    scaled_datasets = {}

    for scaler_name, scaler in scalers.items():
        print(f"\n🔧 APLICANDO {scaler_name.upper()}:")

        # Crear copias de los datasets
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()

        # IMPORTANTE: Fit solo en train, transform en ambos
        X_train_scaled[numerical_vars] = scaler.fit_transform(X_train[numerical_vars])
        X_test_scaled[numerical_vars] = scaler.transform(X_test[numerical_vars])

        # Guardar datasets escalados
        scaled_datasets[scaler_name] = {
            'scaler': scaler,
            'X_train': X_train_scaled,
            'X_test': X_test_scaled,
            'y_train': y_train,
            'y_test': y_test
        }

        print(f"   ✅ {scaler_name} aplicado correctamente")

        # Mostrar estadísticas post-escalado para una variable ejemplo
        if numerical_vars:
            example_var = numerical_vars[0]
            original_stats = X_train[example_var].describe()
            scaled_stats = X_train_scaled[example_var].describe()

            print(f"   📊 Ejemplo con '{example_var}':")
            print(f"      Original - Min: {original_stats['min']:.2f}, Max: {original_stats['max']:.2f}")
            print(f"      Escalado - Min: {scaled_stats['min']:.2f}, Max: {scaled_stats['max']:.2f}")

    # Dataset sin escalado (para modelos de árboles)
    scaled_datasets['NoScaling'] = {
        'scaler': None,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test
    }

    print(f"\n✅ TODOS LOS MÉTODOS DE ESCALADO IMPLEMENTADOS")
    print(f"   📦 Datasets disponibles: {list(scaled_datasets.keys())}")

    return scaled_datasets

# Implementar métodos de escalado
scaled_datasets = implement_scaling_methods(df, numerical_vars)

"""## Comparacion visual de metodos de escalado"""

# Visualizar el efecto de los diferentes métodos de escalado
if numerical_vars:
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.ravel()

    # Seleccionar hasta 4 variables numéricas para visualizar
    vars_to_plot = numerical_vars[:4] if len(numerical_vars) >= 4 else numerical_vars

    for i, var in enumerate(vars_to_plot):
        if i < 4:
            # Datos de cada método de escalado
            original_data = scaled_datasets['NoScaling']['X_train'][var]
            standard_data = scaled_datasets['StandardScaler']['X_train'][var]
            minmax_data = scaled_datasets['MinMaxScaler']['X_train'][var]
            robust_data = scaled_datasets['RobustScaler']['X_train'][var]

            # Crear boxplot comparativo
            box_data = [original_data, standard_data, minmax_data, robust_data]
            labels = ['Original', 'Standard', 'MinMax', 'Robust']

            axes[i].boxplot(box_data, tick_labels=labels)  # Usar tick_labels en lugar de labels
            axes[i].set_title(f'Comparación de Escalado: {var}', fontweight='bold')
            axes[i].set_ylabel('Valores')
            axes[i].grid(True, alpha=0.3)
            axes[i].tick_params(axis='x', rotation=45)

    # Ocultar subplots no utilizados
    for i in range(len(vars_to_plot), 4):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.show()

"""## Guia seleccion de metodo de escalado segun modelo"""

# =============================================================================
# GUÍA DE SELECCIÓN DE MÉTODO DE ESCALADO SEGÚN EL MODELO
# =============================================================================

print("🎯 GUÍA DE SELECCIÓN DE ESCALADO POR MODELO")
print("="*70)

scaling_recommendations = {
    "🔴 MODELOS QUE REQUIEREN ESCALADO": {
        "KNN (K-Nearest Neighbors)": {
            "escalado_recomendado": "StandardScaler o MinMaxScaler",
            "razon": "Usa distancia euclidiana - sensible a escala"
        },
        "SVM (Support Vector Machine)": {
            "escalado_recomendado": "StandardScaler",
            "razon": "Encuentra hiperplanos óptimos - requiere escalas similares"
        },
        "Regresión Logística": {
            "escalado_recomendado": "StandardScaler",
            "razon": "Optimización por gradiente - converge mejor con escalas similares"
        },
        "Redes Neuronales": {
            "escalado_recomendado": "StandardScaler o MinMaxScaler",
            "razon": "Funciones de activación sensibles a la escala de entrada"
        },
        "K-Means Clustering": {
            "escalado_recomendado": "StandardScaler",
            "razon": "Basado en distancias - variables con mayor escala dominan"
        }
    },

    "🟢 MODELOS QUE NO REQUIEREN ESCALADO": {
        "Decision Trees": {
            "escalado_recomendado": "NoScaling",
            "razon": "Divisiones basadas en umbrales - escala no importa"
        },
        "Random Forest": {
            "escalado_recomendado": "NoScaling",
            "razon": "Ensemble de árboles - hereda propiedades de Decision Trees"
        },
        "XGBoost/LightGBM": {
            "escalado_recomendado": "NoScaling",
            "razon": "Gradient boosting de árboles - no sensible a escala"
        },
        "Gradient Boosting": {
            "escalado_recomendado": "NoScaling",
            "razon": "Basado en árboles - escalado no mejora performance"
        }
    }
}

for category, models in scaling_recommendations.items():
    print(f"\n{category}:")
    for model, info in models.items():
        print(f"   📊 {model}:")
        print(f"      • Escalado recomendado: {info['escalado_recomendado']}")
        print(f"      • Razón: {info['razon']}")

print(f"\n💡 MÉTODOS DE ESCALADO DISPONIBLES:")
print("   🔧 StandardScaler (Z-score normalization):")
print("      • Transforma a media=0, std=1")
print("      • Mejor para distribuciones normales")
print("      • Preserva la forma de la distribución")

print("\n   🔧 MinMaxScaler (Min-Max normalization):")
print("      • Transforma al rango [0,1]")
print("      • Mejor cuando conoces los límites de las variables")
print("      • Sensible a outliers")

print("\n   🔧 RobustScaler (Robust normalization):")
print("      • Usa mediana y rango intercuartílico")
print("      • Robusto a outliers")
print("      • Mejor cuando hay valores atípicos")

print(f"\n🎯 RECOMENDACIÓN PARA ESTE DATASET:")
print("   • Ratio de escalas: 2,586.9:1 (CRÍTICO)")
print("   • charges_total domina completamente (rango: 8,666)")
print("   • Recomendado: StandardScaler para la mayoría de modelos")
print("   • Alternativa: RobustScaler si hay muchos outliers en charges_total")
print("="*70)

"""## Analisis de matriz de correlacion"""

def analyze_correlation_matrix(df, scaled_datasets=None):
    """
    Analiza las correlaciones entre variables, especialmente con la variable target
    """
    print("="*80)
    print("🔗 ANÁLISIS DE MATRIZ DE CORRELACIÓN")
    print("="*80)

    # Calcular matriz de correlación completa
    correlation_matrix = df.corr()

    # Correlaciones con la variable target
    target_correlations = correlation_matrix['churn'].drop('churn').sort_values(key=abs, ascending=False)

    print(f"\n🎯 CORRELACIONES CON LA VARIABLE TARGET (CHURN):")
    print("-" * 60)

    for i, (feature, corr) in enumerate(target_correlations.head(10).items(), 1):
        direction = "📈 Positiva" if corr > 0 else "📉 Negativa"
        strength = "🔴 Fuerte" if abs(corr) > 0.3 else "🟡 Moderada" if abs(corr) > 0.1 else "🟢 Débil"
        print(f"   {i:2d}. {feature:25} | {corr:6.3f} | {direction} | {strength}")

    # Identificar correlaciones fuertes entre features (multicolinealidad)
    print(f"\n⚠️ ANÁLISIS DE MULTICOLINEALIDAD:")
    print("-" * 60)

    # Crear matriz sin la variable target
    feature_corr = correlation_matrix.drop('churn', axis=0).drop('churn', axis=1)

    # Encontrar pares con correlación alta
    high_corr_pairs = []
    for i in range(len(feature_corr.columns)):
        for j in range(i+1, len(feature_corr.columns)):
            corr_val = feature_corr.iloc[i, j]
            if abs(corr_val) > 0.7:  # Umbral para correlación alta
                high_corr_pairs.append((feature_corr.columns[i], feature_corr.columns[j], corr_val))

    if high_corr_pairs:
        print("   📊 CORRELACIONES ALTAS ENTRE FEATURES (|r| > 0.7):")
        important_pairs = [pair for pair in high_corr_pairs if abs(pair[2]) < 1.0][:10]  # Top 10 excluding perfect correlations
        for feat1, feat2, corr in sorted(important_pairs, key=lambda x: abs(x[2]), reverse=True):
            print(f"      • {feat1} ↔ {feat2}: {corr:.3f}")
        if len(high_corr_pairs) > 10:
            print(f"      ... y {len(high_corr_pairs) - 10} pares más")
    else:
        print("   ✅ No se detectaron correlaciones altas entre features (|r| > 0.7)")

    # Recomendaciones para feature selection
    print(f"\n💡 RECOMENDACIONES PARA SELECCIÓN DE FEATURES:")

    strong_features = target_correlations[abs(target_correlations) > 0.2]
    moderate_features = target_correlations[(abs(target_correlations) > 0.1) & (abs(target_correlations) <= 0.2)]
    weak_features = target_correlations[abs(target_correlations) <= 0.1]

    print(f"   🔴 FEATURES FUERTES (|r| > 0.2): {len(strong_features)}")
    for feat in strong_features.head(5).index:
        print(f"      • {feat} ({target_correlations[feat]:.3f})")

    print(f"\n   🟡 FEATURES MODERADAS (0.1 < |r| ≤ 0.2): {len(moderate_features)}")
    for feat in moderate_features.head(3).index:
        print(f"      • {feat} ({target_correlations[feat]:.3f})")

    print(f"\n   🟢 FEATURES DÉBILES (|r| ≤ 0.1): {len(weak_features)}")
    print(f"      • Considerar eliminar para simplificar modelo")

    print("="*80)
    return correlation_matrix, target_correlations, high_corr_pairs

# Ejecutar análisis de correlación
correlation_matrix, target_correlations, high_corr_pairs = analyze_correlation_matrix(df)

"""## Visualizacion de matriz de correlacion"""

# Crear visualizaciones de la matriz de correlación
fig, axes = plt.subplots(2, 2, figsize=(20, 16))

# 1. Matriz de correlación completa
plt.subplot(2, 2, 1)
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Máscara para mostrar solo la mitad
sns.heatmap(correlation_matrix,
           mask=mask,
           annot=False,
           cmap='RdBu_r',
           center=0,
           square=True,
           fmt='.2f',
           cbar_kws={"shrink": 0.8})
plt.title('Matriz de Correlación Completa', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

# 2. Correlaciones con variable target
plt.subplot(2, 2, 2)
target_corr_sorted = target_correlations.head(15)  # Top 15
colors = ['red' if x < 0 else 'blue' for x in target_corr_sorted.values]
bars = plt.barh(range(len(target_corr_sorted)), target_corr_sorted.values, color=colors, alpha=0.7)
plt.yticks(range(len(target_corr_sorted)), target_corr_sorted.index)
plt.xlabel('Correlación con Churn')
plt.title('Top 15 Correlaciones con Churn', fontsize=14, fontweight='bold')
plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
plt.grid(axis='x', alpha=0.3)

# Agregar valores en las barras
for i, (bar, val) in enumerate(zip(bars, target_corr_sorted.values)):
    plt.text(val + (0.01 if val > 0 else -0.01), i, f'{val:.3f}',
             ha='left' if val > 0 else 'right', va='center', fontweight='bold')

# 3. Heatmap enfocado en variables numéricas
plt.subplot(2, 2, 3)
numerical_vars_with_target = ['churn'] + numerical_vars
if all(var in df.columns for var in numerical_vars_with_target):
    numerical_corr = df[numerical_vars_with_target].corr()
    sns.heatmap(numerical_corr,
               annot=True,
               cmap='RdBu_r',
               center=0,
               square=True,
               fmt='.3f',
               cbar_kws={"shrink": 0.8})
    plt.title('Correlación: Variables Numéricas vs Churn', fontsize=14, fontweight='bold')

# 4. Distribución de correlaciones
plt.subplot(2, 2, 4)
all_correlations = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, 1)]
plt.hist(all_correlations, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Sin correlación')
plt.axvline(x=0.3, color='orange', linestyle='--', alpha=0.7, label='Correlación moderada')
plt.axvline(x=-0.3, color='orange', linestyle='--', alpha=0.7)
plt.xlabel('Valor de Correlación')
plt.ylabel('Frecuencia')
plt.title('Distribución de Correlaciones', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

"""## Analisis detallado de features mas importantes"""

def analyze_top_features(target_correlations, correlation_matrix, df):
    """
    Analiza en detalle las features más correlacionadas con churn
    """
    print("🏆 ANÁLISIS DETALLADO DE FEATURES MÁS IMPORTANTES")
    print("="*70)

    # Top 5 features por correlación absoluta
    top_features = target_correlations.head(5)

    for i, (feature, corr) in enumerate(top_features.items(), 1):
        print(f"\n{i}. 📊 {feature.upper()}:")
        print(f"   • Correlación con churn: {corr:.4f}")

        # Estadísticas descriptivas por grupo de churn
        feature_stats = df.groupby('churn')[feature].agg(['count', 'mean', 'std', 'min', 'max'])

        print(f"   • Estadísticas por grupo:")
        print(f"     - No Churn (0): Media={feature_stats.loc[0, 'mean']:.2f}, Std={feature_stats.loc[0, 'std']:.2f}")
        print(f"     - Churn (1): Media={feature_stats.loc[1, 'mean']:.2f}, Std={feature_stats.loc[1, 'std']:.2f}")

        # Diferencia de medias
        mean_diff = feature_stats.loc[1, 'mean'] - feature_stats.loc[0, 'mean']
        print(f"     - Diferencia de medias: {mean_diff:.2f}")

        # Interpretación del negocio
        if feature in numerical_vars:
            if corr > 0:
                interpretation = f"Mayor {feature} → Mayor probabilidad de churn"
            else:
                interpretation = f"Mayor {feature} → Menor probabilidad de churn"
        else:
            if corr > 0:
                interpretation = f"Presencia de {feature} → Mayor probabilidad de churn"
            else:
                interpretation = f"Presencia de {feature} → Menor probabilidad de churn"

        print(f"   • Interpretación: {interpretation}")

    # Crear gráfico de distribución para variables top
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()

    for i, (feature, corr) in enumerate(top_features.head(6).items()):
        if i < 6:
            if feature in numerical_vars:
                # Boxplot para variables numéricas
                churn_groups = [df[df['churn'] == 0][feature], df[df['churn'] == 1][feature]]
                axes[i].boxplot(churn_groups, tick_labels=['No Churn', 'Churn'])
                axes[i].set_ylabel(feature)
            else:
                # Barplot para variables categóricas
                cross_tab = pd.crosstab(df[feature], df['churn'], normalize='columns') * 100
                cross_tab.plot(kind='bar', ax=axes[i], color=['skyblue', 'salmon'])
                axes[i].set_ylabel('Porcentaje (%)')
                axes[i].legend(['No Churn', 'Churn'])
                axes[i].tick_params(axis='x', rotation=45)

            axes[i].set_title(f'{feature}\n(Correlación: {corr:.3f})', fontweight='bold')
            axes[i].grid(True, alpha=0.3)

    # Ocultar subplots no utilizados
    for i in range(len(top_features.head(6)), 6):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.show()

    print("="*70)

# Ejecutar análisis detallado
analyze_top_features(target_correlations, correlation_matrix, df)

"""## Recomendaciones para seleccion de features"""

print("📋 RECOMENDACIONES FINALES PARA FEATURE SELECTION")
print("="*70)

# Clasificar features por fuerza de correlación
strong_features = target_correlations[abs(target_correlations) > 0.2]
moderate_features = target_correlations[(abs(target_correlations) > 0.1) & (abs(target_correlations) <= 0.2)]
weak_features = target_correlations[abs(target_correlations) <= 0.1]

print(f"\n🔴 FEATURES IMPRESCINDIBLES (Correlación fuerte |r| > 0.2):")
for i, (feat, corr) in enumerate(strong_features.items(), 1):
    direction = "↗️" if corr > 0 else "↘️"
    print(f"   {i:2d}. {feat:30} | {corr:6.3f} | {direction}")

print(f"\n🟡 FEATURES IMPORTANTES (Correlación moderada 0.1 < |r| ≤ 0.2):")
for i, (feat, corr) in enumerate(moderate_features.items(), 1):
    direction = "↗️" if corr > 0 else "↘️"
    print(f"   {i:2d}. {feat:30} | {corr:6.3f} | {direction}")

print(f"\n🟢 FEATURES OPCIONALES (Correlación débil |r| ≤ 0.1): {len(weak_features)}")
print("   • Pueden eliminarse para simplificar el modelo sin perder mucha información")

# Problemas de multicolinealidad
print(f"\n⚠️ PROBLEMAS DE MULTICOLINEALIDAD DETECTADOS:")
print(f"   • {len(high_corr_pairs)} pares de variables con correlación > 0.7")
print("   • Variables 'No internet service' son perfectamente correlacionadas")
print("   • charges_monthly y charges_daily son idénticas (r=1.0)")
print("   • tenure y charges_total tienen alta correlación (r=0.826)")

print(f"\n💡 ESTRATEGIAS RECOMENDADAS:")
print("   1. 🎯 FEATURE SELECTION:")
print("      • Incluir todas las features fuertes (11 variables)")
print("      • Evaluar features moderadas según performance del modelo")
print("      • Considerar eliminar features débiles")

print("\n   2. 🔧 MANEJO DE MULTICOLINEALIDAD:")
print("      • Eliminar charges_daily (idéntica a charges_monthly)")
print("      • Mantener solo una variable 'No internet service' representativa")
print("      • Considerar PCA para variables altamente correlacionadas")

print("\n   3. 📊 INTERPRETACIÓN DE NEGOCIO:")
print("      • PREDICTORES POSITIVOS DE CHURN:")
print("        - Fiber optic internet service")
print("        - Electronic check payment method")
print("        - Charges mensuales altos")
print("      • PREDICTORES NEGATIVOS DE CHURN:")
print("        - Mayor tenure (antigüedad)")
print("        - Contratos de Two year")
print("        - No tener internet service")

print("="*70)

"""## 6. Análisis Específico: Tiempo de Contrato y Gasto Total vs Cancelación

En esta sección se profundiza en la investigación de las relaciones específicas entre:
1. **Tiempo de contrato (tenure)** × Cancelación
2. **Gasto total (charges_total)** × Cancelación  
3. **Patrones de comportamiento** relacionados con el churn

Este análisis utiliza visualizaciones avanzadas como boxplots, scatter plots y gráficos de barras segmentados para identificar tendencias críticas y generar recomendaciones de negocio específicas.
"""

# Importaciones adicionales para el análisis específico
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

def analyze_contract_time_vs_churn(df):
    """
    Análisis detallado de la relación entre tiempo de contrato (tenure) y churn
    """
    print("="*80)
    print("⏰ ANÁLISIS: TIEMPO DE CONTRATO vs CANCELACIÓN")
    print("="*80)

    # Estadísticas descriptivas por grupo
    tenure_stats = df.groupby('churn')['tenure'].agg([
        'count', 'mean', 'median', 'std', 'min', 'max',
        lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)
    ]).round(2)
    tenure_stats.columns = ['Count', 'Mean', 'Median', 'Std', 'Min', 'Max', 'Q1', 'Q3']

    print("\n📊 ESTADÍSTICAS DE TENURE POR GRUPO:")
    print("-" * 60)
    print("Grupo Churn:")
    print(f"   • No Churn (0): Media = {tenure_stats.loc[0, 'Mean']:.1f} meses")
    print(f"                   Mediana = {tenure_stats.loc[0, 'Median']:.1f} meses")
    print(f"                   Std = {tenure_stats.loc[0, 'Std']:.1f} meses")
    print(f"                   Rango = {tenure_stats.loc[0, 'Min']:.0f} - {tenure_stats.loc[0, 'Max']:.0f} meses")

    print(f"\n   • Churn (1):    Media = {tenure_stats.loc[1, 'Mean']:.1f} meses")
    print(f"                   Mediana = {tenure_stats.loc[1, 'Median']:.1f} meses")
    print(f"                   Std = {tenure_stats.loc[1, 'Std']:.1f} meses")
    print(f"                   Rango = {tenure_stats.loc[1, 'Min']:.0f} - {tenure_stats.loc[1, 'Max']:.0f} meses")

    # Diferencias significativas
    mean_diff = tenure_stats.loc[1, 'Mean'] - tenure_stats.loc[0, 'Mean']
    median_diff = tenure_stats.loc[1, 'Median'] - tenure_stats.loc[0, 'Median']

    print(f"\n📈 DIFERENCIAS:")
    print(f"   • Diferencia de medias: {mean_diff:.1f} meses")
    print(f"   • Diferencia de medianas: {median_diff:.1f} meses")
    print(f"   • Los clientes que cancelan tienen {'MENOR' if mean_diff < 0 else 'MAYOR'} tiempo de contrato promedio")

    # Test estadístico
    no_churn_tenure = df[df['churn'] == 0]['tenure']
    churn_tenure = df[df['churn'] == 1]['tenure']
    t_stat, p_value = stats.ttest_ind(no_churn_tenure, churn_tenure)

    print(f"\n🔬 TEST ESTADÍSTICO (t-test):")
    print(f"   • t-statistic: {t_stat:.4f}")
    print(f"   • p-value: {p_value:.6f}")
    print(f"   • Significativo: {'SÍ' if p_value < 0.05 else 'NO'} (α = 0.05)")

    # Segmentación por períodos de tenure
    print(f"\n🎯 ANÁLISIS POR SEGMENTOS DE TENURE:")
    print("-" * 60)

    # Crear segmentos
    df['tenure_segment'] = pd.cut(df['tenure'],
                                 bins=[0, 12, 24, 36, 48, float('inf')],
                                 labels=['0-12 meses', '13-24 meses', '25-36 meses', '37-48 meses', '48+ meses'],
                                 include_lowest=True)

    # Calcular tasa de churn por segmento
    churn_by_segment = df.groupby('tenure_segment').agg({
        'churn': ['count', 'sum', 'mean']
    }).round(3)
    churn_by_segment.columns = ['Total_Clients', 'Churned_Clients', 'Churn_Rate']

    for segment in churn_by_segment.index:
        total = churn_by_segment.loc[segment, 'Total_Clients']
        churned = churn_by_segment.loc[segment, 'Churned_Clients']
        rate = churn_by_segment.loc[segment, 'Churn_Rate']
        print(f"   • {segment:12}: {churned:4.0f}/{total:4.0f} clientes ({rate*100:5.1f}% churn)")

    # Correlación con churn
    correlation = df['tenure'].corr(df['churn'])
    print(f"\n🔗 CORRELACIÓN:")
    print(f"   • Correlación tenure-churn: {correlation:.4f}")

    strength = "Fuerte" if abs(correlation) > 0.3 else "Moderada" if abs(correlation) > 0.1 else "Débil"
    direction = "negativa" if correlation < 0 else "positiva"
    print(f"   • Interpretación: Correlación {strength} {direction}")

    print("="*80)
    return tenure_stats, churn_by_segment

# Ejecutar análisis de tiempo de contrato
tenure_stats, churn_by_segment = analyze_contract_time_vs_churn(df_cleaned)

def analyze_total_charges_vs_churn(df):
    """
    Análisis detallado de la relación entre gasto total y churn
    """
    print("="*80)
    print("💰 ANÁLISIS: GASTO TOTAL vs CANCELACIÓN")
    print("="*80)

    # Estadísticas descriptivas por grupo
    charges_stats = df.groupby('churn')['charges_total'].agg([
        'count', 'mean', 'median', 'std', 'min', 'max',
        lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)
    ]).round(2)
    charges_stats.columns = ['Count', 'Mean', 'Median', 'Std', 'Min', 'Max', 'Q1', 'Q3']

    print("\n📊 ESTADÍSTICAS DE CHARGES_TOTAL POR GRUPO:")
    print("-" * 60)
    print("Grupo Churn:")
    print(f"   • No Churn (0): Media = ${charges_stats.loc[0, 'Mean']:,.2f}")
    print(f"                   Mediana = ${charges_stats.loc[0, 'Median']:,.2f}")
    print(f"                   Std = ${charges_stats.loc[0, 'Std']:,.2f}")
    print(f"                   Rango = ${charges_stats.loc[0, 'Min']:,.2f} - ${charges_stats.loc[0, 'Max']:,.2f}")

    print(f"\n   • Churn (1):    Media = ${charges_stats.loc[1, 'Mean']:,.2f}")
    print(f"                   Mediana = ${charges_stats.loc[1, 'Median']:,.2f}")
    print(f"                   Std = ${charges_stats.loc[1, 'Std']:,.2f}")
    print(f"                   Rango = ${charges_stats.loc[1, 'Min']:,.2f} - ${charges_stats.loc[1, 'Max']:,.2f}")

    # Diferencias significativas
    mean_diff = charges_stats.loc[1, 'Mean'] - charges_stats.loc[0, 'Mean']
    median_diff = charges_stats.loc[1, 'Median'] - charges_stats.loc[0, 'Median']

    print(f"\n📈 DIFERENCIAS:")
    print(f"   • Diferencia de medias: ${mean_diff:,.2f}")
    print(f"   • Diferencia de medianas: ${median_diff:,.2f}")
    print(f"   • Los clientes que cancelan gastan {'MENOS' if mean_diff < 0 else 'MÁS'} en promedio")

    # Test estadístico
    no_churn_charges = df[df['churn'] == 0]['charges_total']
    churn_charges = df[df['churn'] == 1]['charges_total']
    t_stat, p_value = stats.ttest_ind(no_churn_charges, churn_charges)

    print(f"\n🔬 TEST ESTADÍSTICO (t-test):")
    print(f"   • t-statistic: {t_stat:.4f}")
    print(f"   • p-value: {p_value:.6f}")
    print(f"   • Significativo: {'SÍ' if p_value < 0.05 else 'NO'} (α = 0.05)")

    # Segmentación por niveles de gasto
    print(f"\n🎯 ANÁLISIS POR SEGMENTOS DE GASTO:")
    print("-" * 60)

    # Crear segmentos basados en cuartiles
    df['charges_segment'] = pd.qcut(df['charges_total'],
                                   q=4,
                                   labels=['Bajo', 'Medio-Bajo', 'Medio-Alto', 'Alto'])

    # Calcular tasa de churn por segmento
    churn_by_charges = df.groupby('charges_segment').agg({
        'churn': ['count', 'sum', 'mean'],
        'charges_total': ['mean', 'min', 'max']
    }).round(3)

    for segment in churn_by_charges.index:
        total = churn_by_charges.loc[segment, ('churn', 'count')]
        churned = churn_by_charges.loc[segment, ('churn', 'sum')]
        rate = churn_by_charges.loc[segment, ('churn', 'mean')]
        avg_charges = churn_by_charges.loc[segment, ('charges_total', 'mean')]
        print(f"   • {segment:10}: {churned:4.0f}/{total:4.0f} clientes ({rate*100:5.1f}% churn) - Promedio: ${avg_charges:,.2f}")

    # Correlación con churn
    correlation = df['charges_total'].corr(df['churn'])
    print(f"\n🔗 CORRELACIÓN:")
    print(f"   • Correlación charges_total-churn: {correlation:.4f}")

    strength = "Fuerte" if abs(correlation) > 0.3 else "Moderada" if abs(correlation) > 0.1 else "Débil"
    direction = "negativa" if correlation < 0 else "positiva"
    print(f"   • Interpretación: Correlación {strength} {direction}")

    print("="*80)
    return charges_stats, churn_by_charges

# Ejecutar análisis de gasto total
charges_stats, churn_by_charges = analyze_total_charges_vs_churn(df_cleaned)

# Visualizaciones específicas para Tiempo de Contrato vs Churn
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('📊 ANÁLISIS ESPECÍFICO: TIEMPO DE CONTRATO vs CHURN', fontsize=16, fontweight='bold')

# 1. Boxplot: Tenure vs Churn
ax1 = axes[0, 0]
tenure_data = [df_cleaned[df_cleaned['churn']==0]['tenure'], df_cleaned[df_cleaned['churn']==1]['tenure']]
bp1 = ax1.boxplot(tenure_data, labels=['No Churn', 'Churn'], patch_artist=True)
bp1['boxes'][0].set_facecolor('#3498db')
bp1['boxes'][1].set_facecolor('#e74c3c')
ax1.set_title('⏰ Distribución de Tiempo de Contrato por Churn\n(Boxplot)', fontweight='bold')
ax1.set_ylabel('Tiempo de Contrato (meses)')
ax1.grid(True, alpha=0.3)

# Agregar estadísticas
no_churn_median = df_cleaned[df_cleaned['churn']==0]['tenure'].median()
churn_median = df_cleaned[df_cleaned['churn']==1]['tenure'].median()
ax1.text(1, no_churn_median + 2, f'Mediana: {no_churn_median:.1f}', ha='center', fontweight='bold',
         bbox=dict(boxstyle="round,pad=0.3", facecolor="#3498db", alpha=0.7))
ax1.text(2, churn_median + 2, f'Mediana: {churn_median:.1f}', ha='center', fontweight='bold',
         bbox=dict(boxstyle="round,pad=0.3", facecolor="#e74c3c", alpha=0.7))

# 2. Histogram: Tenure vs Churn (overlapped)
ax2 = axes[0, 1]
ax2.hist(df_cleaned[df_cleaned['churn']==0]['tenure'], bins=30, alpha=0.7, label='No Churn', color='#3498db', density=True)
ax2.hist(df_cleaned[df_cleaned['churn']==1]['tenure'], bins=30, alpha=0.7, label='Churn', color='#e74c3c', density=True)
ax2.set_title('📈 Distribución de Densidad: Tiempo de Contrato\n(Histograma Superpuesto)', fontweight='bold')
ax2.set_xlabel('Tiempo de Contrato (meses)')
ax2.set_ylabel('Densidad')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. Bar Chart: Churn Rate por Segmentos de Tenure
ax3 = axes[1, 0]
churn_by_tenure_viz = df_cleaned.groupby('tenure_segment')['churn'].agg(['count', 'mean']).reset_index()
bars1 = ax3.bar(churn_by_tenure_viz['tenure_segment'], churn_by_tenure_viz['mean'] * 100,
                color=['#85C1E9', '#F8C471', '#F1948A', '#D98880', '#CD6155'], alpha=0.8)
ax3.set_title('📊 Tasa de Churn por Segmentos de Tiempo de Contrato', fontweight='bold')
ax3.set_xlabel('Segmentos de Tiempo de Contrato')
ax3.set_ylabel('Tasa de Churn (%)')
ax3.tick_params(axis='x', rotation=45)
ax3.grid(True, alpha=0.3, axis='y')

# Agregar valores en las barras
for bar, rate in zip(bars1, churn_by_tenure_viz['mean'] * 100):
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
            f'{rate:.1f}%', ha='center', fontweight='bold')

# 4. Scatter Plot: Tenure vs Charges_Total (colored by churn)
ax4 = axes[1, 1]
no_churn_data = df_cleaned[df_cleaned['churn']==0]
churn_data = df_cleaned[df_cleaned['churn']==1]

ax4.scatter(no_churn_data['tenure'], no_churn_data['charges_total'],
           c='#3498db', alpha=0.6, s=20, label='No Churn')
ax4.scatter(churn_data['tenure'], churn_data['charges_total'],
           c='#e74c3c', alpha=0.8, s=20, label='Churn')
ax4.set_title('🔍 Relación: Tiempo de Contrato vs Gasto Total', fontweight='bold')
ax4.set_xlabel('Tiempo de Contrato (meses)')
ax4.set_ylabel('Gasto Total ($)')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualizaciones específicas para Gasto Total vs Churn
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('💰 ANÁLISIS ESPECÍFICO: GASTO TOTAL vs CHURN', fontsize=16, fontweight='bold')

# 1. Boxplot: Charges Total vs Churn
ax1 = axes[0, 0]
charges_data = [df_cleaned[df_cleaned['churn']==0]['charges_total'], df_cleaned[df_cleaned['churn']==1]['charges_total']]
bp2 = ax1.boxplot(charges_data, labels=['No Churn', 'Churn'], patch_artist=True)
bp2['boxes'][0].set_facecolor('#2ECC71')
bp2['boxes'][1].set_facecolor('#E67E22')
ax1.set_title('💰 Distribución de Gasto Total por Churn\n(Boxplot)', fontweight='bold')
ax1.set_ylabel('Gasto Total ($)')
ax1.grid(True, alpha=0.3)

# Agregar estadísticas
no_churn_median_charges = df_cleaned[df_cleaned['churn']==0]['charges_total'].median()
churn_median_charges = df_cleaned[df_cleaned['churn']==1]['charges_total'].median()
ax1.text(1, no_churn_median_charges + 500, f'Mediana: ${no_churn_median_charges:,.0f}',
         ha='center', fontweight='bold', bbox=dict(boxstyle="round,pad=0.3", facecolor="#2ECC71", alpha=0.7))
ax1.text(2, churn_median_charges + 500, f'Mediana: ${churn_median_charges:,.0f}',
         ha='center', fontweight='bold', bbox=dict(boxstyle="round,pad=0.3", facecolor="#E67E22", alpha=0.7))

# 2. Histogram: Charges Total vs Churn (overlapped)
ax2 = axes[0, 1]
ax2.hist(df_cleaned[df_cleaned['churn']==0]['charges_total'], bins=30, alpha=0.7, label='No Churn',
         color='#2ECC71', density=True)
ax2.hist(df_cleaned[df_cleaned['churn']==1]['charges_total'], bins=30, alpha=0.7, label='Churn',
         color='#E67E22', density=True)
ax2.set_title('📈 Distribución de Densidad: Gasto Total\n(Histograma Superpuesto)', fontweight='bold')
ax2.set_xlabel('Gasto Total ($)')
ax2.set_ylabel('Densidad')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. Bar Chart: Churn Rate por Segmentos de Charges
ax3 = axes[1, 0]
churn_by_charges_viz = df_cleaned.groupby('charges_segment')['churn'].agg(['count', 'mean']).reset_index()
bars2 = ax3.bar(churn_by_charges_viz['charges_segment'], churn_by_charges_viz['mean'] * 100,
                color=['#A9DFBF', '#F9E79F', '#F5B7B1', '#EC7063'], alpha=0.8)
ax3.set_title('💰 Tasa de Churn por Segmentos de Gasto Total', fontweight='bold')
ax3.set_xlabel('Segmentos de Gasto Total')
ax3.set_ylabel('Tasa de Churn (%)')
ax3.tick_params(axis='x', rotation=45)
ax3.grid(True, alpha=0.3, axis='y')

# Agregar valores en las barras
for bar, rate in zip(bars2, churn_by_charges_viz['mean'] * 100):
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
            f'{rate:.1f}%', ha='center', fontweight='bold')

# 4. Violin Plot: Distribución más detallada de Charges por Churn
ax4 = axes[1, 1]
violin_data = [df_cleaned[df_cleaned['churn']==0]['charges_total'],
               df_cleaned[df_cleaned['churn']==1]['charges_total']]
parts = ax4.violinplot(violin_data, positions=[0, 1], showmeans=True, showmedians=True)
ax4.set_xticks([0, 1])
ax4.set_xticklabels(['No Churn', 'Churn'])
ax4.set_title('🎻 Distribución Detallada: Gasto Total por Churn\n(Violin Plot)', fontweight='bold')
ax4.set_ylabel('Gasto Total ($)')
ax4.grid(True, alpha=0.3)

# Colorear violin plots
for pc, color in zip(parts['bodies'], ['#2ECC71', '#E67E22']):
    pc.set_facecolor(color)
    pc.set_alpha(0.7)

plt.tight_layout()
plt.show()

# Matriz de Correlación - Variables Clave vs Churn
key_variables = ['churn', 'tenure', 'charges_total', 'charges_monthly']
available_vars = [var for var in key_variables if var in df_cleaned.columns]

if len(available_vars) > 2:
    # Crear matriz de correlación
    corr_matrix = df_cleaned[available_vars].corr()

    # Crear figura
    plt.figure(figsize=(10, 8))

    # Crear heatmap
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix,
                mask=mask,
                annot=True,
                cmap='RdBu_r',
                center=0,
                square=True,
                fmt='.3f',
                cbar_kws={"shrink": 0.8},
                linewidths=0.5)

    plt.title('🌡️ Matriz de Correlación: Variables Clave vs Churn\n',
              fontsize=16, fontweight='bold')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    print("📊 CORRELACIONES DESTACADAS:")
    print("="*50)
    for var in available_vars:
        if var != 'churn':
            corr_value = corr_matrix.loc[var, 'churn']
            strength = "Fuerte" if abs(corr_value) > 0.3 else "Moderada" if abs(corr_value) > 0.1 else "Débil"
            direction = "Negativa" if corr_value < 0 else "Positiva"
            print(f"• {var:15} → Churn: {corr_value:6.3f} ({strength} {direction})")
else:
    print("⚠️ No hay suficientes variables para crear el heatmap")

"""### 📋 Resumen de Insights Clave del Análisis Específico

#### ⏰ **TIEMPO DE CONTRATO (TENURE)**
- **Diferencia crítica**: Los clientes SIN churn tienen aproximadamente **20 meses MÁS** de contrato en promedio
- **Correlación fuerte negativa (-0.35)**: A menor tiempo de contrato, mayor probabilidad de churn
- **Segmento crítico**: Clientes de 0-12 meses presentan **47.7% de churn** (casi 1 de cada 2 cancela)
- **Fidelización**: Después de 48 meses, solo **9.5% de churn** (10x menor riesgo)

#### 💰 **GASTO TOTAL (CHARGES_TOTAL)**
- **Patrón sorprendente**: Los clientes que cancelan gastan **$1,024 MENOS** en promedio
- **Correlación moderada negativa (-0.20)**: Clientes de menor gasto = mayor riesgo de churn
- **Segmento de alto riesgo**: Gasto bajo presenta **43.5% de churn**
- **Clientes premium**: Alto gasto solo **14.5% de churn** (3x menor riesgo)

#### 🔍 **TENDENCIAS IDENTIFICADAS**
1. **El primer año es determinante**: 47.7% vs 9.5% después de 48 meses
2. **Paradoja del gasto**: Menor gasto → Mayor probabilidad de cancelación
3. **Combinación crítica**: Clientes nuevos + bajo gasto = máximo riesgo
4. **Punto de inflexión**: Después del primer año, la retención mejora significativamente

### 💡 **Recomendaciones Estratégicas de Negocio**

#### 🎯 **ACCIONES INMEDIATAS - ALTA PRIORIDAD**

**1. Programa Intensivo de Onboarding (0-12 meses)**
- Implementar seguimiento proactivo semanal para clientes nuevos
- Crear programa de bienvenida con beneficios exclusivos primeros 6 meses
- Asignar representante de cuenta dedicado para período crítico inicial
- **Meta**: Reducir churn de 47.7% a 30% en primer año

**2. Estrategia para Clientes de Bajo Gasto**
- Diseñar planes escalables y opciones de upgrade gradual
- Ofertas personalizadas para incrementar valor percibido
- Programas de fidelidad específicos para segmento bajo gasto
- **Meta**: Reducir churn de 43.5% a 25% en segmento bajo gasto

#### 📊 **MÉTRICAS DE MONITOREO CRÍTICAS**

```python
# Métricas clave a implementar:
metrics = {
    'churn_rate_0_12_months': 'Tasa de churn primeros 12 meses',
    'churn_rate_by_spend_segment': 'Churn por segmento de gasto',
    'time_to_first_renewal': 'Tiempo hasta primera renovación',
    'customer_lifetime_value': 'Valor de vida del cliente',
    'retention_program_roi': 'ROI programas de retención'
}
```

#### 🔄 **ESTRATEGIAS DE RETENCIÓN ESPECÍFICAS**

**Para Clientes Nuevos (0-12 meses):**
- Check-ins automáticos en días 30, 60, 90
- Tutoriales personalizados y soporte técnico gratuito
- Descuentos por permanencia tras primer año

**Para Clientes de Bajo Gasto:**
- Análisis de necesidades para identificar servicios adicionales
- Planes familiares o grupales con descuentos
- Programa de referidos con beneficios mutuos

**Para Retención a Largo Plazo:**
- Contratos multi-año con beneficios crecientes
- Programa VIP para clientes +48 meses
- Servicios premium sin costo adicional por fidelidad

### 🎯 **Conclusiones del Análisis Específico**

#### **📈 HALLAZGOS PRINCIPALES**
Este análisis reveló dos patrones críticos para la retención de clientes en TelecomX:

1. **Factor Tiempo**: La correlación fuerte negativa (-0.35) entre tenure y churn confirma que **el primer año es absolutamente crítico**. Los datos muestran una diferencia dramática: 47.7% de churn en los primeros 12 meses vs 9.5% después de 48 meses.

2. **Factor Económico**: Contrario a la intuición, los clientes de menor gasto presentan mayor riesgo de churn (-0.20 de correlación). Esto sugiere que el valor percibido, no el precio absoluto, es el factor determinante.

#### **🚀 IMPACTO POTENCIAL**
Implementando las estrategias recomendadas, TelecomX podría:
- **Reducir el churn general del 26.5% al 20%**
- **Aumentar la retención en el primer año del 52.3% al 70%**
- **Mejorar el CLV (Customer Lifetime Value) en un 35%**
- **Generar ROI positivo en programas de retención dentro de 6 meses**

#### **🔑 FACTORES CLAVE DE ÉXITO**
1. **Timing**: Actuar proactivamente en los primeros 30-90 días
2. **Segmentación**: Estrategias diferenciadas por nivel de gasto y tenure
3. **Valor percibido**: Enfoque en beneficios tangibles, no solo descuentos
4. **Monitoreo continuo**: Métricas específicas para medir impacto

**El análisis estadístico confirma que ambas relaciones son altamente significativas (p < 0.001), validando la importancia crítica de estas variables para predecir y prevenir la cancelación de clientes.**

## 7. División del Dataset: Entrenamiento y Prueba

En esta sección se divide el conjunto de datos en conjuntos de entrenamiento y prueba para evaluar el rendimiento del modelo de manera objetiva. Se implementa una división estratificada para mantener la proporción de clases en ambos conjuntos, lo cual es especialmente importante dado el desbalance de clases identificado anteriormente.

**Estrategia de División:**
- **80% para entrenamiento** - 20% para prueba (recomendado para datasets > 5,000 registros)
- **División estratificada** para mantener proporción de churn
- **Preparación de variables** categóricas y numéricas
- **Validación de la división** con métricas descriptivas
"""

# Importaciones necesarias para la división y preparación de datos
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pandas as pd
import numpy as np

def prepare_data_for_modeling(df):
    """
    Prepara los datos para el modelado: limpieza, codificación y separación de variables
    """
    print("🔧 PREPARACIÓN DE DATOS PARA MODELADO")
    print("="*70)

    # Crear una copia del dataset para trabajar
    df_model = df.copy()

    # Convertir charges_total a numérico si no lo está
    df_model['charges_total'] = pd.to_numeric(df_model['charges_total'], errors='coerce')

    # Eliminar filas con valores nulos en variables críticas
    initial_rows = len(df_model)
    df_model = df_model.dropna(subset=['churn', 'charges_total']).copy()
    final_rows = len(df_model)

    print(f"📊 LIMPIEZA DE DATOS:")
    print(f"   • Registros iniciales: {initial_rows:,}")
    print(f"   • Registros finales: {final_rows:,}")
    print(f"   • Registros eliminados: {initial_rows - final_rows:,}")

    # Identificar tipos de variables
    categorical_vars = []
    numerical_vars = []
    binary_vars = []

    for col in df_model.columns:
        if col == 'churn':  # Variable target
            continue
        elif df_model[col].dtype == 'object':
            unique_values = df_model[col].nunique()
            if unique_values == 2:
                binary_vars.append(col)
            else:
                categorical_vars.append(col)
        elif df_model[col].nunique() == 2 and set(df_model[col].unique()).issubset({0, 1}):
            binary_vars.append(col)
        else:
            numerical_vars.append(col)

    print(f"\n📋 CLASIFICACIÓN DE VARIABLES:")
    print(f"   • Variables categóricas: {len(categorical_vars)} - {categorical_vars}")
    print(f"   • Variables binarias: {len(binary_vars)} - {binary_vars}")
    print(f"   • Variables numéricas: {len(numerical_vars)} - {numerical_vars}")
    print(f"   • Variable target: churn")

    return df_model, categorical_vars, numerical_vars, binary_vars

def encode_categorical_variables(df, categorical_vars):
    """
    Codifica variables categóricas usando One-Hot Encoding
    """
    print(f"\n🔄 CODIFICACIÓN DE VARIABLES CATEGÓRICAS:")

    if categorical_vars:
        # Aplicar One-Hot Encoding
        df_encoded = pd.get_dummies(df,
                                   columns=categorical_vars,
                                   drop_first=True,  # Evita multicolinealidad
                                   dtype=int)

        # Mostrar nuevas columnas creadas
        original_cols = set(df.columns)
        new_cols = set(df_encoded.columns)
        created_cols = sorted(list(new_cols - original_cols))

        print(f"   • Columnas originales: {len(original_cols)}")
        print(f"   • Columnas después de encoding: {len(new_cols)}")
        print(f"   • Nuevas columnas creadas: {len(created_cols)}")

        if len(created_cols) <= 10:  # Mostrar solo si no son demasiadas
            print(f"   • Columnas nuevas: {created_cols}")

        return df_encoded
    else:
        print("   • No hay variables categóricas para codificar")
        return df

# Ejecutar preparación de datos
df_model, categorical_vars, numerical_vars, binary_vars = prepare_data_for_modeling(df_cleaned)
df_encoded = encode_categorical_variables(df_model, categorical_vars)

print(f"\n✅ DATOS PREPARADOS PARA MODELADO:")
print(f"   • Forma final del dataset: {df_encoded.shape}")
print(f"   • Total de features: {df_encoded.shape[1] - 1} (excluyendo target)")

def split_dataset(df, test_size=0.2, random_state=42):
    """
    Divide el dataset en conjuntos de entrenamiento y prueba con estratificación
    """
    print("🔀 DIVISIÓN DEL DATASET EN ENTRENAMIENTO Y PRUEBA")
    print("="*70)

    # Separar características (X) y variable objetivo (y)
    X = df.drop('churn', axis=1)
    y = df['churn']

    print(f"📊 DATASET COMPLETO:")
    print(f"   • Total de registros: {len(df):,}")
    print(f"   • Total de features: {X.shape[1]}")
    print(f"   • Distribución de clases:")
    print(f"     - No Churn (0): {(y == 0).sum():,} ({(y == 0).mean()*100:.1f}%)")
    print(f"     - Churn (1): {(y == 1).sum():,} ({(y == 1).mean()*100:.1f}%)")

    # División estratificada
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y  # Mantiene la proporción de clases
    )

    print(f"\n🚂 CONJUNTO DE ENTRENAMIENTO:")
    print(f"   • Registros: {len(X_train):,} ({(1-test_size)*100:.0f}%)")
    print(f"   • Features: {X_train.shape[1]}")
    print(f"   • Distribución de clases:")
    print(f"     - No Churn (0): {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)")
    print(f"     - Churn (1): {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.1f}%)")

    print(f"\n🧪 CONJUNTO DE PRUEBA:")
    print(f"   • Registros: {len(X_test):,} ({test_size*100:.0f}%)")
    print(f"   • Features: {X_test.shape[1]}")
    print(f"   • Distribución de clases:")
    print(f"     - No Churn (0): {(y_test == 0).sum():,} ({(y_test == 0).mean()*100:.1f}%)")
    print(f"     - Churn (1): {(y_test == 1).sum():,} ({(y_test == 1).mean()*100:.1f}%)")

    # Verificar que la estratificación funcionó correctamente
    train_churn_rate = y_train.mean()
    test_churn_rate = y_test.mean()
    difference = abs(train_churn_rate - test_churn_rate)

    print(f"\n✅ VERIFICACIÓN DE ESTRATIFICACIÓN:")
    print(f"   • Tasa de churn en entrenamiento: {train_churn_rate*100:.2f}%")
    print(f"   • Tasa de churn en prueba: {test_churn_rate*100:.2f}%")
    print(f"   • Diferencia: {difference*100:.2f}% ({'✅ Buena' if difference < 0.01 else '⚠️ Revisar'})")

    return X_train, X_test, y_train, y_test

# Ejecutar división del dataset
X_train, X_test, y_train, y_test = split_dataset(df_encoded, test_size=0.2, random_state=42)

# Visualización de la división del dataset
fig, axes = plt.subplots(2, 2, figsize=(16, 10))
fig.suptitle('📊 ANÁLISIS DE LA DIVISIÓN DEL DATASET', fontsize=16, fontweight='bold')

# 1. Distribución de clases en conjunto completo
ax1 = axes[0, 0]
y_complete = df_encoded['churn']
complete_counts = y_complete.value_counts().sort_index()
bars1 = ax1.bar(['No Churn', 'Churn'], complete_counts.values,
                color=['#3498db', '#e74c3c'], alpha=0.8)
ax1.set_title('Dataset Completo\nDistribución de Clases', fontweight='bold')
ax1.set_ylabel('Número de Registros')
for i, v in enumerate(complete_counts.values):
    ax1.text(i, v + 50, f'{v:,}\n({v/len(y_complete)*100:.1f}%)',
             ha='center', fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')

# 2. Distribución de clases en entrenamiento
ax2 = axes[0, 1]
train_counts = y_train.value_counts().sort_index()
bars2 = ax2.bar(['No Churn', 'Churn'], train_counts.values,
                color=['#3498db', '#e74c3c'], alpha=0.8)
ax2.set_title('Conjunto de Entrenamiento (80%)\nDistribución de Clases', fontweight='bold')
ax2.set_ylabel('Número de Registros')
for i, v in enumerate(train_counts.values):
    ax2.text(i, v + 30, f'{v:,}\n({v/len(y_train)*100:.1f}%)',
             ha='center', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# 3. Distribución de clases en prueba
ax3 = axes[1, 0]
test_counts = y_test.value_counts().sort_index()
bars3 = ax3.bar(['No Churn', 'Churn'], test_counts.values,
                color=['#3498db', '#e74c3c'], alpha=0.8)
ax3.set_title('Conjunto de Prueba (20%)\nDistribución de Clases', fontweight='bold')
ax3.set_ylabel('Número de Registros')
for i, v in enumerate(test_counts.values):
    ax3.text(i, v + 10, f'{v:,}\n({v/len(y_test)*100:.1f}%)',
             ha='center', fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')

# 4. Comparación de proporciones
ax4 = axes[1, 1]
proportions_data = {
    'Completo': [complete_counts[0]/len(y_complete), complete_counts[1]/len(y_complete)],
    'Entrenamiento': [train_counts[0]/len(y_train), train_counts[1]/len(y_train)],
    'Prueba': [test_counts[0]/len(y_test), test_counts[1]/len(y_test)]
}

x = np.arange(len(proportions_data))
width = 0.35

bars_no_churn = ax4.bar(x - width/2, [prop[0] for prop in proportions_data.values()],
                        width, label='No Churn', color='#3498db', alpha=0.8)
bars_churn = ax4.bar(x + width/2, [prop[1] for prop in proportions_data.values()],
                     width, label='Churn', color='#e74c3c', alpha=0.8)

ax4.set_title('Comparación de Proporciones\n(Verificación de Estratificación)', fontweight='bold')
ax4.set_ylabel('Proporción')
ax4.set_xticks(x)
ax4.set_xticklabels(proportions_data.keys())
ax4.legend()
ax4.grid(True, alpha=0.3, axis='y')

# Agregar valores en las barras
for bars in [bars_no_churn, bars_churn]:
    for bar in bars:
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# Resumen estadístico de la división
print("\n📈 RESUMEN ESTADÍSTICO DE LA DIVISIÓN:")
print("="*60)
print(f"• Tasa de churn original: {y_complete.mean():.4f}")
print(f"• Tasa de churn entrenamiento: {y_train.mean():.4f}")
print(f"• Tasa de churn prueba: {y_test.mean():.4f}")
print(f"• Diferencia máxima: {max(abs(y_complete.mean() - y_train.mean()), abs(y_complete.mean() - y_test.mean())):.4f}")
print(f"• Estado de estratificación: {'✅ EXITOSA' if max(abs(y_complete.mean() - y_train.mean()), abs(y_complete.mean() - y_test.mean())) < 0.02 else '⚠️ REVISAR'}")

def scale_numerical_features(X_train, X_test, numerical_vars):
    """
    Escala las variables numéricas usando StandardScaler
    """
    print("📏 ESCALADO DE VARIABLES NUMÉRICAS")
    print("="*60)

    # Identificar columnas numéricas reales (excluyendo categóricas codificadas)
    # Variables que definitivamente NO deben escalarse
    exclude_from_scaling = [
        'tenure_segment', 'charges_segment',  # Variables categóricas creadas
        'num_services'  # Si es un conteo discreto
    ]

    # Filtrar variables numéricas verdaderas
    available_numerical = []
    for col in X_train.columns:
        if col in numerical_vars and col not in exclude_from_scaling:
            # Verificar que la columna sea realmente numérica
            if X_train[col].dtype in ['int64', 'float64']:
                # Verificar que no tenga valores string
                try:
                    pd.to_numeric(X_train[col].iloc[:5])  # Test con las primeras 5 filas
                    available_numerical.append(col)
                except (ValueError, TypeError):
                    print(f"   ⚠️ Excluyendo {col}: contiene valores no numéricos")

    print(f"📊 VARIABLES NUMÉRICAS IDENTIFICADAS:")
    print(f"   • Variables numéricas disponibles: {len(available_numerical)}")
    print(f"   • Variables: {available_numerical}")
    print(f"   • Variables excluidas del escalado: {exclude_from_scaling}")

    if not available_numerical:
        print("   ⚠️ No se encontraron variables numéricas válidas para escalar")
        return X_train.copy(), X_test.copy(), None

    # Crear copias para no modificar los originales
    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()

    # Inicializar el escalador
    scaler = StandardScaler()

    # Verificar que las columnas existen en ambos conjuntos
    valid_numerical = [col for col in available_numerical if col in X_train.columns and col in X_test.columns]

    if not valid_numerical:
        print("   ⚠️ No hay variables numéricas válidas en ambos conjuntos")
        return X_train_scaled, X_test_scaled, None

    # Ajustar el escalador solo con los datos de entrenamiento
    scaler.fit(X_train[valid_numerical])

    # Aplicar transformación a ambos conjuntos
    X_train_scaled[valid_numerical] = scaler.transform(X_train[valid_numerical])
    X_test_scaled[valid_numerical] = scaler.transform(X_test[valid_numerical])

    print(f"\n🔧 PROCESO DE ESCALADO:")
    print(f"   • Escalador utilizado: StandardScaler (media=0, std=1)")
    print(f"   • Variables escaladas: {len(valid_numerical)}")
    print(f"   • Variables escaladas: {valid_numerical}")
    print(f"   • Ajuste realizado solo con datos de entrenamiento")

    # Mostrar estadísticas antes y después del escalado
    print(f"\n📊 ESTADÍSTICAS ANTES Y DESPUÉS DEL ESCALADO:")
    print("-" * 60)

    for var in valid_numerical[:3]:  # Mostrar solo las primeras 3 variables
        print(f"\n📈 {var.upper()}:")
        print(f"   ANTES - Entrenamiento: Media={X_train[var].mean():.2f}, Std={X_train[var].std():.2f}")
        print(f"   ANTES - Prueba: Media={X_test[var].mean():.2f}, Std={X_test[var].std():.2f}")
        print(f"   DESPUÉS - Entrenamiento: Media={X_train_scaled[var].mean():.2f}, Std={X_train_scaled[var].std():.2f}")
        print(f"   DESPUÉS - Prueba: Media={X_test_scaled[var].mean():.2f}, Std={X_test_scaled[var].std():.2f}")

    if len(valid_numerical) > 3:
        print(f"\n   ... y {len(valid_numerical) - 3} variables más escaladas")

    print(f"\n✅ ESCALADO COMPLETADO EXITOSAMENTE")

    return X_train_scaled, X_test_scaled, scaler

# Aplicar escalado a variables numéricas
X_train_scaled, X_test_scaled, scaler = scale_numerical_features(X_train, X_test, numerical_vars)

# Verificar qué variables categóricas quedaron en el dataset
def check_remaining_categorical_variables(X_train):
    """
    Verifica qué variables categóricas pueden haber quedado sin codificar
    """
    print("🔍 VERIFICACIÓN DE VARIABLES CATEGÓRICAS RESTANTES")
    print("="*60)

    categorical_remaining = []
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            categorical_remaining.append(col)
        elif X_train[col].dtype in ['int64', 'float64']:
            # Verificar si son variables categóricas codificadas como números
            unique_values = X_train[col].unique()
            if len(unique_values) <= 10:  # Asumimos que ≤10 valores únicos podrían ser categóricas
                print(f"   📋 {col}: {len(unique_values)} valores únicos - {sorted(unique_values)}")

    if categorical_remaining:
        print(f"\n⚠️ VARIABLES CATEGÓRICAS SIN CODIFICAR:")
        for col in categorical_remaining:
            unique_vals = X_train[col].unique()
            print(f"   • {col}: {len(unique_vals)} valores únicos")
            if len(unique_vals) <= 10:
                print(f"     Valores: {list(unique_vals)}")

        print(f"\n💡 RECOMENDACIÓN:")
        print(f"   Estas variables necesitan ser eliminadas o codificadas antes del modelado")

        # Eliminar variables categóricas restantes
        X_train_clean = X_train.drop(columns=categorical_remaining)
        X_test_clean = X_test.drop(columns=categorical_remaining)
        X_train_scaled_clean = X_train_scaled.drop(columns=categorical_remaining)
        X_test_scaled_clean = X_test_scaled.drop(columns=categorical_remaining)

        print(f"\n✅ VARIABLES CATEGÓRICAS ELIMINADAS:")
        print(f"   • Forma anterior: {X_train.shape}")
        print(f"   • Forma nueva: {X_train_clean.shape}")

        return X_train_clean, X_test_clean, X_train_scaled_clean, X_test_scaled_clean
    else:
        print("✅ No se encontraron variables categóricas sin codificar")
        return X_train, X_test, X_train_scaled, X_test_scaled

# Verificar y limpiar variables categóricas restantes
X_train_final, X_test_final, X_train_scaled_final, X_test_scaled_final = check_remaining_categorical_variables(X_train)

# Actualizar las variables para usar las versiones limpias
X_train = X_train_final
X_test = X_test_final
X_train_scaled = X_train_scaled_final
X_test_scaled = X_test_scaled_final

print(f"\n🎯 DATASETS FINALES PREPARADOS:")
print(f"   • X_train: {X_train.shape}")
print(f"   • X_test: {X_test.shape}")
print(f"   • X_train_scaled: {X_train_scaled.shape}")
print(f"   • X_test_scaled: {X_test_scaled.shape}")

def validate_data_split(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled):
    """
    Realiza validaciones finales de la división de datos
    """
    print("✅ VALIDACIÓN FINAL DE LA DIVISIÓN DE DATOS")
    print("="*70)

    # 1. Verificar formas de los conjuntos
    print("📊 VERIFICACIÓN DE FORMAS:")
    print(f"   • X_train: {X_train.shape}")
    print(f"   • X_test: {X_test.shape}")
    print(f"   • y_train: {y_train.shape}")
    print(f"   • y_test: {y_test.shape}")
    print(f"   • X_train_scaled: {X_train_scaled.shape}")
    print(f"   • X_test_scaled: {X_test_scaled.shape}")

    # 2. Verificar que no hay valores nulos
    print(f"\n🔍 VERIFICACIÓN DE VALORES NULOS:")
    print(f"   • X_train nulos: {X_train.isnull().sum().sum()}")
    print(f"   • X_test nulos: {X_test.isnull().sum().sum()}")
    print(f"   • y_train nulos: {y_train.isnull().sum()}")
    print(f"   • y_test nulos: {y_test.isnull().sum()}")
    print(f"   • X_train_scaled nulos: {X_train_scaled.isnull().sum().sum()}")
    print(f"   • X_test_scaled nulos: {X_test_scaled.isnull().sum().sum()}")

    # 3. Verificar que las columnas son las mismas
    print(f"\n📋 VERIFICACIÓN DE COLUMNAS:")
    columns_match = list(X_train.columns) == list(X_test.columns)
    columns_scaled_match = list(X_train_scaled.columns) == list(X_test_scaled.columns)
    print(f"   • Columnas coinciden entre train y test: {'✅ SÍ' if columns_match else '❌ NO'}")
    print(f"   • Columnas scaled coinciden: {'✅ SÍ' if columns_scaled_match else '❌ NO'}")
    print(f"   • Número de features: {len(X_train.columns)}")

    # 4. Verificar tipos de datos
    print(f"\n🔢 TIPOS DE DATOS:")
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns
    object_cols = X_train.select_dtypes(include=['object']).columns
    print(f"   • Columnas numéricas: {len(numeric_cols)}")
    print(f"   • Columnas de objeto: {len(object_cols)}")

    if len(object_cols) > 0:
        print(f"   ⚠️ Columnas de objeto detectadas: {list(object_cols)[:5]}")
        print(f"   💡 Estas pueden necesitar codificación adicional")

    # 5. Verificar rango de valores en y
    print(f"\n🎯 VERIFICACIÓN DE VARIABLE TARGET:")
    print(f"   • Valores únicos en y_train: {sorted(y_train.unique())}")
    print(f"   • Valores únicos en y_test: {sorted(y_test.unique())}")
    print(f"   • Tipo de y_train: {y_train.dtype}")
    print(f"   • Tipo de y_test: {y_test.dtype}")

    # 6. Verificar balanceamiento de clases
    print(f"\n⚖️ VERIFICACIÓN DE BALANCEAMIENTO:")
    train_balance = y_train.value_counts(normalize=True).sort_index()
    test_balance = y_test.value_counts(normalize=True).sort_index()
    print(f"   • Balance entrenamiento: No Churn={train_balance[0]:.3f}, Churn={train_balance[1]:.3f}")
    print(f"   • Balance prueba: No Churn={test_balance[0]:.3f}, Churn={test_balance[1]:.3f}")

    balance_diff = abs(train_balance[1] - test_balance[1])
    print(f"   • Diferencia en tasa de churn: {balance_diff:.4f}")
    print(f"   • Estratificación: {'✅ BUENA' if balance_diff < 0.02 else '⚠️ REVISAR'}")

    # 7. Verificar rangos de escalado
    if X_train_scaled is not None:
        print(f"\n📏 VERIFICACIÓN DE ESCALADO:")
        scaled_means = X_train_scaled.select_dtypes(include=[np.number]).mean()
        scaled_stds = X_train_scaled.select_dtypes(include=[np.number]).std()

        # Variables que deberían estar escaladas (media~0, std~1)
        properly_scaled = []
        for col in scaled_means.index:
            if abs(scaled_means[col]) < 0.1 and abs(scaled_stds[col] - 1) < 0.1:
                properly_scaled.append(col)

        print(f"   • Variables correctamente escaladas: {len(properly_scaled)}")
        print(f"   • Total de variables numéricas: {len(scaled_means)}")
        print(f"   • Porcentaje escalado correctamente: {len(properly_scaled)/len(scaled_means)*100:.1f}%")

    # 8. Resumen final
    print(f"\n🏆 RESUMEN FINAL:")
    all_checks = [
        X_train.shape[0] > 0,
        X_test.shape[0] > 0,
        X_train.isnull().sum().sum() == 0,
        X_test.isnull().sum().sum() == 0,
        y_train.isnull().sum() == 0,
        y_test.isnull().sum() == 0,
        columns_match,
        len(object_cols) == 0,
        set(y_train.unique()).issubset({0, 1}),
        set(y_test.unique()).issubset({0, 1}),
        balance_diff < 0.02
    ]

    passed_checks = sum(all_checks)
    total_checks = len(all_checks)

    print(f"   • Verificaciones pasadas: {passed_checks}/{total_checks}")
    print(f"   • Estado general: {'✅ DATOS LISTOS PARA MODELADO' if passed_checks == total_checks else '⚠️ REVISAR ALGUNOS ASPECTOS'}")

    if passed_checks >= total_checks - 1:  # Permitir 1 fallo menor
        print(f"\n🚀 LOS DATOS ESTÁN PREPARADOS PARA:")
        print(f"   • Entrenamiento de modelos de machine learning")
        print(f"   • Validación y evaluación de rendimiento")
        print(f"   • Comparación de diferentes algoritmos")
        print(f"   • Optimización de hiperparámetros")

    return passed_checks >= total_checks - 1

# Ejecutar validación final
data_ready = validate_data_split(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled)

"""### 📋 Resumen de la División y Preparación de Datos

#### ✅ **PROCESO COMPLETADO EXITOSAMENTE**

**1. Preparación de Datos:**
- ✅ Limpieza de valores nulos y inconsistentes
- ✅ Codificación de variables categóricas (One-Hot Encoding)
- ✅ Identificación y tratamiento de variables numéricas
- ✅ Eliminación de variables irrelevantes para el modelado

**2. División Estratificada (80/20):**
- ✅ **Entrenamiento**: ~80% de los datos manteniendo proporción de clases
- ✅ **Prueba**: ~20% de los datos para evaluación objetiva
- ✅ **Estratificación**: Mismo balance de churn en ambos conjuntos
- ✅ **Validación**: Sin data leakage entre conjuntos

**3. Escalado de Variables:**
- ✅ **StandardScaler**: Media=0, Desviación estándar=1
- ✅ **Ajuste solo en entrenamiento**: Previene data leakage
- ✅ **Variables numéricas**: tenure, charges_monthly, charges_total, charges_daily
- ✅ **Variables categóricas**: Mantenidas sin escalar

#### 📊 **DATASETS FINALES DISPONIBLES**

```python
# Conjuntos de datos preparados para modelado:
datasets = {
    'X_train': 'Features de entrenamiento (originales)',
    'X_test': 'Features de prueba (originales)',
    'X_train_scaled': 'Features de entrenamiento (escaladas)',
    'X_test_scaled': 'Features de prueba (escaladas)',
    'y_train': 'Variable target entrenamiento',
    'y_test': 'Variable target prueba',
    'scaler': 'Objeto para transformar nuevos datos'
}
```

#### 🚀 **PRÓXIMOS PASOS RECOMENDADOS**

**1. Entrenamiento de Modelos Baseline:**
- Regresión Logística
- Random Forest
- Gradient Boosting (XGBoost/LightGBM)

**2. Evaluación de Modelos:**
- Métricas: Precision, Recall, F1-Score, AUC-ROC
- Matrices de confusión
- Curvas ROC y Precision-Recall

**3. Optimización:**
- Validación cruzada estratificada
- Búsqueda de hiperparámetros
- Selección de features

**4. Interpretabilidad:**
- Feature importance
- SHAP values
- Análisis de errores
"""

# Verificación final y resumen de los datasets preparados
print("🎯 VERIFICACIÓN FINAL DE DATASETS PREPARADOS")
print("="*70)

print(f"\n📦 DATASETS DISPONIBLES:")
print(f"   • X_train: {X_train.shape} - Features entrenamiento (sin escalar)")
print(f"   • X_test: {X_test.shape} - Features prueba (sin escalar)")
print(f"   • X_train_scaled: {X_train_scaled.shape} - Features entrenamiento (escaladas)")
print(f"   • X_test_scaled: {X_test_scaled.shape} - Features prueba (escaladas)")
print(f"   • y_train: {y_train.shape} - Target entrenamiento")
print(f"   • y_test: {y_test.shape} - Target prueba")

print(f"\n📊 DISTRIBUCIÓN DE CLASES:")
print(f"   • Entrenamiento - No Churn: {(y_train==0).sum():,} ({(y_train==0).mean()*100:.1f}%)")
print(f"   • Entrenamiento - Churn: {(y_train==1).sum():,} ({(y_train==1).mean()*100:.1f}%)")
print(f"   • Prueba - No Churn: {(y_test==0).sum():,} ({(y_test==0).mean()*100:.1f}%)")
print(f"   • Prueba - Churn: {(y_test==1).sum():,} ({(y_test==1).mean()*100:.1f}%)")

print(f"\n🔧 ESCALADOR DISPONIBLE:")
if scaler is not None:
    print(f"   • Tipo: {type(scaler).__name__}")
    print(f"   • Variables escaladas: {len(scaler.feature_names_in_) if hasattr(scaler, 'feature_names_in_') else 'N/A'}")
    print(f"   • Listo para transformar nuevos datos")
else:
    print(f"   • No se aplicó escalado")

print(f"\n✅ ESTADO FINAL:")
print(f"   🎯 Datos completamente preparados para modelado")
print(f"   🔀 División estratificada exitosa")
print(f"   📏 Escalado aplicado correctamente")
print(f"   🚀 Listos para entrenar modelos de ML")

# Guardar información de los datasets para referencia futura
dataset_info = {
    'train_shape': X_train.shape,
    'test_shape': X_test.shape,
    'features': list(X_train.columns),
    'target_distribution_train': y_train.value_counts().to_dict(),
    'target_distribution_test': y_test.value_counts().to_dict(),
    'scaled': scaler is not None
}

print(f"\n📋 INFORMACIÓN DE DATASETS GUARDADA PARA REFERENCIA")

"""# 🤖 PARTE 3: MODELADO PREDICTIVO PARA CHURN

En esta sección implementaremos dos modelos predictivos diferentes para predecir la cancelación de clientes:

1. **Regresión Logística**: Modelo lineal que requiere normalización de datos
2. **Random Forest**: Modelo basado en árboles que no requiere normalización

## 🎯 Objetivos del Modelado:
- Comparar el rendimiento de modelos con y sin normalización
- Evaluar diferentes enfoques algorítmicos (lineal vs ensemble)
- Identificar las variables más importantes para predecir churn
- Proporcionar métricas de evaluación robustas

## 7. Preparación de Datos para Modelado

Antes de implementar los modelos, necesitamos preparar los datos adecuadamente:

### 📋 Pasos de Preparación:
1. **Identificación de variables**: Separar features numéricas, binarias y categóricas
2. **One-Hot Encoding**: Para variables categóricas con múltiples valores
3. **División train/test**: 80% entrenamiento, 20% prueba con estratificación
4. **Normalización condicional**: Solo para modelos que la requieren (Regresión Logística)

### 🔍 Justificación de la Normalización:
- **Regresión Logística**: Es sensible a la escala porque optimiza utilizando gradientes. Variables con diferentes escalas pueden dominar el proceso de aprendizaje
- **Random Forest**: No requiere normalización porque usa divisiones basadas en umbrales relativos, no distancias absolutas
"""

# Importación de librerías para modelado
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    roc_curve, precision_recall_curve, accuracy_score,
    precision_score, recall_score, f1_score
)
import warnings
warnings.filterwarnings('ignore')

print("📦 LIBRERÍAS DE MACHINE LEARNING IMPORTADAS")
print("="*50)
print("✅ Modelos: LogisticRegression, RandomForestClassifier")
print("✅ Métricas: Precisión, Recall, F1-Score, AUC-ROC")
print("✅ Preprocesamiento: StandardScaler, train_test_split")
print("✅ Validación: Cross-validation, StratifiedKFold")

def prepare_data_for_modeling(df):
    """
    Prepara los datos para modelado predictivo
    """
    print("🔧 PREPARANDO DATOS PARA MODELADO PREDICTIVO")
    print("="*60)

    # Crear una copia para no modificar el dataset original
    df_model = df.copy()

    # PASO CRÍTICO: Eliminar columnas creadas en análisis anteriores que no son para modelado
    columns_to_remove = [
        'tenure_segment',    # Segmentos creados en análisis anterior
        'charges_segment',   # Segmentos creados en análisis anterior
        'num_services'       # Variable derivada del análisis anterior
    ]

    # Eliminar columnas problemáticas si existen
    for col in columns_to_remove:
        if col in df_model.columns:
            df_model = df_model.drop(columns=[col])
            print(f"   🗑️ Eliminada columna de análisis anterior: {col}")

    # 1. IDENTIFICAR TIPOS DE VARIABLES
    print("\n🔍 IDENTIFICANDO TIPOS DE VARIABLES:")
    print("-" * 40)

    # Variables numéricas (excluyendo target)
    numerical_vars = ['tenure', 'charges_monthly', 'charges_total', 'charges_daily']
    numerical_vars = [var for var in numerical_vars if var in df_model.columns]

    # Variables categóricas que necesitan One-Hot Encoding
    categorical_vars = []
    for col in df_model.columns:
        if col != 'churn' and df_model[col].dtype == 'object':
            categorical_vars.append(col)

    # Variables binarias ya codificadas
    binary_vars = []
    for col in df_model.columns:
        if col != 'churn' and col not in numerical_vars and col not in categorical_vars:
            unique_vals = df_model[col].unique()
            if len(unique_vals) <= 2 and all(v in [0, 1, 0.0, 1.0] for v in unique_vals if pd.notna(v)):
                binary_vars.append(col)

    print(f"   📊 Variables numéricas: {len(numerical_vars)} → {numerical_vars}")
    print(f"   🔤 Variables categóricas: {len(categorical_vars)} → {categorical_vars}")
    print(f"   🔘 Variables binarias: {len(binary_vars)} → {binary_vars[:5]}...")

    # 2. APLICAR ONE-HOT ENCODING A VARIABLES CATEGÓRICAS
    if categorical_vars:
        print(f"\n🔄 APLICANDO ONE-HOT ENCODING A VARIABLES CATEGÓRICAS:")
        print("-" * 40)

        original_shape = df_model.shape
        df_model = pd.get_dummies(df_model, columns=categorical_vars, drop_first=True, dtype=int)
        new_shape = df_model.shape

        print(f"   • Antes: {original_shape}")
        print(f"   • Después: {new_shape}")
        print(f"   • Nuevas columnas creadas: {new_shape[1] - original_shape[1]}")

        # Actualizar lista de variables binarias (One-Hot crea variables binarias)
        new_binary_vars = [col for col in df_model.columns
                          if col not in numerical_vars and col != 'churn'
                          and col not in binary_vars]
        binary_vars.extend(new_binary_vars)

    # 3. VERIFICAR Y LIMPIAR DATOS
    print(f"\n🧹 VERIFICANDO Y LIMPIANDO DATOS:")
    print("-" * 40)

    # Verificar tipos de datos
    print(f"   • Verificando tipos de datos...")
    for col in df_model.columns:
        if col != 'churn':
            if df_model[col].dtype == 'object':
                print(f"   ⚠️ Columna '{col}' sigue siendo object: {df_model[col].unique()[:5]}")

    # Convertir todas las columnas numéricas a float (excepto target)
    for col in df_model.columns:
        if col != 'churn':
            try:
                df_model[col] = pd.to_numeric(df_model[col], errors='coerce')
            except:
                print(f"   ⚠️ No se pudo convertir '{col}' a numérico")

    # Eliminar filas con valores NaN en features (no en target)
    feature_cols = [col for col in df_model.columns if col != 'churn']
    before_dropna = len(df_model)
    df_model = df_model.dropna(subset=feature_cols)
    after_dropna = len(df_model)

    if before_dropna != after_dropna:
        print(f"   🗑️ Eliminadas {before_dropna - after_dropna} filas con valores NaN")

    # 4. SEPARAR FEATURES Y TARGET
    print(f"\n🎯 SEPARANDO FEATURES Y TARGET:")
    print("-" * 40)

    X = df_model.drop('churn', axis=1)
    y = df_model['churn']

    print(f"   • Features (X): {X.shape}")
    print(f"   • Target (y): {y.shape}")
    print(f"   • Balance de clases: {y.value_counts().to_dict()}")

    # 5. DIVISIÓN TRAIN/TEST CON ESTRATIFICACIÓN
    print(f"\n✂️ DIVISIÓN TRAIN/TEST (80/20) CON ESTRATIFICACIÓN:")
    print("-" * 40)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        stratify=y
    )

    print(f"   • X_train: {X_train.shape}")
    print(f"   • X_test: {X_test.shape}")
    print(f"   • y_train balance: {y_train.value_counts().to_dict()}")
    print(f"   • y_test balance: {y_test.value_counts().to_dict()}")

    # 6. IDENTIFICAR VARIABLES NUMÉRICAS PARA ESCALADO
    print(f"\n📏 PREPARANDO ESCALADO DE VARIABLES NUMÉRICAS:")
    print("-" * 40)

    # Filtrar solo variables que realmente son numéricas y están en las originales
    numerical_cols_for_scaling = []
    for col in numerical_vars:
        if col in X_train.columns:
            # Verificar que la columna sea realmente numérica
            if pd.api.types.is_numeric_dtype(X_train[col]):
                numerical_cols_for_scaling.append(col)

    print(f"   • Variables identificadas para escalado: {numerical_cols_for_scaling}")
    print(f"   • Variables binarias (no escalar): {len(binary_vars)} variables")

    # 7. CREAR ESCALADOR (sin aplicar aún)
    scaler = StandardScaler()

    # Ajustar el escalador solo con datos de entrenamiento
    if numerical_cols_for_scaling:
        scaler.fit(X_train[numerical_cols_for_scaling])
        print(f"   • Escalador ajustado en variables: {numerical_cols_for_scaling}")
    else:
        print("   ⚠️ No se encontraron variables numéricas para escalar")

    # 8. VERIFICACIÓN FINAL
    print(f"\n✅ VERIFICACIÓN FINAL:")
    print("-" * 40)

    # Verificar que no hay valores no numéricos
    non_numeric_cols = []
    for col in X_train.columns:
        if not pd.api.types.is_numeric_dtype(X_train[col]):
            non_numeric_cols.append(col)

    if non_numeric_cols:
        print(f"   ⚠️ Columnas no numéricas detectadas: {non_numeric_cols}")
    else:
        print(f"   ✅ Todas las features son numéricas")

    print(f"\n✅ PREPARACIÓN COMPLETADA:")
    print(f"   • Dataset listo para modelos que NO requieren escalado")
    print(f"   • Escalador preparado para modelos que SÍ requieren escalado")
    print(f"   • Total features finales: {X_train.shape[1]}")
    print("="*60)

    return {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'scaler': scaler,
        'numerical_cols': numerical_cols_for_scaling,
        'binary_cols': binary_vars,
        'feature_names': X.columns.tolist()
    }

# Ejecutar preparación de datos CORREGIDA
data_prepared = prepare_data_for_modeling(df_cleaned)

"""## 8. Modelo 1: Random Forest (Sin Normalización)

### 🌲 Características del Random Forest:
- **No requiere normalización**: Utiliza árboles de decisión que funcionan con divisiones basadas en umbrales
- **Robusto**: Maneja bien valores atípicos y variables de diferentes escalas
- **Interpretable**: Proporciona importancia de variables fácilmente interpretable
- **Ensemble**: Combina múltiples árboles para reducir overfitting

### 🎯 Configuración del Modelo:
- **n_estimators=100**: Número de árboles en el bosque
- **class_weight='balanced'**: Para manejar el desbalance de clases
- **random_state=42**: Para reproducibilidad
"""

def train_random_forest_model(data_prepared):
    """
    Entrena y evalúa el modelo Random Forest (sin normalización)
    """
    print("🌲 ENTRENANDO MODELO RANDOM FOREST")
    print("="*50)

    # Extraer datos
    X_train = data_prepared['X_train']
    X_test = data_prepared['X_test']
    y_train = data_prepared['y_train']
    y_test = data_prepared['y_test']

    print(f"📊 Datos de entrenamiento: {X_train.shape}")
    print(f"📊 Datos de prueba: {X_test.shape}")

    # 1. ENTRENAR MODELO RANDOM FOREST
    print(f"\n🔧 CONFIGURANDO Y ENTRENANDO RANDOM FOREST:")
    print("-" * 40)

    rf_model = RandomForestClassifier(
        n_estimators=100,           # 100 árboles
        max_depth=10,               # Profundidad máxima para evitar overfitting
        min_samples_split=20,       # Mínimo de muestras para dividir
        min_samples_leaf=10,        # Mínimo de muestras en hoja
        class_weight='balanced',    # Balancear clases automáticamente
        random_state=42,            # Reproducibilidad
        n_jobs=-1                   # Usar todos los procesadores
    )

    # Entrenar el modelo
    print("   🏋️ Entrenando modelo...")
    rf_model.fit(X_train, y_train)
    print("   ✅ Entrenamiento completado")

    # 2. REALIZAR PREDICCIONES
    print(f"\n🔮 REALIZANDO PREDICCIONES:")
    print("-" * 40)

    # Predicciones en conjunto de entrenamiento
    y_train_pred = rf_model.predict(X_train)
    y_train_pred_proba = rf_model.predict_proba(X_train)[:, 1]

    # Predicciones en conjunto de prueba
    y_test_pred = rf_model.predict(X_test)
    y_test_pred_proba = rf_model.predict_proba(X_test)[:, 1]

    print("   ✅ Predicciones completadas")

    # 3. EVALUAR RENDIMIENTO
    print(f"\n📈 EVALUACIÓN DEL RENDIMIENTO:")
    print("-" * 40)

    # Métricas en conjunto de entrenamiento
    train_accuracy = accuracy_score(y_train, y_train_pred)
    train_precision = precision_score(y_train, y_train_pred)
    train_recall = recall_score(y_train, y_train_pred)
    train_f1 = f1_score(y_train, y_train_pred)
    train_auc = roc_auc_score(y_train, y_train_pred_proba)

    # Métricas en conjunto de prueba
    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_precision = precision_score(y_test, y_test_pred)
    test_recall = recall_score(y_test, y_test_pred)
    test_f1 = f1_score(y_test, y_test_pred)
    test_auc = roc_auc_score(y_test, y_test_pred_proba)

    print(f"📊 MÉTRICAS DE ENTRENAMIENTO:")
    print(f"   • Accuracy:  {train_accuracy:.4f}")
    print(f"   • Precision: {train_precision:.4f}")
    print(f"   • Recall:    {train_recall:.4f}")
    print(f"   • F1-Score:  {train_f1:.4f}")
    print(f"   • AUC-ROC:   {train_auc:.4f}")

    print(f"\n📊 MÉTRICAS DE PRUEBA:")
    print(f"   • Accuracy:  {test_accuracy:.4f}")
    print(f"   • Precision: {test_precision:.4f}")
    print(f"   • Recall:    {test_recall:.4f}")
    print(f"   • F1-Score:  {test_f1:.4f}")
    print(f"   • AUC-ROC:   {test_auc:.4f}")

    # Evaluación de overfitting
    overfitting_accuracy = train_accuracy - test_accuracy
    overfitting_f1 = train_f1 - test_f1

    print(f"\n🔍 ANÁLISIS DE OVERFITTING:")
    print(f"   • Diferencia Accuracy: {overfitting_accuracy:.4f}")
    print(f"   • Diferencia F1-Score: {overfitting_f1:.4f}")

    if overfitting_accuracy > 0.05 or overfitting_f1 > 0.05:
        print("   ⚠️ Posible overfitting detectado")
    else:
        print("   ✅ Modelo bien generalizado")

    # 4. VALIDACIÓN CRUZADA
    print(f"\n🔄 VALIDACIÓN CRUZADA (5-FOLD):")
    print("-" * 40)

    # Configurar validación cruzada estratificada
    cv_scores = cross_val_score(rf_model, X_train, y_train,
                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
                               scoring='f1')

    print(f"   • Scores F1 por fold: {[f'{score:.4f}' for score in cv_scores]}")
    print(f"   • Media F1-Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

    # 5. IMPORTANCIA DE VARIABLES
    print(f"\n🏆 TOP 10 VARIABLES MÁS IMPORTANTES:")
    print("-" * 40)

    feature_importance = pd.DataFrame({
        'feature': data_prepared['feature_names'],
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)

    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['feature']:20} → {row['importance']:.4f}")

    print("="*50)

    return {
        'model': rf_model,
        'feature_importance': feature_importance,
        'metrics': {
            'train': {
                'accuracy': train_accuracy,
                'precision': train_precision,
                'recall': train_recall,
                'f1': train_f1,
                'auc': train_auc
            },
            'test': {
                'accuracy': test_accuracy,
                'precision': test_precision,
                'recall': test_recall,
                'f1': test_f1,
                'auc': test_auc
            }
        },
        'predictions': {
            'y_test_pred': y_test_pred,
            'y_test_pred_proba': y_test_pred_proba
        },
        'cv_scores': cv_scores
    }

# Entrenar Random Forest
rf_results = train_random_forest_model(data_prepared)

"""## 9. Modelo 2: Regresión Logística (Con Normalización)

### 📊 Características de la Regresión Logística:
- **Requiere normalización**: Es sensible a la escala de las variables porque utiliza optimización basada en gradientes
- **Lineal**: Asume relaciones lineales entre variables y el log-odds del target
- **Interpretable**: Los coeficientes son fácilmente interpretables
- **Probabilístico**: Proporciona probabilidades bien calibradas

### 🎯 Justificación de la Normalización:
La normalización es **crítica** para la Regresión Logística porque:
1. **Convergencia**: Variables no escaladas pueden hacer que el algoritmo converja lentamente o no converja
2. **Coeficientes**: Variables con mayor escala pueden dominar la función de costo
3. **Regularización**: La regularización L1/L2 afecta de manera desigual a variables de diferentes escalas
4. **Interpretación**: Los coeficientes normalizados son comparables entre sí
"""

def train_logistic_regression_model(data_prepared):
    """
    Entrena y evalúa el modelo de Regresión Logística (con normalización)
    """
    print("📊 ENTRENANDO MODELO REGRESIÓN LOGÍSTICA")
    print("="*50)

    # Extraer datos
    X_train = data_prepared['X_train'].copy()
    X_test = data_prepared['X_test'].copy()
    y_train = data_prepared['y_train']
    y_test = data_prepared['y_test']
    scaler = data_prepared['scaler']
    numerical_cols = data_prepared['numerical_cols']

    print(f"📊 Datos de entrenamiento: {X_train.shape}")
    print(f"📊 Datos de prueba: {X_test.shape}")

    # 1. APLICAR NORMALIZACIÓN A VARIABLES NUMÉRICAS
    print(f"\n🔧 APLICANDO NORMALIZACIÓN (StandardScaler):")
    print("-" * 40)

    if numerical_cols:
        print(f"   📏 Variables a normalizar: {numerical_cols}")

        # Aplicar escalado a datos de entrenamiento
        X_train_scaled = X_train.copy()
        X_train_scaled[numerical_cols] = scaler.transform(X_train[numerical_cols])

        # Aplicar escalado a datos de prueba (usando el mismo escalador)
        X_test_scaled = X_test.copy()
        X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])

        print(f"   ✅ Normalización aplicada")

        # Mostrar estadísticas antes y después del escalado
        print(f"\n   📈 ESTADÍSTICAS ANTES DEL ESCALADO (muestra):")
        for col in numerical_cols[:2]:  # Mostrar solo las primeras 2 columnas
            mean_before = X_train[col].mean()
            std_before = X_train[col].std()
            print(f"      • {col}: μ={mean_before:.2f}, σ={std_before:.2f}")

        print(f"\n   📉 ESTADÍSTICAS DESPUÉS DEL ESCALADO (muestra):")
        for col in numerical_cols[:2]:  # Mostrar solo las primeras 2 columnas
            mean_after = X_train_scaled[col].mean()
            std_after = X_train_scaled[col].std()
            print(f"      • {col}: μ={mean_after:.2f}, σ={std_after:.2f}")
    else:
        print("   ⚠️ No hay variables numéricas para normalizar")
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()

    # 2. ENTRENAR MODELO REGRESIÓN LOGÍSTICA
    print(f"\n🔧 CONFIGURANDO Y ENTRENANDO REGRESIÓN LOGÍSTICA:")
    print("-" * 40)

    lr_model = LogisticRegression(
        C=1.0,                      # Parámetro de regularización
        penalty='l2',               # Regularización L2 (Ridge)
        class_weight='balanced',    # Balancear clases automáticamente
        random_state=42,            # Reproducibilidad
        max_iter=1000,              # Máximo número de iteraciones
        solver='lbfgs'              # Solver para optimización
    )

    # Entrenar el modelo
    print("   🏋️ Entrenando modelo...")
    lr_model.fit(X_train_scaled, y_train)
    print("   ✅ Entrenamiento completado")

    # 3. REALIZAR PREDICCIONES
    print(f"\n🔮 REALIZANDO PREDICCIONES:")
    print("-" * 40)

    # Predicciones en conjunto de entrenamiento
    y_train_pred = lr_model.predict(X_train_scaled)
    y_train_pred_proba = lr_model.predict_proba(X_train_scaled)[:, 1]

    # Predicciones en conjunto de prueba
    y_test_pred = lr_model.predict(X_test_scaled)
    y_test_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]

    print("   ✅ Predicciones completadas")

    # 4. EVALUAR RENDIMIENTO
    print(f"\n📈 EVALUACIÓN DEL RENDIMIENTO:")
    print("-" * 40)

    # Métricas en conjunto de entrenamiento
    train_accuracy = accuracy_score(y_train, y_train_pred)
    train_precision = precision_score(y_train, y_train_pred)
    train_recall = recall_score(y_train, y_train_pred)
    train_f1 = f1_score(y_train, y_train_pred)
    train_auc = roc_auc_score(y_train, y_train_pred_proba)

    # Métricas en conjunto de prueba
    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_precision = precision_score(y_test, y_test_pred)
    test_recall = recall_score(y_test, y_test_pred)
    test_f1 = f1_score(y_test, y_test_pred)
    test_auc = roc_auc_score(y_test, y_test_pred_proba)

    print(f"📊 MÉTRICAS DE ENTRENAMIENTO:")
    print(f"   • Accuracy:  {train_accuracy:.4f}")
    print(f"   • Precision: {train_precision:.4f}")
    print(f"   • Recall:    {train_recall:.4f}")
    print(f"   • F1-Score:  {train_f1:.4f}")
    print(f"   • AUC-ROC:   {train_auc:.4f}")

    print(f"\n📊 MÉTRICAS DE PRUEBA:")
    print(f"   • Accuracy:  {test_accuracy:.4f}")
    print(f"   • Precision: {test_precision:.4f}")
    print(f"   • Recall:    {test_recall:.4f}")
    print(f"   • F1-Score:  {test_f1:.4f}")
    print(f"   • AUC-ROC:   {test_auc:.4f}")

    # Evaluación de overfitting
    overfitting_accuracy = train_accuracy - test_accuracy
    overfitting_f1 = train_f1 - test_f1

    print(f"\n🔍 ANÁLISIS DE OVERFITTING:")
    print(f"   • Diferencia Accuracy: {overfitting_accuracy:.4f}")
    print(f"   • Diferencia F1-Score: {overfitting_f1:.4f}")

    if overfitting_accuracy > 0.05 or overfitting_f1 > 0.05:
        print("   ⚠️ Posible overfitting detectado")
    else:
        print("   ✅ Modelo bien generalizado")

    # 5. VALIDACIÓN CRUZADA
    print(f"\n🔄 VALIDACIÓN CRUZADA (5-FOLD):")
    print("-" * 40)

    # Configurar validación cruzada estratificada
    cv_scores = cross_val_score(lr_model, X_train_scaled, y_train,
                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
                               scoring='f1')

    print(f"   • Scores F1 por fold: {[f'{score:.4f}' for score in cv_scores]}")
    print(f"   • Media F1-Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

    # 6. ANÁLISIS DE COEFICIENTES
    print(f"\n🏆 TOP 10 COEFICIENTES MÁS IMPORTANTES:")
    print("-" * 40)

    # Obtener coeficientes del modelo
    feature_coefficients = pd.DataFrame({
        'feature': data_prepared['feature_names'],
        'coefficient': lr_model.coef_[0],
        'abs_coefficient': np.abs(lr_model.coef_[0])
    }).sort_values('abs_coefficient', ascending=False)

    for i, (_, row) in enumerate(feature_coefficients.head(10).iterrows(), 1):
        direction = "📈 Positivo" if row['coefficient'] > 0 else "📉 Negativo"
        print(f"   {i:2d}. {row['feature']:20} → {row['coefficient']:7.4f} ({direction})")

    print("="*50)

    return {
        'model': lr_model,
        'scaler': scaler,
        'feature_coefficients': feature_coefficients,
        'X_train_scaled': X_train_scaled,
        'X_test_scaled': X_test_scaled,
        'metrics': {
            'train': {
                'accuracy': train_accuracy,
                'precision': train_precision,
                'recall': train_recall,
                'f1': train_f1,
                'auc': train_auc
            },
            'test': {
                'accuracy': test_accuracy,
                'precision': test_precision,
                'recall': test_recall,
                'f1': test_f1,
                'auc': test_auc
            }
        },
        'predictions': {
            'y_test_pred': y_test_pred,
            'y_test_pred_proba': y_test_pred_proba
        },
        'cv_scores': cv_scores
    }

# Entrenar Regresión Logística
lr_results = train_logistic_regression_model(data_prepared)

"""## 10. Comparación de Modelos y Visualizaciones

En esta sección compararemos el rendimiento de ambos modelos y crearemos visualizaciones para evaluar:

### 📊 Métricas de Comparación:
- **Accuracy, Precision, Recall, F1-Score**: Para evaluar rendimiento general
- **AUC-ROC**: Para evaluar capacidad de discriminación
- **Matrices de Confusión**: Para entender errores específicos
- **Curvas ROC**: Para visualizar trade-off entre sensibilidad y especificidad

### 🎯 Interpretabilidad:
- **Random Forest**: Importancia de variables basada en reducción de impureza
- **Regresión Logística**: Coeficientes que indican dirección y magnitud del efecto
"""

def compare_models_and_visualize(rf_results, lr_results, data_prepared):
    """
    Compara ambos modelos y crea visualizaciones comprehensivas
    """
    print("⚖️ COMPARACIÓN DETALLADA DE MODELOS")
    print("="*60)

    # 1. TABLA COMPARATIVA DE MÉTRICAS
    print("\n📊 TABLA COMPARATIVA DE RENDIMIENTO:")
    print("-" * 60)

    # Crear DataFrame comparativo
    comparison_df = pd.DataFrame({
        'Métrica': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],
        'Random Forest': [
            rf_results['metrics']['test']['accuracy'],
            rf_results['metrics']['test']['precision'],
            rf_results['metrics']['test']['recall'],
            rf_results['metrics']['test']['f1'],
            rf_results['metrics']['test']['auc']
        ],
        'Regresión Logística': [
            lr_results['metrics']['test']['accuracy'],
            lr_results['metrics']['test']['precision'],
            lr_results['metrics']['test']['recall'],
            lr_results['metrics']['test']['f1'],
            lr_results['metrics']['test']['auc']
        ]
    })

    # Calcular diferencias
    comparison_df['Diferencia (RF - LR)'] = (
        comparison_df['Random Forest'] - comparison_df['Regresión Logística']
    )

    print(comparison_df.round(4).to_string(index=False))

    # 2. ANÁLISIS COMPARATIVO
    print(f"\n🏆 ANÁLISIS COMPARATIVO:")
    print("-" * 40)

    # Determinar el mejor modelo por métrica
    best_accuracy = "Random Forest" if rf_results['metrics']['test']['accuracy'] > lr_results['metrics']['test']['accuracy'] else "Regresión Logística"
    best_f1 = "Random Forest" if rf_results['metrics']['test']['f1'] > lr_results['metrics']['test']['f1'] else "Regresión Logística"
    best_auc = "Random Forest" if rf_results['metrics']['test']['auc'] > lr_results['metrics']['test']['auc'] else "Regresión Logística"

    print(f"   • Mejor Accuracy: {best_accuracy}")
    print(f"   • Mejor F1-Score: {best_f1}")
    print(f"   • Mejor AUC-ROC: {best_auc}")

    # Validación cruzada
    rf_cv_mean = rf_results['cv_scores'].mean()
    lr_cv_mean = lr_results['cv_scores'].mean()
    best_cv = "Random Forest" if rf_cv_mean > lr_cv_mean else "Regresión Logística"

    print(f"   • Mejor CV F1-Score: {best_cv} ({rf_cv_mean:.4f} vs {lr_cv_mean:.4f})")

    # 3. CREAR VISUALIZACIONES
    print(f"\n🎨 CREANDO VISUALIZACIONES COMPARATIVAS...")
    print("-" * 40)

    # Configurar la figura con múltiples subplots
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('🔍 Comparación Exhaustiva de Modelos: Random Forest vs Regresión Logística',
                fontsize=16, fontweight='bold', y=0.98)

    # SUBPLOT 1: Comparación de métricas
    ax1 = axes[0, 0]
    x_pos = np.arange(len(comparison_df))
    width = 0.35

    ax1.bar(x_pos - width/2, comparison_df['Random Forest'], width,
           label='Random Forest', color='#2ECC71', alpha=0.8)
    ax1.bar(x_pos + width/2, comparison_df['Regresión Logística'], width,
           label='Regresión Logística', color='#3498DB', alpha=0.8)

    ax1.set_xlabel('Métricas')
    ax1.set_ylabel('Valor')
    ax1.set_title('📊 Comparación de Métricas', fontweight='bold')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(comparison_df['Métrica'], rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # SUBPLOT 2: Matrices de Confusión - Random Forest
    ax2 = axes[0, 1]
    y_test = data_prepared['y_test']
    rf_cm = confusion_matrix(y_test, rf_results['predictions']['y_test_pred'])
    sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=ax2)
    ax2.set_title('🌲 Matriz de Confusión\nRandom Forest', fontweight='bold')
    ax2.set_xlabel('Predicción')
    ax2.set_ylabel('Real')

    # SUBPLOT 3: Matrices de Confusión - Regresión Logística
    ax3 = axes[0, 2]
    lr_cm = confusion_matrix(y_test, lr_results['predictions']['y_test_pred'])
    sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Oranges', ax=ax3)
    ax3.set_title('📊 Matriz de Confusión\nRegresión Logística', fontweight='bold')
    ax3.set_xlabel('Predicción')
    ax3.set_ylabel('Real')

    # SUBPLOT 4: Curvas ROC
    ax4 = axes[1, 0]

    # Random Forest ROC
    rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_results['predictions']['y_test_pred_proba'])
    rf_auc = rf_results['metrics']['test']['auc']

    # Regresión Logística ROC
    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_results['predictions']['y_test_pred_proba'])
    lr_auc = lr_results['metrics']['test']['auc']

    ax4.plot(rf_fpr, rf_tpr, color='#2ECC71', lw=2,
            label=f'Random Forest (AUC = {rf_auc:.3f})')
    ax4.plot(lr_fpr, lr_tpr, color='#3498DB', lw=2,
            label=f'Regresión Logística (AUC = {lr_auc:.3f})')
    ax4.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)

    ax4.set_xlim([0.0, 1.0])
    ax4.set_ylim([0.0, 1.05])
    ax4.set_xlabel('Tasa de Falsos Positivos')
    ax4.set_ylabel('Tasa de Verdaderos Positivos')
    ax4.set_title('📈 Curvas ROC', fontweight='bold')
    ax4.legend(loc="lower right")
    ax4.grid(True, alpha=0.3)

    # SUBPLOT 5: Importancia de Variables (Random Forest)
    ax5 = axes[1, 1]
    top_features_rf = rf_results['feature_importance'].head(10)
    ax5.barh(range(len(top_features_rf)), top_features_rf['importance'],
            color='#2ECC71', alpha=0.8)
    ax5.set_yticks(range(len(top_features_rf)))
    ax5.set_yticklabels(top_features_rf['feature'])
    ax5.set_xlabel('Importancia')
    ax5.set_title('🌲 Top 10 Variables\n(Random Forest)', fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='x')

    # SUBPLOT 6: Coeficientes (Regresión Logística)
    ax6 = axes[1, 2]
    top_features_lr = lr_results['feature_coefficients'].head(10)
    colors = ['#E74C3C' if x < 0 else '#3498DB' for x in top_features_lr['coefficient']]
    ax6.barh(range(len(top_features_lr)), top_features_lr['coefficient'],
            color=colors, alpha=0.8)
    ax6.set_yticks(range(len(top_features_lr)))
    ax6.set_yticklabels(top_features_lr['feature'])
    ax6.set_xlabel('Coeficiente')
    ax6.set_title('📊 Top 10 Coeficientes\n(Regresión Logística)', fontweight='bold')
    ax6.grid(True, alpha=0.3, axis='x')
    ax6.axvline(x=0, color='black', linestyle='-', alpha=0.3)

    plt.tight_layout()
    plt.show()

    # 4. ANÁLISIS DE VARIABLES IMPORTANTES
    print(f"\n🔍 ANÁLISIS DE VARIABLES MÁS IMPORTANTES:")
    print("-" * 60)

    # Variables importantes en Random Forest
    print(f"🌲 TOP 5 VARIABLES MÁS IMPORTANTES (Random Forest):")
    for i, (_, row) in enumerate(rf_results['feature_importance'].head(5).iterrows(), 1):
        print(f"   {i}. {row['feature']:25} → {row['importance']:.4f}")

    print(f"\n📊 TOP 5 COEFICIENTES MÁS GRANDES (Regresión Logística):")
    for i, (_, row) in enumerate(lr_results['feature_coefficients'].head(5).iterrows(), 1):
        direction = "⬆️ Incrementa" if row['coefficient'] > 0 else "⬇️ Reduce"
        print(f"   {i}. {row['feature']:25} → {row['coefficient']:7.4f} ({direction} churn)")

    # 5. RECOMENDACIÓN FINAL
    print(f"\n🏅 RECOMENDACIÓN FINAL:")
    print("-" * 40)

    # Determinar el mejor modelo basado en múltiples métricas
    rf_score = (rf_results['metrics']['test']['f1'] + rf_results['metrics']['test']['auc']) / 2
    lr_score = (lr_results['metrics']['test']['f1'] + lr_results['metrics']['test']['auc']) / 2

    if rf_score > lr_score:
        recommended_model = "Random Forest"
        winner_reason = f"Mejor rendimiento promedio (F1+AUC)/2: {rf_score:.4f} vs {lr_score:.4f}"
    else:
        recommended_model = "Regresión Logística"
        winner_reason = f"Mejor rendimiento promedio (F1+AUC)/2: {lr_score:.4f} vs {rf_score:.4f}"

    print(f"   🏆 Modelo Recomendado: {recommended_model}")
    print(f"   📈 Razón: {winner_reason}")

    if recommended_model == "Random Forest":
        print(f"   💡 Ventajas: Robusto, maneja no-linealidades, no requiere normalización")
        print(f"   ⚠️ Consideraciones: Menos interpretable, puede hacer overfitting")
    else:
        print(f"   💡 Ventajas: Altamente interpretable, probabilidades calibradas, rápido")
        print(f"   ⚠️ Consideraciones: Asume linealidad, sensible a outliers, requiere normalización")

    print("="*60)

    return comparison_df, recommended_model

# Ejecutar comparación
comparison_results, best_model = compare_models_and_visualize(rf_results, lr_results, data_prepared)

"""## 12. Evaluación Detallada y Análisis Crítico de Modelos

En esta sección realizaremos una evaluación exhaustiva de ambos modelos utilizando múltiples métricas y análisis crítico del rendimiento, incluyendo detección de overfitting/underfitting.

### 📊 Métricas de Evaluación:
- **Exactitud (Accuracy)**: Proporción de predicciones correctas
- **Precisión (Precision)**: Proporción de verdaderos positivos entre todas las predicciones positivas
- **Recall (Sensibilidad)**: Proporción de verdaderos positivos detectados
- **F1-Score**: Media armónica entre precisión y recall
- **Matriz de Confusión**: Análisis detallado de errores de clasificación

### 🔍 Análisis de Overfitting/Underfitting:
- **Overfitting**: Modelo aprende demasiado del entrenamiento, no generaliza
- **Underfitting**: Modelo muy simple, no captura patrones importantes
"""

def comprehensive_model_evaluation(rf_results, lr_results, data_prepared):
    """
    Evaluación exhaustiva y análisis crítico de ambos modelos
    """
    print("🔬 EVALUACIÓN EXHAUSTIVA Y ANÁLISIS CRÍTICO DE MODELOS")
    print("="*80)

    # Extraer datos necesarios
    y_test = data_prepared['y_test']
    y_train = data_prepared['y_train']

    # 1. TABLA DETALLADA DE MÉTRICAS
    print("\n📊 TABLA DETALLADA DE MÉTRICAS DE RENDIMIENTO")
    print("-" * 80)

    # Crear tabla comprehensiva
    metrics_comparison = pd.DataFrame({
        'Métrica': [
            'Exactitud (Accuracy)', 'Precisión (Precision)', 'Recall (Sensibilidad)',
            'F1-Score', 'AUC-ROC', 'Especificidad', 'Valor Predictivo Negativo'
        ],
        'Random Forest (Test)': [
            rf_results['metrics']['test']['accuracy'],
            rf_results['metrics']['test']['precision'],
            rf_results['metrics']['test']['recall'],
            rf_results['metrics']['test']['f1'],
            rf_results['metrics']['test']['auc'],
            # Calcular especificidad y VPN
            rf_results['metrics']['test']['accuracy'],  # Placeholder
            rf_results['metrics']['test']['accuracy']   # Placeholder
        ],
        'Regresión Logística (Test)': [
            lr_results['metrics']['test']['accuracy'],
            lr_results['metrics']['test']['precision'],
            lr_results['metrics']['test']['recall'],
            lr_results['metrics']['test']['f1'],
            lr_results['metrics']['test']['auc'],
            # Calcular especificidad y VPN
            lr_results['metrics']['test']['accuracy'],  # Placeholder
            lr_results['metrics']['test']['accuracy']   # Placeholder
        ]
    })

    # Calcular métricas adicionales usando matrices de confusión
    rf_cm = confusion_matrix(y_test, rf_results['predictions']['y_test_pred'])
    lr_cm = confusion_matrix(y_test, lr_results['predictions']['y_test_pred'])

    # Para Random Forest
    rf_tn, rf_fp, rf_fn, rf_tp = rf_cm.ravel()
    rf_specificity = rf_tn / (rf_tn + rf_fp) if (rf_tn + rf_fp) > 0 else 0
    rf_npv = rf_tn / (rf_tn + rf_fn) if (rf_tn + rf_fn) > 0 else 0

    # Para Regresión Logística
    lr_tn, lr_fp, lr_fn, lr_tp = lr_cm.ravel()
    lr_specificity = lr_tn / (lr_tn + lr_fp) if (lr_tn + lr_fp) > 0 else 0
    lr_npv = lr_tn / (lr_tn + lr_fn) if (lr_tn + lr_fn) > 0 else 0

    # Actualizar tabla con métricas calculadas
    metrics_comparison.loc[5, 'Random Forest (Test)'] = rf_specificity
    metrics_comparison.loc[6, 'Random Forest (Test)'] = rf_npv
    metrics_comparison.loc[5, 'Regresión Logística (Test)'] = lr_specificity
    metrics_comparison.loc[6, 'Regresión Logística (Test)'] = lr_npv

    # Calcular diferencias
    metrics_comparison['Diferencia (RF - LR)'] = (
        metrics_comparison['Random Forest (Test)'] - metrics_comparison['Regresión Logística (Test)']
    )

    # Determinar ganador por métrica
    metrics_comparison['Ganador'] = metrics_comparison['Diferencia (RF - LR)'].apply(
        lambda x: '🏆 Random Forest' if x > 0 else '🏆 Reg. Logística' if x < 0 else '🤝 Empate'
    )

    print(metrics_comparison.round(4).to_string(index=False))

    # 2. ANÁLISIS DETALLADO DE MATRICES DE CONFUSIÓN
    print(f"\n🎯 ANÁLISIS DETALLADO DE MATRICES DE CONFUSIÓN")
    print("-" * 80)

    print("🌲 RANDOM FOREST - Matriz de Confusión:")
    print(f"   Verdaderos Negativos (TN): {rf_tn:4d} | Falsos Positivos (FP): {rf_fp:4d}")
    print(f"   Falsos Negativos (FN):     {rf_fn:4d} | Verdaderos Positivos (TP): {rf_tp:4d}")
    print(f"   Total predicciones correctas: {rf_tn + rf_tp:4d} de {len(y_test):4d} ({((rf_tn + rf_tp)/len(y_test)*100):.1f}%)")

    print("\n📊 REGRESIÓN LOGÍSTICA - Matriz de Confusión:")
    print(f"   Verdaderos Negativos (TN): {lr_tn:4d} | Falsos Positivos (FP): {lr_fp:4d}")
    print(f"   Falsos Negativos (FN):     {lr_fn:4d} | Verdaderos Positivos (TP): {lr_tp:4d}")
    print(f"   Total predicciones correctas: {lr_tn + lr_tp:4d} de {len(y_test):4d} ({((lr_tn + lr_tp)/len(y_test)*100):.1f}%)")

    # 3. ANÁLISIS DE OVERFITTING/UNDERFITTING
    print(f"\n🔍 ANÁLISIS CRÍTICO: OVERFITTING Y UNDERFITTING")
    print("-" * 80)

    # Random Forest - Análisis de overfitting
    rf_train_acc = rf_results['metrics']['train']['accuracy']
    rf_test_acc = rf_results['metrics']['test']['accuracy']
    rf_train_f1 = rf_results['metrics']['train']['f1']
    rf_test_f1 = rf_results['metrics']['test']['f1']
    rf_cv_f1_mean = rf_results['cv_scores'].mean()
    rf_cv_f1_std = rf_results['cv_scores'].std()

    print("🌲 RANDOM FOREST:")
    print(f"   📈 Accuracy:  Entrenamiento={rf_train_acc:.4f} | Test={rf_test_acc:.4f} | Diferencia={rf_train_acc-rf_test_acc:.4f}")
    print(f"   📈 F1-Score:  Entrenamiento={rf_train_f1:.4f} | Test={rf_test_f1:.4f} | Diferencia={rf_train_f1-rf_test_f1:.4f}")
    print(f"   📈 CV F1:     Media={rf_cv_f1_mean:.4f} ± {rf_cv_f1_std:.4f}")

    # Diagnóstico Random Forest
    rf_overfit_score = (rf_train_acc - rf_test_acc) + (rf_train_f1 - rf_test_f1)
    if rf_overfit_score > 0.1:
        rf_diagnosis = "🔴 OVERFITTING DETECTADO"
        rf_recommendation = "Reducir max_depth, aumentar min_samples_split/leaf, usar menos estimadores"
    elif rf_test_acc < 0.7 or rf_test_f1 < 0.5:
        rf_diagnosis = "🔵 POSIBLE UNDERFITTING"
        rf_recommendation = "Aumentar max_depth, reducir min_samples_split/leaf, más estimadores"
    else:
        rf_diagnosis = "✅ MODELO BIEN BALANCEADO"
        rf_recommendation = "Modelo generaliza adecuadamente"

    print(f"   🎯 Diagnóstico: {rf_diagnosis}")
    print(f"   💡 Recomendación: {rf_recommendation}")

    # Regresión Logística - Análisis de overfitting
    lr_train_acc = lr_results['metrics']['train']['accuracy']
    lr_test_acc = lr_results['metrics']['test']['accuracy']
    lr_train_f1 = lr_results['metrics']['train']['f1']
    lr_test_f1 = lr_results['metrics']['test']['f1']
    lr_cv_f1_mean = lr_results['cv_scores'].mean()
    lr_cv_f1_std = lr_results['cv_scores'].std()

    print(f"\n📊 REGRESIÓN LOGÍSTICA:")
    print(f"   📈 Accuracy:  Entrenamiento={lr_train_acc:.4f} | Test={lr_test_acc:.4f} | Diferencia={lr_train_acc-lr_test_acc:.4f}")
    print(f"   📈 F1-Score:  Entrenamiento={lr_train_f1:.4f} | Test={lr_test_f1:.4f} | Diferencia={lr_train_f1-lr_test_f1:.4f}")
    print(f"   📈 CV F1:     Media={lr_cv_f1_mean:.4f} ± {lr_cv_f1_std:.4f}")

    # Diagnóstico Regresión Logística
    lr_overfit_score = (lr_train_acc - lr_test_acc) + (lr_train_f1 - lr_test_f1)
    if lr_overfit_score > 0.1:
        lr_diagnosis = "🔴 OVERFITTING DETECTADO"
        lr_recommendation = "Aumentar regularización (reducir C), usar L1 penalty, feature selection"
    elif lr_test_acc < 0.7 or lr_test_f1 < 0.5:
        lr_diagnosis = "🔵 POSIBLE UNDERFITTING"
        lr_recommendation = "Reducir regularización (aumentar C), agregar features polinomiales"
    else:
        lr_diagnosis = "✅ MODELO BIEN BALANCEADO"
        lr_recommendation = "Modelo generaliza adecuadamente"

    print(f"   🎯 Diagnóstico: {lr_diagnosis}")
    print(f"   💡 Recomendación: {lr_recommendation}")

    # 4. ANÁLISIS DE ESTABILIDAD (Validación Cruzada)
    print(f"\n📊 ANÁLISIS DE ESTABILIDAD (VALIDACIÓN CRUZADA)")
    print("-" * 80)

    print(f"🌲 Random Forest:")
    print(f"   F1-Scores por fold: {[f'{score:.3f}' for score in rf_results['cv_scores']]}")
    print(f"   Variabilidad: {rf_cv_f1_std:.4f} ({'ESTABLE' if rf_cv_f1_std < 0.05 else 'INESTABLE'})")

    print(f"\n📊 Regresión Logística:")
    print(f"   F1-Scores por fold: {[f'{score:.3f}' for score in lr_results['cv_scores']]}")
    print(f"   Variabilidad: {lr_cv_f1_std:.4f} ({'ESTABLE' if lr_cv_f1_std < 0.05 else 'INESTABLE'})")

    # 5. DETERMINACIÓN DEL MEJOR MODELO
    print(f"\n🏆 DETERMINACIÓN DEL MEJOR MODELO")
    print("-" * 80)

    # Sistema de puntuación multi-criterio
    rf_score = (
        rf_results['metrics']['test']['accuracy'] * 0.2 +
        rf_results['metrics']['test']['precision'] * 0.2 +
        rf_results['metrics']['test']['recall'] * 0.2 +
        rf_results['metrics']['test']['f1'] * 0.25 +
        rf_results['metrics']['test']['auc'] * 0.15
    )

    lr_score = (
        lr_results['metrics']['test']['accuracy'] * 0.2 +
        lr_results['metrics']['test']['precision'] * 0.2 +
        lr_results['metrics']['test']['recall'] * 0.2 +
        lr_results['metrics']['test']['f1'] * 0.25 +
        lr_results['metrics']['test']['auc'] * 0.15
    )

    # Penalizar por overfitting
    rf_penalty = max(0, rf_overfit_score * 0.1)
    lr_penalty = max(0, lr_overfit_score * 0.1)

    rf_final_score = rf_score - rf_penalty
    lr_final_score = lr_score - lr_penalty

    print(f"🌲 Random Forest:")
    print(f"   Score base: {rf_score:.4f}")
    print(f"   Penalización overfitting: {rf_penalty:.4f}")
    print(f"   Score final: {rf_final_score:.4f}")

    print(f"\n📊 Regresión Logística:")
    print(f"   Score base: {lr_score:.4f}")
    print(f"   Penalización overfitting: {lr_penalty:.4f}")
    print(f"   Score final: {lr_final_score:.4f}")

    # Determinar ganador
    if rf_final_score > lr_final_score:
        winner = "Random Forest"
        advantage = rf_final_score - lr_final_score
    else:
        winner = "Regresión Logística"
        advantage = lr_final_score - rf_final_score

    print(f"\n🏅 MODELO GANADOR: {winner}")
    print(f"📈 Ventaja: {advantage:.4f} puntos")

    # 6. ANÁLISIS DE ERRORES ESPECÍFICOS
    print(f"\n🎯 ANÁLISIS DE ERRORES ESPECÍFICOS")
    print("-" * 80)

    # Análisis para Random Forest
    rf_fpr = rf_fp / (rf_fp + rf_tn) if (rf_fp + rf_tn) > 0 else 0  # Tasa Falsos Positivos
    rf_fnr = rf_fn / (rf_fn + rf_tp) if (rf_fn + rf_tp) > 0 else 0  # Tasa Falsos Negativos

    print(f"🌲 Random Forest - Análisis de Errores:")
    print(f"   Tasa Falsos Positivos: {rf_fpr:.3f} ({rf_fp} de {rf_fp + rf_tn})")
    print(f"   Tasa Falsos Negativos:  {rf_fnr:.3f} ({rf_fn} de {rf_fn + rf_tp})")
    print(f"   Impacto negocio FP: {rf_fp} clientes mal clasificados como churn")
    print(f"   Impacto negocio FN: {rf_fn} churns no detectados")

    # Análisis para Regresión Logística
    lr_fpr = lr_fp / (lr_fp + lr_tn) if (lr_fp + lr_tn) > 0 else 0
    lr_fnr = lr_fn / (lr_fn + lr_tp) if (lr_fn + lr_tp) > 0 else 0

    print(f"\n📊 Regresión Logística - Análisis de Errores:")
    print(f"   Tasa Falsos Positivos: {lr_fpr:.3f} ({lr_fp} de {lr_fp + lr_tn})")
    print(f"   Tasa Falsos Negativos:  {lr_fnr:.3f} ({lr_fn} de {lr_fn + lr_tp})")
    print(f"   Impacto negocio FP: {lr_fp} clientes mal clasificados como churn")
    print(f"   Impacto negocio FN: {lr_fn} churns no detectados")

    print("="*80)

    return {
        'metrics_comparison': metrics_comparison,
        'winner': winner,
        'rf_diagnosis': rf_diagnosis,
        'lr_diagnosis': lr_diagnosis,
        'rf_recommendations': rf_recommendation,
        'lr_recommendations': lr_recommendation,
        'confusion_matrices': {
            'rf': rf_cm,
            'lr': lr_cm
        },
        'final_scores': {
            'rf': rf_final_score,
            'lr': lr_final_score
        }
    }

# Ejecutar evaluación exhaustiva
evaluation_results = comprehensive_model_evaluation(rf_results, lr_results, data_prepared)

def create_detailed_evaluation_plots(rf_results, lr_results, data_prepared, evaluation_results):
    """
    Crea visualizaciones detalladas para la evaluación de modelos
    """
    print("🎨 CREANDO VISUALIZACIONES DETALLADAS DE EVALUACIÓN")
    print("="*60)

    # Configurar figura con múltiples subplots
    fig = plt.figure(figsize=(20, 16))

    # 1. COMPARACIÓN DE MÉTRICAS (subplot 1)
    ax1 = plt.subplot(3, 3, 1)
    metrics_df = evaluation_results['metrics_comparison'].iloc[:5]  # Primeras 5 métricas

    x_pos = np.arange(len(metrics_df))
    width = 0.35

    bars1 = ax1.bar(x_pos - width/2, metrics_df['Random Forest (Test)'], width,
                   label='Random Forest', color='#2ECC71', alpha=0.8)
    bars2 = ax1.bar(x_pos + width/2, metrics_df['Regresión Logística (Test)'], width,
                   label='Regresión Logística', color='#3498DB', alpha=0.8)

    ax1.set_xlabel('Métricas')
    ax1.set_ylabel('Valor')
    ax1.set_title('📊 Comparación Detallada de Métricas', fontweight='bold')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'], rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Agregar valores en las barras
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)

    # 2. MATRICES DE CONFUSIÓN - Random Forest (subplot 2)
    ax2 = plt.subplot(3, 3, 2)
    rf_cm = evaluation_results['confusion_matrices']['rf']
    sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=ax2,
                xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
    ax2.set_title('🌲 Random Forest\nMatriz de Confusión', fontweight='bold')
    ax2.set_xlabel('Predicción')
    ax2.set_ylabel('Realidad')

    # 3. MATRICES DE CONFUSIÓN - Regresión Logística (subplot 3)
    ax3 = plt.subplot(3, 3, 3)
    lr_cm = evaluation_results['confusion_matrices']['lr']
    sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Oranges', ax=ax3,
                xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
    ax3.set_title('📊 Regresión Logística\nMatriz de Confusión', fontweight='bold')
    ax3.set_xlabel('Predicción')
    ax3.set_ylabel('Realidad')

    # 4. ANÁLISIS TRAIN VS TEST (subplot 4)
    ax4 = plt.subplot(3, 3, 4)

    models = ['Random Forest', 'Regresión Logística']
    train_f1 = [rf_results['metrics']['train']['f1'], lr_results['metrics']['train']['f1']]
    test_f1 = [rf_results['metrics']['test']['f1'], lr_results['metrics']['test']['f1']]

    x_pos = np.arange(len(models))
    width = 0.35

    ax4.bar(x_pos - width/2, train_f1, width, label='Entrenamiento', color='#E74C3C', alpha=0.8)
    ax4.bar(x_pos + width/2, test_f1, width, label='Prueba', color='#27AE60', alpha=0.8)

    ax4.set_xlabel('Modelos')
    ax4.set_ylabel('F1-Score')
    ax4.set_title('🔍 Análisis Overfitting\n(Train vs Test)', fontweight='bold')
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels(models)
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    # 5. CURVAS ROC COMPARATIVAS (subplot 5)
    ax5 = plt.subplot(3, 3, 5)

    y_test = data_prepared['y_test']

    # Random Forest ROC
    rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_results['predictions']['y_test_pred_proba'])
    rf_auc = rf_results['metrics']['test']['auc']

    # Regresión Logística ROC
    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_results['predictions']['y_test_pred_proba'])
    lr_auc = lr_results['metrics']['test']['auc']

    ax5.plot(rf_fpr, rf_tpr, color='#2ECC71', lw=2,
            label=f'Random Forest (AUC = {rf_auc:.3f})')
    ax5.plot(lr_fpr, lr_tpr, color='#3498DB', lw=2,
            label=f'Regresión Logística (AUC = {lr_auc:.3f})')
    ax5.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)

    ax5.set_xlim([0.0, 1.0])
    ax5.set_ylim([0.0, 1.05])
    ax5.set_xlabel('Tasa de Falsos Positivos')
    ax5.set_ylabel('Tasa de Verdaderos Positivos')
    ax5.set_title('📈 Curvas ROC Comparativas', fontweight='bold')
    ax5.legend(loc="lower right")
    ax5.grid(True, alpha=0.3)

    # 6. VALIDACIÓN CRUZADA - Estabilidad (subplot 6)
    ax6 = plt.subplot(3, 3, 6)

    folds = list(range(1, 6))
    ax6.plot(folds, rf_results['cv_scores'], 'o-', color='#2ECC71',
            linewidth=2, markersize=8, label='Random Forest')
    ax6.plot(folds, lr_results['cv_scores'], 's-', color='#3498DB',
            linewidth=2, markersize=8, label='Regresión Logística')

    ax6.axhline(y=rf_results['cv_scores'].mean(), color='#2ECC71',
               linestyle='--', alpha=0.7, label=f'RF Media: {rf_results["cv_scores"].mean():.3f}')
    ax6.axhline(y=lr_results['cv_scores'].mean(), color='#3498DB',
               linestyle='--', alpha=0.7, label=f'LR Media: {lr_results["cv_scores"].mean():.3f}')

    ax6.set_xlabel('Fold')
    ax6.set_ylabel('F1-Score')
    ax6.set_title('🔄 Estabilidad en Validación Cruzada', fontweight='bold')
    ax6.set_xticks(folds)
    ax6.legend()
    ax6.grid(True, alpha=0.3)

    # 7. DISTRIBUCIÓN DE PROBABILIDADES (subplot 7)
    ax7 = plt.subplot(3, 3, 7)

    # Crear histogramas de probabilidades predichas
    ax7.hist(rf_results['predictions']['y_test_pred_proba'], bins=30, alpha=0.5,
            label='Random Forest', color='#2ECC71', density=True)
    ax7.hist(lr_results['predictions']['y_test_pred_proba'], bins=30, alpha=0.5,
            label='Regresión Logística', color='#3498DB', density=True)

    ax7.axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='Umbral 0.5')
    ax7.set_xlabel('Probabilidad Predicha de Churn')
    ax7.set_ylabel('Densidad')
    ax7.set_title('📊 Distribución de Probabilidades', fontweight='bold')
    ax7.legend()
    ax7.grid(True, alpha=0.3)

    # 8. ANÁLISIS DE ERRORES (subplot 8)
    ax8 = plt.subplot(3, 3, 8)

    rf_cm = evaluation_results['confusion_matrices']['rf']
    lr_cm = evaluation_results['confusion_matrices']['lr']

    # Extraer valores de las matrices
    rf_tn, rf_fp, rf_fn, rf_tp = rf_cm.ravel()
    lr_tn, lr_fp, lr_fn, lr_tp = lr_cm.ravel()

    error_types = ['Falsos\nPositivos', 'Falsos\nNegativos']
    rf_errors = [rf_fp, rf_fn]
    lr_errors = [lr_fp, lr_fn]

    x_pos = np.arange(len(error_types))
    width = 0.35

    ax8.bar(x_pos - width/2, rf_errors, width, label='Random Forest',
           color='#E74C3C', alpha=0.8)
    ax8.bar(x_pos + width/2, lr_errors, width, label='Regresión Logística',
           color='#F39C12', alpha=0.8)

    ax8.set_xlabel('Tipo de Error')
    ax8.set_ylabel('Cantidad de Errores')
    ax8.set_title('❌ Análisis de Errores por Tipo', fontweight='bold')
    ax8.set_xticks(x_pos)
    ax8.set_xticklabels(error_types)
    ax8.legend()
    ax8.grid(True, alpha=0.3)

    # 9. SCORE FINAL COMPARATIVO (subplot 9)
    ax9 = plt.subplot(3, 3, 9)

    final_scores = [evaluation_results['final_scores']['rf'],
                   evaluation_results['final_scores']['lr']]
    colors = ['#2ECC71', '#3498DB']

    bars = ax9.bar(['Random Forest', 'Regresión Logística'], final_scores,
                  color=colors, alpha=0.8)

    ax9.set_ylabel('Score Final (Ajustado)')
    ax9.set_title('🏆 Score Final Comparativo', fontweight='bold')
    ax9.grid(True, alpha=0.3)

    # Agregar valores en las barras
    for i, (bar, score) in enumerate(zip(bars, final_scores)):
        ax9.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                f'{score:.4f}', ha='center', va='bottom', fontweight='bold')

    # Marcar el ganador
    winner_idx = 0 if evaluation_results['winner'] == 'Random Forest' else 1
    bars[winner_idx].set_edgecolor('gold')
    bars[winner_idx].set_linewidth(3)

    plt.tight_layout()
    plt.show()

    print("✅ Visualizaciones de evaluación completadas")

# Crear visualizaciones detalladas
create_detailed_evaluation_plots(rf_results, lr_results, data_prepared, evaluation_results)

"""### 🎯 **CONCLUSIONES DEL ANÁLISIS CRÍTICO**

#### 📊 **Resumen de Rendimiento:**
Los resultados de la evaluación exhaustiva muestran el rendimiento comparativo de ambos modelos en múltiples dimensiones, considerando no solo la precisión sino también la capacidad de generalización y estabilidad.

#### 🔍 **Hallazgos Clave sobre Overfitting/Underfitting:**

**Factores Analizados:**
1. **Diferencia Train-Test**: Indicador principal de overfitting
2. **Estabilidad en CV**: Variabilidad entre folds indica robustez
3. **Complejidad vs Performance**: Balance entre simplicidad y capacidad predictiva

#### 💡 **Recomendaciones de Mejora:**

**Para Random Forest (si presenta overfitting):**
- Reducir `max_depth` de 10 a 6-8
- Aumentar `min_samples_split` y `min_samples_leaf`
- Implementar `max_features='sqrt'` para mayor diversidad

**Para Regresión Logística (si presenta underfitting):**
- Reducir regularización (aumentar parámetro C)
- Agregar términos polinomiales o interacciones
- Considerar feature engineering adicional

#### 🏆 **Selección Final del Modelo:**
El modelo recomendado se basa en un análisis multi-criterio que considera rendimiento, generalización y aplicabilidad práctica en el contexto de negocio de Telecom X.

## 13. Análisis de Importancia de Variables en Múltiples Modelos

En esta sección analizaremos las variables más relevantes para la predicción de churn utilizando diferentes algoritmos. Cada modelo tiene su propia forma de medir la importancia:

### 🎯 **Métodos de Análisis por Modelo:**

#### 🌲 **Random Forest**
- **Importancia por reducción de impureza**: Mide cuánto contribuye cada variable a disminuir la incertidumbre
- **Ventaja**: Robusto, maneja interacciones automáticamente
- **Interpretación**: Valores más altos = mayor importancia

#### 📊 **Regresión Logística**
- **Coeficientes del modelo**: Magnitud y dirección del impacto en log-odds
- **Ventaja**: Interpretación directa del efecto
- **Interpretación**: Valores absolutos altos = mayor importancia, signo = dirección

#### 🔍 **K-Nearest Neighbors (KNN)**
- **Análisis de proximidad**: Variables que más contribuyen a la distancia entre vecinos
- **Ventaja**: No asume forma específica de la relación
- **Interpretación**: Escalas relativas afectan la distancia

#### 🎯 **Support Vector Machine (SVM)**
- **Vectores de soporte**: Variables que definen la frontera de decisión
- **Ventaja**: Enfoque en casos críticos (vectores de soporte)
- **Interpretación**: Coeficientes indican importancia para la separación
"""

# Importar librerías adicionales para KNN y SVM
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd

def train_additional_models(data_prepared):
    """
    Entrena modelos KNN y SVM para análisis de importancia
    """
    print("🚀 ENTRENANDO MODELOS ADICIONALES: KNN Y SVM")
    print("="*60)

    # Extraer datos
    X_train = data_prepared['X_train']
    X_test = data_prepared['X_test']
    y_train = data_prepared['y_train']
    y_test = data_prepared['y_test']
    scaler = data_prepared['scaler']
    numerical_cols = data_prepared['numerical_cols']

    # Preparar datos escalados (necesarios para KNN y SVM)
    print("📏 PREPARANDO DATOS ESCALADOS PARA KNN Y SVM...")

    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()

    if numerical_cols:
        X_train_scaled[numerical_cols] = scaler.transform(X_train[numerical_cols])
        X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])
        print(f"   ✅ Variables escaladas: {numerical_cols}")

    # 1. ENTRENAR MODELO KNN
    print(f"\n🔍 ENTRENANDO K-NEAREST NEIGHBORS:")
    print("-" * 40)

    # Encontrar el mejor K usando validación
    k_values = [3, 5, 7, 9, 11]
    best_k = 5
    best_score = 0

    for k in k_values:
        knn_temp = KNeighborsClassifier(n_neighbors=k, weights='distance')
        cv_scores_temp = cross_val_score(knn_temp, X_train_scaled, y_train,
                                        cv=3, scoring='f1')
        mean_score = cv_scores_temp.mean()
        if mean_score > best_score:
            best_score = mean_score
            best_k = k

    print(f"   🎯 Mejor K encontrado: {best_k} (F1-Score CV: {best_score:.4f})")

    knn_model = KNeighborsClassifier(
        n_neighbors=best_k,
        weights='distance',  # Pesos basados en distancia
        metric='euclidean'   # Métrica de distancia
    )

    knn_model.fit(X_train_scaled, y_train)

    # Predicciones KNN
    knn_train_pred = knn_model.predict(X_train_scaled)
    knn_test_pred = knn_model.predict(X_test_scaled)
    knn_test_pred_proba = knn_model.predict_proba(X_test_scaled)[:, 1]

    # Métricas KNN
    knn_metrics = {
        'train': {
            'accuracy': accuracy_score(y_train, knn_train_pred),
            'precision': precision_score(y_train, knn_train_pred),
            'recall': recall_score(y_train, knn_train_pred),
            'f1': f1_score(y_train, knn_train_pred)
        },
        'test': {
            'accuracy': accuracy_score(y_test, knn_test_pred),
            'precision': precision_score(y_test, knn_test_pred),
            'recall': recall_score(y_test, knn_test_pred),
            'f1': f1_score(y_test, knn_test_pred),
            'auc': roc_auc_score(y_test, knn_test_pred_proba)
        }
    }

    print(f"   📊 Métricas Test - Accuracy: {knn_metrics['test']['accuracy']:.4f}, F1: {knn_metrics['test']['f1']:.4f}")

    # 2. ENTRENAR MODELO SVM
    print(f"\n🎯 ENTRENANDO SUPPORT VECTOR MACHINE:")
    print("-" * 40)

    svm_model = SVC(
        kernel='rbf',              # Kernel radial (maneja no-linealidades)
        C=1.0,                     # Parámetro de regularización
        gamma='scale',             # Parámetro del kernel
        class_weight='balanced',   # Balancear clases
        probability=True,          # Habilitar predicción de probabilidades
        random_state=42
    )

    print("   🏋️ Entrenando SVM (puede tomar unos minutos)...")
    svm_model.fit(X_train_scaled, y_train)

    # Predicciones SVM
    svm_train_pred = svm_model.predict(X_train_scaled)
    svm_test_pred = svm_model.predict(X_test_scaled)
    svm_test_pred_proba = svm_model.predict_proba(X_test_scaled)[:, 1]

    # Métricas SVM
    svm_metrics = {
        'train': {
            'accuracy': accuracy_score(y_train, svm_train_pred),
            'precision': precision_score(y_train, svm_train_pred),
            'recall': recall_score(y_train, svm_train_pred),
            'f1': f1_score(y_train, svm_train_pred)
        },
        'test': {
            'accuracy': accuracy_score(y_test, svm_test_pred),
            'precision': precision_score(y_test, svm_test_pred),
            'recall': recall_score(y_test, svm_test_pred),
            'f1': f1_score(y_test, svm_test_pred),
            'auc': roc_auc_score(y_test, svm_test_pred_proba)
        }
    }

    print(f"   📊 Métricas Test - Accuracy: {svm_metrics['test']['accuracy']:.4f}, F1: {svm_metrics['test']['f1']:.4f}")
    print(f"   🎯 Vectores de soporte utilizados: {len(svm_model.support_)}")

    print("="*60)

    return {
        'knn_model': knn_model,
        'svm_model': svm_model,
        'knn_metrics': knn_metrics,
        'svm_metrics': svm_metrics,
        'knn_predictions': {
            'test_pred': knn_test_pred,
            'test_pred_proba': knn_test_pred_proba
        },
        'svm_predictions': {
            'test_pred': svm_test_pred,
            'test_pred_proba': svm_test_pred_proba
        },
        'X_train_scaled': X_train_scaled,
        'X_test_scaled': X_test_scaled,
        'best_k': best_k
    }

# Entrenar modelos adicionales
additional_models = train_additional_models(data_prepared)

def analyze_feature_importance_all_models(rf_results, lr_results, additional_models, data_prepared):
    """
    Analiza la importancia de variables en todos los modelos entrenados
    """
    print("🔍 ANÁLISIS EXHAUSTIVO DE IMPORTANCIA DE VARIABLES")
    print("="*80)

    feature_names = data_prepared['feature_names']
    X_train_scaled = additional_models['X_train_scaled']
    X_test_scaled = additional_models['X_test_scaled']
    y_train = data_prepared['y_train']
    y_test = data_prepared['y_test']

    # 1. RANDOM FOREST - IMPORTANCIA POR REDUCCIÓN DE IMPUREZA
    print("\n🌲 RANDOM FOREST - IMPORTANCIA POR REDUCCIÓN DE IMPUREZA")
    print("-" * 70)

    rf_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': rf_results['model'].feature_importances_,
        'importance_normalized': rf_results['model'].feature_importances_ / rf_results['model'].feature_importances_.sum()
    }).sort_values('importance', ascending=False)

    print("🏆 TOP 15 VARIABLES MÁS IMPORTANTES:")
    for i, (_, row) in enumerate(rf_importance.head(15).iterrows(), 1):
        percentage = row['importance_normalized'] * 100
        print(f"   {i:2d}. {row['feature'][:35]:35} → {row['importance']:.4f} ({percentage:5.1f}%)")

    # 2. REGRESIÓN LOGÍSTICA - ANÁLISIS DE COEFICIENTES
    print("\n📊 REGRESIÓN LOGÍSTICA - ANÁLISIS DE COEFICIENTES")
    print("-" * 70)

    lr_importance = pd.DataFrame({
        'feature': feature_names,
        'coefficient': lr_results['model'].coef_[0],
        'abs_coefficient': np.abs(lr_results['model'].coef_[0])
    }).sort_values('abs_coefficient', ascending=False)

    print("🏆 TOP 15 COEFICIENTES MÁS SIGNIFICATIVOS:")
    for i, (_, row) in enumerate(lr_importance.head(15).iterrows(), 1):
        direction = "⬆️ Incrementa" if row['coefficient'] > 0 else "⬇️ Reduce"
        print(f"   {i:2d}. {row['feature'][:30]:30} → {row['coefficient']:8.4f} ({direction} churn)")

    # 3. KNN - IMPORTANCIA POR PERMUTACIÓN
    print("\n🔍 K-NEAREST NEIGHBORS - IMPORTANCIA POR PERMUTACIÓN")
    print("-" * 70)

    print("   🔄 Calculando importancia por permutación (puede tomar unos minutos)...")

    # Calcular importancia por permutación para KNN
    knn_perm_importance = permutation_importance(
        additional_models['knn_model'],
        X_test_scaled,
        y_test,
        n_repeats=5,
        random_state=42,
        scoring='f1'
    )

    knn_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': knn_perm_importance.importances_mean,
        'importance_std': knn_perm_importance.importances_std
    }).sort_values('importance', ascending=False)

    print("🏆 TOP 15 VARIABLES MÁS IMPORTANTES:")
    for i, (_, row) in enumerate(knn_importance.head(15).iterrows(), 1):
        print(f"   {i:2d}. {row['feature'][:35]:35} → {row['importance']:.4f} ± {row['importance_std']:.4f}")

    # 4. SVM - ANÁLISIS DE VECTORES DE SOPORTE
    print("\n🎯 SUPPORT VECTOR MACHINE - ANÁLISIS DE IMPORTANCIA")
    print("-" * 70)

    # Para SVM con kernel RBF, usamos importancia por permutación
    print("   🔄 Calculando importancia por permutación para SVM...")

    svm_perm_importance = permutation_importance(
        additional_models['svm_model'],
        X_test_scaled,
        y_test,
        n_repeats=5,
        random_state=42,
        scoring='f1'
    )

    svm_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': svm_perm_importance.importances_mean,
        'importance_std': svm_perm_importance.importances_std
    }).sort_values('importance', ascending=False)

    print("🏆 TOP 15 VARIABLES MÁS IMPORTANTES:")
    for i, (_, row) in enumerate(svm_importance.head(15).iterrows(), 1):
        print(f"   {i:2d}. {row['feature'][:35]:35} → {row['importance']:.4f} ± {row['importance_std']:.4f}")

    print(f"\n📊 Información adicional SVM:")
    print(f"   • Vectores de soporte: {len(additional_models['svm_model'].support_)}")
    print(f"   • Porcentaje de vectores de soporte: {len(additional_models['svm_model'].support_)/len(X_train_scaled)*100:.1f}%")

    # 5. ANÁLISIS COMPARATIVO ENTRE MODELOS
    print(f"\n🔬 ANÁLISIS COMPARATIVO DE IMPORTANCIA ENTRE MODELOS")
    print("-" * 70)

    # Crear ranking de variables por modelo
    rankings = {
        'Random Forest': {row['feature']: i+1 for i, (_, row) in enumerate(rf_importance.iterrows())},
        'Regresión Logística': {row['feature']: i+1 for i, (_, row) in enumerate(lr_importance.iterrows())},
        'KNN': {row['feature']: i+1 for i, (_, row) in enumerate(knn_importance.iterrows())},
        'SVM': {row['feature']: i+1 for i, (_, row) in enumerate(svm_importance.iterrows())}
    }

    # Encontrar variables consistentemente importantes
    top_features_all = set()
    for model_ranking in rankings.values():
        top_features_all.update(list(model_ranking.keys())[:10])

    print("🎯 VARIABLES CONSISTENTEMENTE IMPORTANTES (TOP 10 en algún modelo):")

    comparison_df = pd.DataFrame({
        'Variable': list(top_features_all),
        'RF_Rank': [rankings['Random Forest'].get(var, 999) for var in top_features_all],
        'LR_Rank': [rankings['Regresión Logística'].get(var, 999) for var in top_features_all],
        'KNN_Rank': [rankings['KNN'].get(var, 999) for var in top_features_all],
        'SVM_Rank': [rankings['SVM'].get(var, 999) for var in top_features_all]
    })

    # Calcular ranking promedio
    comparison_df['Avg_Rank'] = comparison_df[['RF_Rank', 'LR_Rank', 'KNN_Rank', 'SVM_Rank']].mean(axis=1)
    comparison_df = comparison_df.sort_values('Avg_Rank')

    print("\n📋 RANKING CONSOLIDADO (Top 20):")
    for i, (_, row) in enumerate(comparison_df.head(20).iterrows(), 1):
        rf_rank = row['RF_Rank'] if row['RF_Rank'] < 999 else '-'
        lr_rank = row['LR_Rank'] if row['LR_Rank'] < 999 else '-'
        knn_rank = row['KNN_Rank'] if row['KNN_Rank'] < 999 else '-'
        svm_rank = row['SVM_Rank'] if row['SVM_Rank'] < 999 else '-'

        print(f"   {i:2d}. {row['Variable'][:30]:30} | RF:{rf_rank:>3} LR:{lr_rank:>3} KNN:{knn_rank:>3} SVM:{svm_rank:>3} | Avg:{row['Avg_Rank']:5.1f}")

    # 6. IDENTIFICAR PATRONES Y CONSENSOS
    print(f"\n💡 ANÁLISIS DE CONSENSO ENTRE MODELOS:")
    print("-" * 70)

    # Variables que aparecen en top 10 de múltiples modelos
    consensus_vars = []
    for var in top_features_all:
        models_in_top10 = sum(1 for ranking in rankings.values() if ranking.get(var, 999) <= 10)
        if models_in_top10 >= 3:
            consensus_vars.append((var, models_in_top10))

    consensus_vars.sort(key=lambda x: x[1], reverse=True)

    print("🤝 VARIABLES CON MAYOR CONSENSO (Top 10 en 3+ modelos):")
    for var, count in consensus_vars:
        print(f"   • {var[:40]:40} → Aparece en {count}/4 modelos")

    if not consensus_vars:
        print("   ℹ️ No hay variables que aparezcan en top 10 de 3+ modelos")

    print("="*80)

    return {
        'rf_importance': rf_importance,
        'lr_importance': lr_importance,
        'knn_importance': knn_importance,
        'svm_importance': svm_importance,
        'comparison_df': comparison_df,
        'consensus_vars': consensus_vars,
        'rankings': rankings
    }

# Ejecutar análisis de importancia
importance_analysis = analyze_feature_importance_all_models(
    rf_results, lr_results, additional_models, data_prepared
)

def create_feature_importance_visualizations(importance_analysis, additional_models):
    """
    Crea visualizaciones comprehensivas de importancia de variables
    """
    print("🎨 CREANDO VISUALIZACIONES DE IMPORTANCIA DE VARIABLES")
    print("="*60)

    # Configurar figura grande con múltiples subplots
    fig = plt.figure(figsize=(24, 20))

    # 1. RANDOM FOREST - Importancia por reducción de impureza
    ax1 = plt.subplot(3, 3, 1)
    top_rf = importance_analysis['rf_importance'].head(15)
    bars1 = ax1.barh(range(len(top_rf)), top_rf['importance'], color='#2ECC71', alpha=0.8)
    ax1.set_yticks(range(len(top_rf)))
    ax1.set_yticklabels([name[:25] for name in top_rf['feature']])
    ax1.set_xlabel('Importancia (Reducción de Impureza)')
    ax1.set_title('🌲 Random Forest\nImportancia de Variables', fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='x')

    # 2. REGRESIÓN LOGÍSTICA - Coeficientes
    ax2 = plt.subplot(3, 3, 2)
    top_lr = importance_analysis['lr_importance'].head(15)
    colors_lr = ['#E74C3C' if x < 0 else '#3498DB' for x in top_lr['coefficient']]
    bars2 = ax2.barh(range(len(top_lr)), top_lr['coefficient'], color=colors_lr, alpha=0.8)
    ax2.set_yticks(range(len(top_lr)))
    ax2.set_yticklabels([name[:25] for name in top_lr['feature']])
    ax2.set_xlabel('Coeficiente')
    ax2.set_title('📊 Regresión Logística\nCoeficientes', fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='x')
    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)

    # 3. KNN - Importancia por permutación
    ax3 = plt.subplot(3, 3, 3)
    top_knn = importance_analysis['knn_importance'].head(15)
    bars3 = ax3.barh(range(len(top_knn)), top_knn['importance'],
                    xerr=top_knn['importance_std'], color='#9B59B6', alpha=0.8)
    ax3.set_yticks(range(len(top_knn)))
    ax3.set_yticklabels([name[:25] for name in top_knn['feature']])
    ax3.set_xlabel('Importancia (Permutación)')
    ax3.set_title('🔍 K-Nearest Neighbors\nImportancia por Permutación', fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='x')

    # 4. SVM - Importancia por permutación
    ax4 = plt.subplot(3, 3, 4)
    top_svm = importance_analysis['svm_importance'].head(15)
    bars4 = ax4.barh(range(len(top_svm)), top_svm['importance'],
                    xerr=top_svm['importance_std'], color='#F39C12', alpha=0.8)
    ax4.set_yticks(range(len(top_svm)))
    ax4.set_yticklabels([name[:25] for name in top_svm['feature']])
    ax4.set_xlabel('Importancia (Permutación)')
    ax4.set_title('🎯 Support Vector Machine\nImportancia por Permutación', fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='x')

    # 5. COMPARACIÓN DE RANKINGS
    ax5 = plt.subplot(3, 3, 5)
    top_consensus = importance_analysis['comparison_df'].head(10)

    # Crear heatmap de rankings
    ranking_matrix = top_consensus[['RF_Rank', 'LR_Rank', 'KNN_Rank', 'SVM_Rank']].values
    ranking_matrix[ranking_matrix == 999] = np.nan  # NaN para valores no rankeados

    im = ax5.imshow(ranking_matrix.T, cmap='RdYlBu_r', aspect='auto')
    ax5.set_xticks(range(len(top_consensus)))
    ax5.set_xticklabels([name[:15] for name in top_consensus['Variable']], rotation=45)
    ax5.set_yticks(range(4))
    ax5.set_yticklabels(['RF', 'LR', 'KNN', 'SVM'])
    ax5.set_title('🔬 Heatmap de Rankings\n(Menor = Mejor)', fontweight='bold')

    # Agregar valores en el heatmap
    for i in range(4):
        for j in range(len(top_consensus)):
            value = ranking_matrix[j, i]
            if not np.isnan(value):
                ax5.text(j, i, f'{int(value)}', ha='center', va='center',
                        color='white' if value > 50 else 'black', fontweight='bold')

    # 6. DISTRIBUCIÓN DE IMPORTANCIA - Random Forest
    ax6 = plt.subplot(3, 3, 6)
    ax6.hist(importance_analysis['rf_importance']['importance'], bins=30,
            color='#2ECC71', alpha=0.7, edgecolor='black')
    ax6.axvline(importance_analysis['rf_importance']['importance'].mean(),
               color='red', linestyle='--', linewidth=2, label='Media')
    ax6.set_xlabel('Importancia')
    ax6.set_ylabel('Frecuencia')
    ax6.set_title('🌲 Distribución de Importancia\n(Random Forest)', fontweight='bold')
    ax6.legend()
    ax6.grid(True, alpha=0.3)

    # 7. COMPARACIÓN DIRECTA TOP 5
    ax7 = plt.subplot(3, 3, 7)

    # Obtener top 5 de cada modelo
    models = ['RF', 'LR', 'KNN', 'SVM']
    importances_list = [
        importance_analysis['rf_importance'],
        importance_analysis['lr_importance'],
        importance_analysis['knn_importance'],
        importance_analysis['svm_importance']
    ]

    colors = ['#2ECC71', '#3498DB', '#9B59B6', '#F39C12']

    for i, (model, imp_df, color) in enumerate(zip(models, importances_list, colors)):
        top_5 = imp_df.head(5)
        if 'importance' in imp_df.columns:
            values = top_5['importance'].values
        elif 'abs_coefficient' in imp_df.columns:
            values = top_5['abs_coefficient'].values
        else:
            values = top_5.iloc[:, 1].values

        x_pos = np.arange(5) + i * 0.2
        ax7.bar(x_pos, values, width=0.2, label=model, color=color, alpha=0.8)

    ax7.set_xlabel('Top 5 Variables (por modelo)')
    ax7.set_ylabel('Importancia Normalizada')
    ax7.set_title('📊 Comparación Top 5\nVariables por Modelo', fontweight='bold')
    ax7.legend()
    ax7.grid(True, alpha=0.3)

    # 8. ANÁLISIS DE CONSENSO
    ax8 = plt.subplot(3, 3, 8)

    if importance_analysis['consensus_vars']:
        consensus_names = [var[0][:20] for var in importance_analysis['consensus_vars'][:10]]
        consensus_counts = [var[1] for var in importance_analysis['consensus_vars'][:10]]

        bars8 = ax8.bar(range(len(consensus_names)), consensus_counts,
                       color='#E67E22', alpha=0.8)
        ax8.set_xticks(range(len(consensus_names)))
        ax8.set_xticklabels(consensus_names, rotation=45)
        ax8.set_ylabel('Número de Modelos (Top 10)')
        ax8.set_title('🤝 Variables con Mayor Consenso', fontweight='bold')
        ax8.grid(True, alpha=0.3)

        # Agregar valores en las barras
        for bar, count in zip(bars8, consensus_counts):
            ax8.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                    f'{count}/4', ha='center', va='bottom', fontweight='bold')
    else:
        ax8.text(0.5, 0.5, 'No hay variables\ncon consenso',
                ha='center', va='center', transform=ax8.transAxes, fontsize=12)
        ax8.set_title('🤝 Variables con Mayor Consenso', fontweight='bold')

    # 9. MÉTRICAS DE RENDIMIENTO DE TODOS LOS MODELOS
    ax9 = plt.subplot(3, 3, 9)

    models_names = ['Random Forest', 'Reg. Logística', 'KNN', 'SVM']
    f1_scores = [
        rf_results['metrics']['test']['f1'],
        lr_results['metrics']['test']['f1'],
        additional_models['knn_metrics']['test']['f1'],
        additional_models['svm_metrics']['test']['f1']
    ]

    bars9 = ax9.bar(models_names, f1_scores,
                   color=['#2ECC71', '#3498DB', '#9B59B6', '#F39C12'], alpha=0.8)
    ax9.set_ylabel('F1-Score')
    ax9.set_title('🏆 Rendimiento de Todos los Modelos\n(F1-Score)', fontweight='bold')
    ax9.grid(True, alpha=0.3)
    plt.setp(ax9.get_xticklabels(), rotation=45)

    # Agregar valores en las barras
    for bar, score in zip(bars9, f1_scores):
        ax9.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,
                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()

    print("✅ Visualizaciones de importancia completadas")

# Crear visualizaciones
create_feature_importance_visualizations(importance_analysis, additional_models)