# -*- coding: utf-8 -*-
"""Telecom_X_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HpHi8aTw83ivYPo8MFqnWrj4_bFYRGwi

## 1. ImportaciÃ³n de librerÃ­as y configuraciÃ³n

En esta secciÃ³n se importan las librerÃ­as necesarias para el anÃ¡lisis de datos, visualizaciÃ³n y manejo de advertencias. AdemÃ¡s, se configura el estilo de los grÃ¡ficos para asegurar una presentaciÃ³n visual profesional y se suprimen advertencias irrelevantes para mantener el notebook limpio.
"""

# ImportaciÃ³n de librerÃ­as necesarias para el anÃ¡lisis de datos y visualizaciÃ³n
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import requests

# ConfiguraciÃ³n de estilo para grÃ¡ficos y supresiÃ³n de advertencias irrelevantes
sns.set(style="whitegrid", palette="muted")
warnings.filterwarnings('ignore')

"""## 2. Carga, limpieza y transformaciÃ³n de los datos

Se cargan los datos desde una fuente externa, se normalizan y renombran las columnas para mayor claridad. Posteriormente, se transforman variables categÃ³ricas a formato numÃ©rico binario, se crean nuevas variables Ãºtiles para el anÃ¡lisis y se eliminan registros con valores nulos en las columnas clave. Este proceso garantiza que los datos estÃ©n listos para un anÃ¡lisis exploratorio robusto.
"""

# Carga de datos desde un archivo JSON alojado en GitHub
url = "https://raw.githubusercontent.com/alura-cursos/challenge2-data-science-LATAM/refs/heads/main/TelecomX_Data.json"
response = requests.get(url)
response.raise_for_status()
json_data = response.json()

# NormalizaciÃ³n y conversiÃ³n a DataFrame
df = pd.json_normalize(json_data, sep='_')

# Renombrado de columnas para mayor claridad y consistencia
def clean_column_names(col):
    for prefix in ['customer_', 'internet_', 'phone_', 'account_']:
        if col.startswith(prefix):
            return col.replace(prefix, '')
    return col

df.columns = [clean_column_names(col).lower() for col in df.columns]

# ConversiÃ³n de variables categÃ³ricas a formato numÃ©rico binario
binary_columns = [
    'churn', 'partner', 'dependents', 'onlinesecurity', 'onlinebackup',
    'deviceprotection', 'techsupport', 'streamingtv', 'streamingmovies',
    'paperlessbilling', 'phoneservice', 'multiplelines'
]
for col in binary_columns:
    df[col] = df[col].replace({'Yes': 1, 'No': 0})

# ConversiÃ³n de la columna 'charges_total' a numÃ©rico, manejando errores
df['charges_total'] = pd.to_numeric(df['charges_total'], errors='coerce')

# CreaciÃ³n de una nueva columna para el cargo diario promedio
df['charges_daily'] = df['charges_monthly'] / 30

# EliminaciÃ³n de filas con valores nulos en 'charges_total' para anÃ¡lisis numÃ©ricos
df_cleaned = df.dropna(subset=['charges_total']).copy()

"""## 3. AnÃ¡lisis exploratorio: DistribuciÃ³n de Churn

En esta secciÃ³n se visualiza la distribuciÃ³n de clientes que han abandonado la empresa (churn) frente a los que permanecen. Se utilizan grÃ¡ficos de barras y de pastel para comprender la proporciÃ³n y cantidad de clientes en cada grupo, lo que permite dimensionar el problema de la evasiÃ³n.
"""

plt.figure(figsize=(6, 4))
df['churn'] = pd.to_numeric(df['churn'].replace({'Yes': 1, 'No': 0}), errors='coerce')
df.dropna(subset=['churn'], inplace=True)
churn_counts = df['churn'].value_counts().sort_index()
sns.barplot(x=churn_counts.index, y=churn_counts.values, palette=['#3498db', '#e74c3c'])
plt.title('DistribuciÃ³n de Churn (0: No, 1: SÃ­)')
plt.xlabel('Churn')
plt.ylabel('NÃºmero de Clientes')
plt.xticks([0, 1], ['No Churn', 'Churn'])
plt.show()

# GrÃ¡fico de pastel para proporciÃ³n de churn
plt.figure(figsize=(6, 6))
plt.pie(churn_counts, labels=['No Churn', 'Churn'], autopct='%1.1f%%', startangle=90, colors=['#3498db', '#e74c3c'])
plt.title('ProporciÃ³n de Clientes con y sin Churn')
plt.axis('equal')
plt.show()

"""## 4. AnÃ¡lisis exploratorio: Variables categÃ³ricas

AquÃ­ se analiza la relaciÃ³n entre la evasiÃ³n de clientes (churn) y diferentes variables categÃ³ricas, como gÃ©nero, tipo de contrato, servicios contratados y mÃ©todo de pago. Se emplean grÃ¡ficos de barras segmentados para identificar patrones y diferencias en la tasa de churn segÃºn cada categorÃ­a.
"""

# AnÃ¡lisis de la relaciÃ³n entre churn y variables categÃ³ricas
categorical_cols = [
    'gender', 'partner', 'dependents', 'phoneservice', 'multiplelines',
    'internetservice', 'onlinesecurity', 'onlinebackup', 'deviceprotection',
    'techsupport', 'streamingtv', 'streamingmovies', 'contract',
    'paperlessbilling', 'paymentmethod'
]

for col in categorical_cols:
    plt.figure(figsize=(8, 5))
    sns.countplot(data=df, x=col, hue='churn', palette=['#3498db', '#e74c3c'])
    plt.title(f'DistribuciÃ³n de Churn por {col.capitalize()}')
    plt.xlabel(col.capitalize())
    plt.ylabel('NÃºmero de Clientes')
    plt.legend(title='Churn', labels=['No Churn', 'Churn'])
    plt.tight_layout()
    plt.show()

"""## 5. AnÃ¡lisis exploratorio: Variables numÃ©ricas

En esta secciÃ³n se explora cÃ³mo las variables numÃ©ricas, como la antigÃ¼edad del cliente y los cargos mensuales o totales, se relacionan con la evasiÃ³n. Se utilizan boxplots e histogramas para comparar la distribuciÃ³n de estas variables entre clientes que permanecen y los que han abandonado la empresa, identificando posibles factores de riesgo.
"""

# AnÃ¡lisis de la relaciÃ³n entre churn y variables numÃ©ricas
numeric_cols = ['tenure', 'charges_monthly', 'charges_daily', 'charges_total']

for col in numeric_cols:
    if col in df_cleaned.columns:
        # Este bloque de cÃ³digo ahora estÃ¡ indentado correctamente
        plt.figure(figsize=(8, 5))
        sns.boxplot(data=df_cleaned, x='churn', y=col, palette=['#3498db', '#e74c3c'])
        plt.title(f'DistribuciÃ³n de {col.replace("_", " ").capitalize()} por Churn')
        plt.xlabel('Churn (0: No, 1: SÃ­)')
        plt.ylabel(col.replace('_', ' ').capitalize())
        plt.xticks([0, 1], ['No Churn', 'Churn'])
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(8, 5))
        sns.histplot(data=df_cleaned, x=col, hue='churn', multiple='stack', kde=True, palette=['#3498db', '#e74c3c'])
        plt.title(f'Histograma de {col.replace("_", " ").capitalize()} por Churn')
        plt.xlabel(col.replace('_', ' ').capitalize())
        plt.ylabel('Frecuencia')
        plt.legend(title='Churn', labels=['No Churn', 'Churn'])
        plt.tight_layout()
        plt.show()

"""# ğŸ“„ Informe Final: AnÃ¡lisis de EvasiÃ³n de Clientes (Churn) en TelecomX

## 1. IntroducciÃ³n

El objetivo de este anÃ¡lisis es comprender los factores que influyen en la evasiÃ³n de clientes (churn) en la empresa TelecomX. El churn representa la pÃ©rdida de clientes, un problema crÃ­tico para las empresas de telecomunicaciones, ya que retener clientes existentes suele ser mÃ¡s rentable que adquirir nuevos. Identificar patrones y variables asociadas al churn permite diseÃ±ar estrategias efectivas para reducir la fuga de clientes y mejorar la rentabilidad.

## 2. Limpieza y Tratamiento de Datos

- **ImportaciÃ³n:** Se importaron los datos desde un archivo JSON alojado en GitHub.
- **NormalizaciÃ³n:** Se normalizaron los datos y se renombraron las columnas para mayor claridad.
- **ConversiÃ³n de variables:** Se transformaron variables categÃ³ricas (como 'Yes'/'No') a valores binarios (1/0) para facilitar el anÃ¡lisis.
- **CreaciÃ³n de variables:** Se creÃ³ la variable `charges_daily` para analizar el gasto diario promedio.
- **Manejo de valores nulos:** Se identificaron y trataron valores nulos, especialmente en la columna `charges_total`, convirtiÃ©ndola a formato numÃ©rico y eliminando filas con datos faltantes para ciertos anÃ¡lisis.

## 3. AnÃ¡lisis Exploratorio de Datos

- **DistribuciÃ³n de Churn:** Se observÃ³ que aproximadamente el 26% de los clientes han abandonado la empresa.
- **Variables categÃ³ricas:** Se analizaron variables como gÃ©nero, tipo de contrato, servicios contratados, y mÃ©todos de pago. Se identificaron diferencias notables en la tasa de churn segÃºn el tipo de contrato y la presencia de servicios adicionales (seguridad en lÃ­nea, soporte tÃ©cnico, etc.).
- **Variables numÃ©ricas:** Se explorÃ³ la relaciÃ³n entre el churn y variables como la antigÃ¼edad del cliente (`tenure`) y los cargos mensuales/totales. Los clientes con menor antigÃ¼edad y cargos mensuales mÃ¡s altos tienden a presentar mayor churn.

## 4. Conclusiones e Insights

- **AntigÃ¼edad:** Los clientes con menor tiempo en la empresa son mÃ¡s propensos a abandonar el servicio.
- **Tipo de contrato:** Los contratos mensuales presentan una tasa de churn significativamente mayor que los contratos a largo plazo.
- **Servicios adicionales:** La ausencia de servicios como seguridad en lÃ­nea, respaldo o soporte tÃ©cnico estÃ¡ asociada a una mayor evasiÃ³n.
- **Cargos mensuales:** Los clientes con cargos mensuales elevados tienden a abandonar mÃ¡s, posiblemente por percibir un menor valor por el costo.

## 5. Recomendaciones

- **Fomentar contratos a largo plazo:** Ofrecer incentivos para que los clientes opten por contratos anuales o bianuales puede reducir el churn.
- **Promocionar servicios adicionales:** Incluir servicios de seguridad, respaldo y soporte tÃ©cnico en los paquetes bÃ¡sicos o a precios promocionales puede aumentar la retenciÃ³n.
- **Estrategias para nuevos clientes:** Implementar programas de bienvenida y seguimiento personalizado durante los primeros meses puede disminuir la fuga temprana.
- **RevisiÃ³n de precios:** Analizar la estructura de precios para clientes con cargos mensuales altos y ofrecer descuentos o beneficios adicionales.
- **Alertas tempranas:** Desarrollar modelos predictivos para identificar clientes en riesgo y aplicar acciones preventivas.

# ADICIONALES

### 6. AnÃ¡lisis de correlaciÃ³n entre variables

En esta secciÃ³n se exploran las correlaciones entre diferentes variables del dataset para identificar los factores que tienen mayor relaciÃ³n con la evasiÃ³n de clientes (churn). Se analiza, por ejemplo, la relaciÃ³n entre el gasto diario y la evasiÃ³n, asÃ­ como el impacto de la cantidad de servicios contratados en la probabilidad de churn. Este anÃ¡lisis es fundamental para obtener insights que permitan construir modelos predictivos mÃ¡s robustos.
"""

# CÃ¡lculo de la matriz de correlaciÃ³n para las variables numÃ©ricas relevantes
df_cleaned['churn'] = pd.to_numeric(df_cleaned['churn'], errors='coerce')
df_cleaned.dropna(subset=['churn'], inplace=True)


correlation_matrix = df_cleaned[['churn', 'charges_daily', 'tenure', 'charges_monthly', 'charges_total']].corr()

# VisualizaciÃ³n de la matriz de correlaciÃ³n
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de correlaciÃ³n entre variables numÃ©ricas')
plt.show()

# AnÃ¡lisis especÃ­fico: relaciÃ³n entre el gasto diario y la evasiÃ³n de clientes
plt.figure(figsize=(7, 5))
sns.boxplot(x='churn', y='charges_daily', data=df_cleaned, palette=['#3498db', '#e74c3c'])
plt.title('RelaciÃ³n entre el gasto diario y la evasiÃ³n de clientes (churn)')
plt.xlabel('Churn (0: No, 1: SÃ­)')
plt.ylabel('Gasto diario promedio')
plt.xticks([0, 1], ['No Churn', 'Churn'])
plt.show()

# AnÃ¡lisis adicional: cantidad de servicios contratados y churn
# Se cuenta el nÃºmero de servicios adicionales contratados por cada cliente
service_cols = [
    'onlinesecurity', 'onlinebackup', 'deviceprotection',
    'techsupport', 'streamingtv', 'streamingmovies'
]

for col in service_cols:
    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')

df_cleaned['num_services'] = df_cleaned[service_cols].sum(axis=1)

plt.figure(figsize=(7, 5))
sns.boxplot(x='churn', y='num_services', data=df_cleaned, palette=['#3498db', '#e74c3c'])
plt.title('Cantidad de servicios contratados vs. churn')
plt.xlabel('Churn (0: No, 1: SÃ­)')
plt.ylabel('Cantidad de servicios contratados')
plt.xticks([0, 1], ['No Churn', 'Churn'])
plt.show()

# GrÃ¡fico de dispersiÃ³n: servicios contratados vs. gasto diario, coloreado por churn
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x='num_services', y='charges_daily', hue='churn',
    data=df_cleaned, palette=['#3498db', '#e74c3c'], alpha=0.6
)
plt.title('Servicios contratados vs. gasto diario, segÃºn churn')
plt.xlabel('Cantidad de servicios contratados')
plt.ylabel('Gasto diario promedio')
plt.legend(title='Churn', labels=['No Churn', 'Churn'])
plt.show()

"""**ConclusiÃ³n:**  
Se observa que los clientes con menor cantidad de servicios contratados y mayor gasto diario presentan una mayor tasa de evasiÃ³n. AdemÃ¡s, la matriz de correlaciÃ³n permite identificar quÃ© variables estÃ¡n mÃ¡s asociadas al churn, lo que es clave para futuros modelos predictivos.

# TELECOM PARTE 2

## Eliminacion de columnas irrelevantes para el modelo predictivo
"""

# Eliminar columnas que no aportan valor predictivo
columns_to_drop = [
    'customerid',  # Identificador Ãºnico - no aporta valor predictivo
]

# Verificar quÃ© columnas existen antes de eliminar
existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
print(f"Columnas que se eliminarÃ¡n: {existing_columns_to_drop}")

# Eliminar las columnas
df = df.drop(columns=existing_columns_to_drop)

print(f"\nDataset despuÃ©s de eliminar columnas irrelevantes:")
print(f"Forma: {df.shape}")
print(f"Columnas restantes: {df.columns.tolist()}")

"""## Limpieza adiccional de datos"""

# Limpiar valores problemÃ¡ticos en la columna churn (strings vacÃ­os)
print(f"Valores Ãºnicos en churn antes de limpieza: {df['churn'].unique()}")
print(f"Tipos de datos en churn: {df['churn'].dtype}")

# Convertir strings vacÃ­os a NaN y luego eliminar esas filas
df['churn'] = df['churn'].replace('', np.nan)
initial_rows = df.shape[0]
df = df.dropna(subset=['churn']).copy()
final_rows = df.shape[0]

print(f"\nValores Ãºnicos en churn despuÃ©s de limpieza: {df['churn'].unique()}")
print(f"Filas eliminadas: {initial_rows - final_rows}")
print(f"Dataset despuÃ©s de limpieza: {df.shape}")

# Verificar distribuciÃ³n final de churn
churn_distribution = df['churn'].value_counts()
print(f"\nDistribuciÃ³n final de Churn:")
print(f"No Churn (0): {churn_distribution[0]} ({churn_distribution[0]/len(df)*100:.1f}%)")
print(f"Churn (1): {churn_distribution[1]} ({churn_distribution[1]/len(df)*100:.1f}%)")

"""## Resumen del preprocesamiento para machine learning"""

print("="*60)
print("RESUMEN DEL PREPROCESAMIENTO PARA MACHINE LEARNING")
print("="*60)

print(f"\nğŸ“Š INFORMACIÃ“N DEL DATASET:")
print(f"   â€¢ Filas: {df.shape[0]:,}")
print(f"   â€¢ Columnas: {df.shape[1]}")
print(f"   â€¢ Target variable: 'churn'")

print(f"\nğŸ—‘ï¸ COLUMNAS ELIMINADAS:")
print(f"   â€¢ customerid (identificador Ãºnico sin valor predictivo)")

print(f"\nğŸ§¹ LIMPIEZA REALIZADA:")
print(f"   â€¢ Eliminados registros con valores vacÃ­os en 'churn'")
print(f"   â€¢ TransformaciÃ³n Yes/No â†’ 1/0 en variables categÃ³ricas")

print(f"\nğŸ“ˆ DISTRIBUCIÃ“N DE CHURN:")
churn_counts = df['churn'].value_counts()
for value, count in churn_counts.items():
    label = "No Churn" if value == 0 else "Churn"
    percentage = count/len(df)*100
    print(f"   â€¢ {label}: {count:,} ({percentage:.1f}%)")

print(f"\nğŸ“‹ COLUMNAS FINALES:")
categorical_features = ['gender', 'internetservice', 'contract', 'paymentmethod']
binary_features = ['seniorcitizen', 'partner', 'dependents', 'phoneservice',
                  'multiplelines', 'onlinesecurity', 'onlinebackup',
                  'deviceprotection', 'techsupport', 'streamingtv',
                  'streamingmovies', 'paperlessbilling']
numerical_features = ['tenure', 'charges_monthly', 'charges_total', 'charges_daily']

print(f"   â€¢ CategÃ³ricas: {len(categorical_features)} - {categorical_features}")
print(f"   â€¢ Binarias: {len(binary_features)} - {binary_features}")
print(f"   â€¢ NumÃ©ricas: {len(numerical_features)} - {numerical_features}")
print(f"   â€¢ Target: churn")

print(f"\nâœ… DATASET LISTO PARA MACHINE LEARNING")
print("="*60)

"""## Analisis de variables categoricas para one-hot encoding"""

# FunciÃ³n auxiliar para analizar variables categÃ³ricas
def analyze_categorical_variables(df):
    """Analiza y clasifica variables categÃ³ricas del dataset"""

    print("="*60)
    print("ANÃLISIS DE VARIABLES CATEGÃ“RICAS")
    print("="*60)

    categorical_vars = []
    binary_vars = []
    numerical_vars = []

    for col in df.columns:
        if col == 'churn':  # Skip target variable
            continue

        unique_count = df[col].nunique()
        unique_values = df[col].unique()

        print(f"\nğŸ“Š {col.upper()}:")
        print(f"   â€¢ Tipo: {df[col].dtype}")
        print(f"   â€¢ Valores Ãºnicos: {unique_count}")
        print(f"   â€¢ Valores: {list(unique_values)}")

        # Clasificar variable
        if df[col].dtype == 'object':
            categorical_vars.append(col)
            print(f"   â€¢ ClasificaciÃ³n: CATEGÃ“RICA (necesita One-Hot Encoding)")
        elif unique_count == 2 and set(unique_values).issubset({0, 1}):
            binary_vars.append(col)
            print(f"   â€¢ ClasificaciÃ³n: BINARIA (ya codificada)")
        else:
            numerical_vars.append(col)
            print(f"   â€¢ ClasificaciÃ³n: NUMÃ‰RICA")

    print(f"\n{'='*60}")
    print("RESUMEN DE CLASIFICACIÃ“N:")
    print(f"   ğŸ”¤ Variables categÃ³ricas (One-Hot): {len(categorical_vars)} - {categorical_vars}")
    print(f"   ğŸ”˜ Variables binarias: {len(binary_vars)} - {binary_vars}")
    print(f"   ğŸ”¢ Variables numÃ©ricas: {len(numerical_vars)} - {numerical_vars}")
    print(f"   ğŸ¯ Variable target: churn")
    print("="*60)

    return categorical_vars, binary_vars, numerical_vars

# Ejecutar anÃ¡lisis
categorical_vars, binary_vars, numerical_vars = analyze_categorical_variables(df)

"""## One-hot encoding de variables categoricas"""

def identify_categorical_variables(df):
    """Identifica variables categÃ³ricas que necesitan One-Hot Encoding"""

    print("="*60)
    print("ANÃLISIS DE VARIABLES CATEGÃ“RICAS REALES")
    print("="*60)

    categorical_vars = []
    binary_vars = []
    numerical_vars = []

    for col in df.columns:
        if col == 'churn':  # Skip target variable
            continue

        unique_count = df[col].nunique()
        unique_values = df[col].unique()

        print(f"\nğŸ“Š {col.upper()}:")
        print(f"   â€¢ Tipo: {df[col].dtype}")
        print(f"   â€¢ Valores Ãºnicos: {unique_count}")
        print(f"   â€¢ Valores: {list(unique_values)[:10]}")  # Mostrar mÃ¡ximo 10 valores

        # Clasificar variable
        if df[col].dtype == 'object' and unique_count > 2:
            categorical_vars.append(col)
            print(f"   â€¢ ClasificaciÃ³n: CATEGÃ“RICA (necesita One-Hot Encoding)")
        elif df[col].dtype == 'object' and unique_count == 2:
            # Variables de texto binarias que necesitan codificaciÃ³n manual
            categorical_vars.append(col)
            print(f"   â€¢ ClasificaciÃ³n: BINARIA TEXTO (necesita codificaciÃ³n)")
        elif unique_count == 2 and set(unique_values).issubset({0, 1}):
            binary_vars.append(col)
            print(f"   â€¢ ClasificaciÃ³n: BINARIA (ya codificada)")
        else:
            numerical_vars.append(col)
            print(f"   â€¢ ClasificaciÃ³n: NUMÃ‰RICA")

    print(f"\n{'='*60}")
    print("RESUMEN DE CLASIFICACIÃ“N:")
    print(f"   ğŸ”¤ Variables categÃ³ricas (One-Hot): {len(categorical_vars)} - {categorical_vars}")
    print(f"   ğŸ”˜ Variables binarias: {len(binary_vars)} - {binary_vars}")
    print(f"   ğŸ”¢ Variables numÃ©ricas: {len(numerical_vars)} - {numerical_vars}")
    print(f"   ğŸ¯ Variable target: churn")
    print("="*60)

    return categorical_vars, binary_vars, numerical_vars

# Ejecutar anÃ¡lisis
categorical_vars, binary_vars, numerical_vars = identify_categorical_variables(df)

"""## Verificacion del One-hot encoding"""

if categorical_vars:
    print(f"\nğŸ”„ APLICANDO ONE-HOT ENCODING A: {categorical_vars}")

    # Hacer una copia del dataset original para comparaciÃ³n
    df_original = df.copy()

    # Aplicar One-Hot Encoding usando pandas get_dummies
    df_encoded = pd.get_dummies(df,
                               columns=categorical_vars,
                               drop_first=True,  # Evita multicolinealidad
                               dtype=int)        # Asegura que sean integers (0,1)

    print(f"âœ… One-Hot Encoding completado!")
    print(f"   â€¢ Dataset original: {df_original.shape}")
    print(f"   â€¢ Dataset codificado: {df_encoded.shape}")
    print(f"   â€¢ Nuevas columnas creadas: {df_encoded.shape[1] - df_original.shape[1]}")

    # Mostrar las nuevas columnas creadas
    original_cols = set(df_original.columns)
    new_cols = set(df_encoded.columns)
    created_cols = sorted(list(new_cols - original_cols))

    print(f"\nğŸ“‹ NUEVAS COLUMNAS CREADAS ({len(created_cols)}):")
    for i, col in enumerate(created_cols, 1):
        print(f"   {i:2d}. {col}")

    # Actualizar el DataFrame principal
    df = df_encoded.copy()

else:
    print("â„¹ï¸ No se encontraron variables categÃ³ricas que requieran One-Hot Encoding")
    print("   Todas las variables ya estÃ¡n en formato numÃ©rico")

print(f"\nğŸ¯ DATASET FINAL PREPARADO:")
print(f"   â€¢ Forma: {df.shape}")
print(f"   â€¢ Total de features: {df.shape[1] - 1} (excluyendo target)")

"""## Preparacion final del dataset para machine learning"""

# Convertir charges_total a numÃ©rico (por si no se hizo antes)
df['charges_total'] = pd.to_numeric(df['charges_total'], errors='coerce')

# Verificar que no hay variables categÃ³ricas sin codificar
remaining_categorical = []
for col in df.columns:
    if col != 'churn' and df[col].dtype == 'object':
        remaining_categorical.append(col)

if remaining_categorical:
    print(f"âš ï¸  Variables categÃ³ricas sin codificar: {remaining_categorical}")
else:
    print("âœ… Todas las variables categÃ³ricas han sido codificadas correctamente")

# Separar features (X) y target (y)
X = df.drop('churn', axis=1)
y = df['churn']

print("\nğŸ¯ DATASET FINAL PARA MACHINE LEARNING")
print("="*60)
print(f"ğŸ“Š INFORMACIÃ“N DEL DATASET:")
print(f"   â€¢ Total de registros: {df.shape[0]:,}")
print(f"   â€¢ Total de features (X): {X.shape[1]}")
print(f"   â€¢ Variable target (y): churn")
print(f"   â€¢ DistribuciÃ³n de churn:")
print(f"     - No Churn (0): {(y == 0).sum():,} ({(y == 0).mean()*100:.1f}%)")
print(f"     - Churn (1): {(y == 1).sum():,} ({(y == 1).mean()*100:.1f}%)")

print(f"\nğŸ“‹ TIPOS DE FEATURES:")
feature_summary = {
    'NumÃ©ricas originales': ['tenure', 'charges_monthly', 'charges_total', 'charges_daily'],
    'Binarias originales': [col for col in X.columns if col in binary_vars],
    'One-Hot Encoded': [col for col in X.columns if any(col.startswith(f"{cat}_") for cat in categorical_vars)]
}

for feature_type, cols in feature_summary.items():
    actual_cols = [col for col in cols if col in X.columns]
    print(f"   â€¢ {feature_type}: {len(actual_cols)} columnas")

print(f"\nğŸ“Š TIPOS DE DATOS EN EL DATASET FINAL:")
data_types = df.dtypes.value_counts()
for dtype, count in data_types.items():
    print(f"   â€¢ {dtype}: {count} columnas")

print(f"\nâœ… DATASET COMPLETAMENTE PREPARADO PARA MACHINE LEARNING")
print("="*60)

# Mostrar primeras filas del dataset final (opcional)
print(f"\nğŸ” MUESTRA DEL DATASET CODIFICADO:")
print(df.head())

"""## Analisis de balance de clases"""

def analyze_class_balance(df, target_column='churn'):
    """
    Analiza el balance de clases en el dataset para evaluar desbalance
    y proporcionar recomendaciones para el modelado
    """
    print("="*70)
    print("ğŸ“Š ANÃLISIS DE BALANCE DE CLASES")
    print("="*70)

    # Conteo absoluto y proporciones usando value_counts()
    class_counts = df[target_column].value_counts().sort_index()
    class_proportions = df[target_column].value_counts(normalize=True).sort_index()

    print(f"\nğŸ“ˆ CONTEOS ABSOLUTOS:")
    print(f"   â€¢ No Churn (0): {class_counts[0]:,} clientes")
    print(f"   â€¢ Churn (1): {class_counts[1]:,} clientes")
    print(f"   â€¢ Total: {class_counts.sum():,} clientes")

    print(f"\nğŸ“Š PROPORCIONES:")
    print(f"   â€¢ No Churn (0): {class_proportions[0]:.3f} ({class_proportions[0]*100:.1f}%)")
    print(f"   â€¢ Churn (1): {class_proportions[1]:.3f} ({class_proportions[1]*100:.1f}%)")

    # Ratio de desbalance
    ratio = class_counts[0] / class_counts[1]
    print(f"\nâš–ï¸ RATIO DE DESBALANCE:")
    print(f"   â€¢ No Churn : Churn = {ratio:.2f} : 1")
    print(f"   â€¢ Por cada cliente que cancela, {ratio:.1f} permanecen activos")

    # EvaluaciÃ³n del desbalance
    minority_percentage = min(class_proportions) * 100

    print(f"\nğŸ¯ EVALUACIÃ“N DEL DESBALANCE:")
    if minority_percentage >= 40:
        balance_status = "BALANCEADO"
        impact = "BAJO"
        color = "ğŸŸ¢"
    elif minority_percentage >= 20:
        balance_status = "MODERADAMENTE DESBALANCEADO"
        impact = "MEDIO"
        color = "ğŸŸ¡"
    else:
        balance_status = "SEVERAMENTE DESBALANCEADO"
        impact = "ALTO"
        color = "ğŸ”´"

    print(f"   â€¢ Estado: {color} {balance_status}")
    print(f"   â€¢ Clase minoritaria: {minority_percentage:.1f}%")
    print(f"   â€¢ Impacto potencial en modelos: {impact}")

    # Recomendaciones especÃ­ficas
    print(f"\nğŸ’¡ RECOMENDACIONES PARA MODELADO:")
    if minority_percentage >= 40:
        print("   âœ… El dataset estÃ¡ bien balanceado")
        print("   âœ… Puedes usar mÃ©tricas estÃ¡ndar (accuracy, precision, recall)")
        print("   âœ… No requiere tÃ©cnicas especiales de balanceo")
    elif minority_percentage >= 20:
        print("   âš ï¸ Considera usar tÃ©cnicas de balanceo:")
        print("     - Ajustar pesos de clase (class_weight='balanced')")
        print("     - Usar mÃ©tricas como F1-score, AUC-ROC")
        print("     - Validar con stratified sampling")
        print("     - Considerar threshold tuning")
    else:
        print("   ğŸš¨ Es crÃ­tico aplicar tÃ©cnicas de balanceo:")
        print("     - SMOTE (Synthetic Minority Oversampling)")
        print("     - Random undersampling de clase mayoritaria")
        print("     - Ensemble methods (Random Forest con balanced weights)")
        print("     - MÃ©tricas: precision, recall, F1-score (NO accuracy)")

    print("="*70)
    return class_counts, class_proportions, ratio

# Ejecutar anÃ¡lisis de balance de clases
class_counts, class_proportions, balance_ratio = analyze_class_balance(df)

"""## Visualizacion del balance de clases"""

# Crear visualizaciones del balance de clases
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# GrÃ¡fico de barras - Conteo absoluto
axes[0].bar(['No Churn', 'Churn'], class_counts.values,
           color=['skyblue', 'salmon'], alpha=0.8, edgecolor='black')
axes[0].set_title('DistribuciÃ³n Absoluta de Clases', fontsize=14, fontweight='bold')
axes[0].set_ylabel('NÃºmero de Clientes')
axes[0].grid(axis='y', alpha=0.3)
for i, v in enumerate(class_counts.values):
    axes[0].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold', fontsize=12)

# GrÃ¡fico circular - Proporciones
axes[1].pie(class_counts.values, labels=['No Churn', 'Churn'],
           autopct='%1.1f%%', startangle=90,
           colors=['skyblue', 'salmon'],
           explode=(0.05, 0.05))  # Separar un poco las secciones
axes[1].set_title('ProporciÃ³n de Clases', fontsize=14, fontweight='bold')

# GrÃ¡fico de ratio - Desbalance
ratio_data = [balance_ratio, 1]
bars = axes[2].bar(['No Churn', 'Churn'], ratio_data,
                   color=['skyblue', 'salmon'], alpha=0.8, edgecolor='black')
axes[2].set_title('Ratio de Desbalance', fontsize=14, fontweight='bold')
axes[2].set_ylabel('Ratio (relativo a Churn)')
axes[2].grid(axis='y', alpha=0.3)
axes[2].text(0, balance_ratio + 0.1, f'{balance_ratio:.1f}:1',
            ha='center', fontweight='bold', fontsize=12)
axes[2].text(1, 1 + 0.1, '1:1', ha='center', fontweight='bold', fontsize=12)

plt.tight_layout()
plt.show()

"""## Impacto en modelos de machine learning"""

print("ğŸ¤– IMPACTO DEL DESBALANCE EN MODELOS ML")
print("="*60)

# AnÃ¡lisis de impacto especÃ­fico
minority_pct = min(class_proportions) * 100

print(f"\nğŸ“Š SITUACIÃ“N ACTUAL:")
print(f"   â€¢ Clase minoritaria (Churn): {minority_pct:.1f}%")
print(f"   â€¢ Ratio de desbalance: {balance_ratio:.1f}:1")

print(f"\nâš ï¸ RIESGOS POTENCIALES:")
print("   â€¢ Los modelos pueden tener sesgo hacia la clase mayoritaria")
print("   â€¢ Accuracy puede ser engaÃ±osa (un modelo que prediga siempre 'No Churn' tendrÃ­a 73.5% accuracy)")
print("   â€¢ Menor sensibilidad para detectar churn (falsos negativos)")
print("   â€¢ Impacto en mÃ©tricas de negocio (clientes perdidos no detectados)")

print(f"\nâœ… ESTRATEGIAS RECOMENDADAS:")
print("   1. ğŸ¯ MÃ‰TRICAS APROPIADAS:")
print("      - Precision, Recall, F1-score para clase minoritaria")
print("      - AUC-ROC y AUC-PR (Precision-Recall)")
print("      - Confusion Matrix detallada")
print("      - Evitar accuracy como mÃ©trica principal")

print("\n   2. ğŸ”§ TÃ‰CNICAS DE BALANCEO:")
print("      - class_weight='balanced' en algoritmos")
print("      - SMOTE para oversampling inteligente")
print("      - Random undersampling de clase mayoritaria")
print("      - Ensemble methods con balanced weights")

print("\n   3. ğŸ“Š VALIDACIÃ“N:")
print("      - Stratified K-Fold Cross-Validation")
print("      - Validar en conjunto de prueba balanceado")
print("      - AnÃ¡lisis de curvas ROC y Precision-Recall")

print("\n   4. ğŸ›ï¸ OPTIMIZACIÃ“N:")
print("      - Threshold tuning para optimizar recall")
print("      - Cost-sensitive learning")
print("      - Ensemble de modelos con diferentes enfoques")

print(f"\nğŸ’¼ IMPACTO EN NEGOCIO:")
print("   â€¢ Costo de falsos negativos: perder clientes valiosos")
print("   â€¢ Costo de falsos positivos: campaÃ±as de retenciÃ³n innecesarias")
print("   â€¢ RecomendaciÃ³n: priorizar recall para detectar mÃ¡s churn")
print("="*60)

"""## Analisis de escalas de variables numericas"""

def analyze_numerical_scales(df):
    """
    Analiza las escalas de las variables numÃ©ricas para evaluar
    la necesidad de normalizaciÃ³n/estandarizaciÃ³n
    """
    print("="*80)
    print("ğŸ“ ANÃLISIS DE ESCALAS DE VARIABLES NUMÃ‰RICAS")
    print("="*80)

    # Identificar variables numÃ©ricas (excluyendo binarias)
    numerical_vars = []
    for col in df.columns:
        if col == 'churn':  # Skip target
            continue
        if df[col].dtype in ['int64', 'float64'] and df[col].nunique() > 2:
            numerical_vars.append(col)

    print(f"\nğŸ”¢ VARIABLES NUMÃ‰RICAS IDENTIFICADAS: {numerical_vars}")

    # AnÃ¡lisis detallado de cada variable
    scale_analysis = {}

    for var in numerical_vars:
        if var in df.columns:
            stats = {
                'min': df[var].min(),
                'max': df[var].max(),
                'mean': df[var].mean(),
                'std': df[var].std(),
                'range': df[var].max() - df[var].min()
            }
            scale_analysis[var] = stats

            print(f"\nğŸ“Š {var.upper()}:")
            print(f"   â€¢ Min: {stats['min']:,.2f}")
            print(f"   â€¢ Max: {stats['max']:,.2f}")
            print(f"   â€¢ Mean: {stats['mean']:,.2f}")
            print(f"   â€¢ Std: {stats['std']:,.2f}")
            print(f"   â€¢ Rango: {stats['range']:,.2f}")

    # ComparaciÃ³n de escalas
    print(f"\nâš–ï¸ COMPARACIÃ“N DE ESCALAS:")
    ranges = [(var, data['range']) for var, data in scale_analysis.items()]
    ranges.sort(key=lambda x: x[1], reverse=True)

    for i, (var, range_val) in enumerate(ranges, 1):
        print(f"   {i}. {var}: {range_val:,.2f}")

    # Determinar necesidad de escalado
    max_range = max([data['range'] for data in scale_analysis.values()])
    min_range = min([data['range'] for data in scale_analysis.values()])
    scale_ratio = max_range / min_range if min_range > 0 else float('inf')

    print(f"\nğŸ¯ EVALUACIÃ“N DE NECESIDAD DE ESCALADO:")
    print(f"   â€¢ Ratio entre escalas: {scale_ratio:,.1f}:1")

    if scale_ratio > 100:
        need_scaling = "CRÃTICA"
        color = "ğŸ”´"
    elif scale_ratio > 10:
        need_scaling = "ALTA"
        color = "ğŸŸ¡"
    else:
        need_scaling = "BAJA"
        color = "ğŸŸ¢"

    print(f"   â€¢ Necesidad de escalado: {color} {need_scaling}")

    print(f"\nğŸ“š RECOMENDACIONES POR TIPO DE MODELO:")
    print("   ğŸ¯ MODELOS SENSIBLES A LA ESCALA (requieren normalizaciÃ³n):")
    print("     - KNN (K-Nearest Neighbors)")
    print("     - SVM (Support Vector Machine)")
    print("     - RegresiÃ³n LogÃ­stica")
    print("     - Redes Neuronales")
    print("     - K-Means Clustering")

    print("\n   ğŸŒ³ MODELOS NO SENSIBLES A LA ESCALA (no requieren normalizaciÃ³n):")
    print("     - Decision Trees")
    print("     - Random Forest")
    print("     - XGBoost/LightGBM")
    print("     - Gradient Boosting")

    print("="*80)
    return numerical_vars, scale_analysis

# Ejecutar anÃ¡lisis de escalas
numerical_vars, scale_analysis = analyze_numerical_scales(df)

"""## Implementacion de metodos de normalizacion/estandarizacion"""

# =============================================================================
# IMPLEMENTACIÃ“N DE MÃ‰TODOS DE NORMALIZACIÃ“N/ESTANDARIZACIÃ“N
# =============================================================================

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split

def implement_scaling_methods(df, numerical_vars):
    """
    Implementa diferentes mÃ©todos de escalado para variables numÃ©ricas
    """
    print("ğŸ”§ IMPLEMENTACIÃ“N DE MÃ‰TODOS DE ESCALADO")
    print("="*60)

    # Separar features y target
    X = df.drop('churn', axis=1)
    y = df['churn']

    # DivisiÃ³n inicial train/test (estratificada para balance de clases)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"ğŸ“Š DIVISIÃ“N DEL DATASET:")
    print(f"   â€¢ Train: {X_train.shape[0]:,} muestras")
    print(f"   â€¢ Test: {X_test.shape[0]:,} muestras")
    print(f"   â€¢ Features: {X_train.shape[1]}")

    # Crear diferentes escaladores
    scalers = {
        'StandardScaler': StandardScaler(),      # Media=0, Std=1 (Z-score)
        'MinMaxScaler': MinMaxScaler(),          # Rango [0,1]
        'RobustScaler': RobustScaler()           # Mediana y IQR (robusto a outliers)
    }

    # Aplicar cada mÃ©todo de escalado
    scaled_datasets = {}

    for scaler_name, scaler in scalers.items():
        print(f"\nğŸ”§ APLICANDO {scaler_name.upper()}:")

        # Crear copias de los datasets
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()

        # IMPORTANTE: Fit solo en train, transform en ambos
        X_train_scaled[numerical_vars] = scaler.fit_transform(X_train[numerical_vars])
        X_test_scaled[numerical_vars] = scaler.transform(X_test[numerical_vars])

        # Guardar datasets escalados
        scaled_datasets[scaler_name] = {
            'scaler': scaler,
            'X_train': X_train_scaled,
            'X_test': X_test_scaled,
            'y_train': y_train,
            'y_test': y_test
        }

        print(f"   âœ… {scaler_name} aplicado correctamente")

        # Mostrar estadÃ­sticas post-escalado para una variable ejemplo
        if numerical_vars:
            example_var = numerical_vars[0]
            original_stats = X_train[example_var].describe()
            scaled_stats = X_train_scaled[example_var].describe()

            print(f"   ğŸ“Š Ejemplo con '{example_var}':")
            print(f"      Original - Min: {original_stats['min']:.2f}, Max: {original_stats['max']:.2f}")
            print(f"      Escalado - Min: {scaled_stats['min']:.2f}, Max: {scaled_stats['max']:.2f}")

    # Dataset sin escalado (para modelos de Ã¡rboles)
    scaled_datasets['NoScaling'] = {
        'scaler': None,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test
    }

    print(f"\nâœ… TODOS LOS MÃ‰TODOS DE ESCALADO IMPLEMENTADOS")
    print(f"   ğŸ“¦ Datasets disponibles: {list(scaled_datasets.keys())}")

    return scaled_datasets

# Implementar mÃ©todos de escalado
scaled_datasets = implement_scaling_methods(df, numerical_vars)

"""## Comparacion visual de metodos de escalado"""

# Visualizar el efecto de los diferentes mÃ©todos de escalado
if numerical_vars:
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.ravel()

    # Seleccionar hasta 4 variables numÃ©ricas para visualizar
    vars_to_plot = numerical_vars[:4] if len(numerical_vars) >= 4 else numerical_vars

    for i, var in enumerate(vars_to_plot):
        if i < 4:
            # Datos de cada mÃ©todo de escalado
            original_data = scaled_datasets['NoScaling']['X_train'][var]
            standard_data = scaled_datasets['StandardScaler']['X_train'][var]
            minmax_data = scaled_datasets['MinMaxScaler']['X_train'][var]
            robust_data = scaled_datasets['RobustScaler']['X_train'][var]

            # Crear boxplot comparativo
            box_data = [original_data, standard_data, minmax_data, robust_data]
            labels = ['Original', 'Standard', 'MinMax', 'Robust']

            axes[i].boxplot(box_data, tick_labels=labels)  # Usar tick_labels en lugar de labels
            axes[i].set_title(f'ComparaciÃ³n de Escalado: {var}', fontweight='bold')
            axes[i].set_ylabel('Valores')
            axes[i].grid(True, alpha=0.3)
            axes[i].tick_params(axis='x', rotation=45)

    # Ocultar subplots no utilizados
    for i in range(len(vars_to_plot), 4):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.show()

"""## Guia seleccion de metodo de escalado segun modelo"""

# =============================================================================
# GUÃA DE SELECCIÃ“N DE MÃ‰TODO DE ESCALADO SEGÃšN EL MODELO
# =============================================================================

print("ğŸ¯ GUÃA DE SELECCIÃ“N DE ESCALADO POR MODELO")
print("="*70)

scaling_recommendations = {
    "ğŸ”´ MODELOS QUE REQUIEREN ESCALADO": {
        "KNN (K-Nearest Neighbors)": {
            "escalado_recomendado": "StandardScaler o MinMaxScaler",
            "razon": "Usa distancia euclidiana - sensible a escala"
        },
        "SVM (Support Vector Machine)": {
            "escalado_recomendado": "StandardScaler",
            "razon": "Encuentra hiperplanos Ã³ptimos - requiere escalas similares"
        },
        "RegresiÃ³n LogÃ­stica": {
            "escalado_recomendado": "StandardScaler",
            "razon": "OptimizaciÃ³n por gradiente - converge mejor con escalas similares"
        },
        "Redes Neuronales": {
            "escalado_recomendado": "StandardScaler o MinMaxScaler",
            "razon": "Funciones de activaciÃ³n sensibles a la escala de entrada"
        },
        "K-Means Clustering": {
            "escalado_recomendado": "StandardScaler",
            "razon": "Basado en distancias - variables con mayor escala dominan"
        }
    },

    "ğŸŸ¢ MODELOS QUE NO REQUIEREN ESCALADO": {
        "Decision Trees": {
            "escalado_recomendado": "NoScaling",
            "razon": "Divisiones basadas en umbrales - escala no importa"
        },
        "Random Forest": {
            "escalado_recomendado": "NoScaling",
            "razon": "Ensemble de Ã¡rboles - hereda propiedades de Decision Trees"
        },
        "XGBoost/LightGBM": {
            "escalado_recomendado": "NoScaling",
            "razon": "Gradient boosting de Ã¡rboles - no sensible a escala"
        },
        "Gradient Boosting": {
            "escalado_recomendado": "NoScaling",
            "razon": "Basado en Ã¡rboles - escalado no mejora performance"
        }
    }
}

for category, models in scaling_recommendations.items():
    print(f"\n{category}:")
    for model, info in models.items():
        print(f"   ğŸ“Š {model}:")
        print(f"      â€¢ Escalado recomendado: {info['escalado_recomendado']}")
        print(f"      â€¢ RazÃ³n: {info['razon']}")

print(f"\nğŸ’¡ MÃ‰TODOS DE ESCALADO DISPONIBLES:")
print("   ğŸ”§ StandardScaler (Z-score normalization):")
print("      â€¢ Transforma a media=0, std=1")
print("      â€¢ Mejor para distribuciones normales")
print("      â€¢ Preserva la forma de la distribuciÃ³n")

print("\n   ğŸ”§ MinMaxScaler (Min-Max normalization):")
print("      â€¢ Transforma al rango [0,1]")
print("      â€¢ Mejor cuando conoces los lÃ­mites de las variables")
print("      â€¢ Sensible a outliers")

print("\n   ğŸ”§ RobustScaler (Robust normalization):")
print("      â€¢ Usa mediana y rango intercuartÃ­lico")
print("      â€¢ Robusto a outliers")
print("      â€¢ Mejor cuando hay valores atÃ­picos")

print(f"\nğŸ¯ RECOMENDACIÃ“N PARA ESTE DATASET:")
print("   â€¢ Ratio de escalas: 2,586.9:1 (CRÃTICO)")
print("   â€¢ charges_total domina completamente (rango: 8,666)")
print("   â€¢ Recomendado: StandardScaler para la mayorÃ­a de modelos")
print("   â€¢ Alternativa: RobustScaler si hay muchos outliers en charges_total")
print("="*70)

"""## Analisis de matriz de correlacion"""

def analyze_correlation_matrix(df, scaled_datasets=None):
    """
    Analiza las correlaciones entre variables, especialmente con la variable target
    """
    print("="*80)
    print("ğŸ”— ANÃLISIS DE MATRIZ DE CORRELACIÃ“N")
    print("="*80)

    # Calcular matriz de correlaciÃ³n completa
    correlation_matrix = df.corr()

    # Correlaciones con la variable target
    target_correlations = correlation_matrix['churn'].drop('churn').sort_values(key=abs, ascending=False)

    print(f"\nğŸ¯ CORRELACIONES CON LA VARIABLE TARGET (CHURN):")
    print("-" * 60)

    for i, (feature, corr) in enumerate(target_correlations.head(10).items(), 1):
        direction = "ğŸ“ˆ Positiva" if corr > 0 else "ğŸ“‰ Negativa"
        strength = "ğŸ”´ Fuerte" if abs(corr) > 0.3 else "ğŸŸ¡ Moderada" if abs(corr) > 0.1 else "ğŸŸ¢ DÃ©bil"
        print(f"   {i:2d}. {feature:25} | {corr:6.3f} | {direction} | {strength}")

    # Identificar correlaciones fuertes entre features (multicolinealidad)
    print(f"\nâš ï¸ ANÃLISIS DE MULTICOLINEALIDAD:")
    print("-" * 60)

    # Crear matriz sin la variable target
    feature_corr = correlation_matrix.drop('churn', axis=0).drop('churn', axis=1)

    # Encontrar pares con correlaciÃ³n alta
    high_corr_pairs = []
    for i in range(len(feature_corr.columns)):
        for j in range(i+1, len(feature_corr.columns)):
            corr_val = feature_corr.iloc[i, j]
            if abs(corr_val) > 0.7:  # Umbral para correlaciÃ³n alta
                high_corr_pairs.append((feature_corr.columns[i], feature_corr.columns[j], corr_val))

    if high_corr_pairs:
        print("   ğŸ“Š CORRELACIONES ALTAS ENTRE FEATURES (|r| > 0.7):")
        important_pairs = [pair for pair in high_corr_pairs if abs(pair[2]) < 1.0][:10]  # Top 10 excluding perfect correlations
        for feat1, feat2, corr in sorted(important_pairs, key=lambda x: abs(x[2]), reverse=True):
            print(f"      â€¢ {feat1} â†” {feat2}: {corr:.3f}")
        if len(high_corr_pairs) > 10:
            print(f"      ... y {len(high_corr_pairs) - 10} pares mÃ¡s")
    else:
        print("   âœ… No se detectaron correlaciones altas entre features (|r| > 0.7)")

    # Recomendaciones para feature selection
    print(f"\nğŸ’¡ RECOMENDACIONES PARA SELECCIÃ“N DE FEATURES:")

    strong_features = target_correlations[abs(target_correlations) > 0.2]
    moderate_features = target_correlations[(abs(target_correlations) > 0.1) & (abs(target_correlations) <= 0.2)]
    weak_features = target_correlations[abs(target_correlations) <= 0.1]

    print(f"   ğŸ”´ FEATURES FUERTES (|r| > 0.2): {len(strong_features)}")
    for feat in strong_features.head(5).index:
        print(f"      â€¢ {feat} ({target_correlations[feat]:.3f})")

    print(f"\n   ğŸŸ¡ FEATURES MODERADAS (0.1 < |r| â‰¤ 0.2): {len(moderate_features)}")
    for feat in moderate_features.head(3).index:
        print(f"      â€¢ {feat} ({target_correlations[feat]:.3f})")

    print(f"\n   ğŸŸ¢ FEATURES DÃ‰BILES (|r| â‰¤ 0.1): {len(weak_features)}")
    print(f"      â€¢ Considerar eliminar para simplificar modelo")

    print("="*80)
    return correlation_matrix, target_correlations, high_corr_pairs

# Ejecutar anÃ¡lisis de correlaciÃ³n
correlation_matrix, target_correlations, high_corr_pairs = analyze_correlation_matrix(df)

"""## Visualizacion de matriz de correlacion"""

# Crear visualizaciones de la matriz de correlaciÃ³n
fig, axes = plt.subplots(2, 2, figsize=(20, 16))

# 1. Matriz de correlaciÃ³n completa
plt.subplot(2, 2, 1)
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # MÃ¡scara para mostrar solo la mitad
sns.heatmap(correlation_matrix,
           mask=mask,
           annot=False,
           cmap='RdBu_r',
           center=0,
           square=True,
           fmt='.2f',
           cbar_kws={"shrink": 0.8})
plt.title('Matriz de CorrelaciÃ³n Completa', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

# 2. Correlaciones con variable target
plt.subplot(2, 2, 2)
target_corr_sorted = target_correlations.head(15)  # Top 15
colors = ['red' if x < 0 else 'blue' for x in target_corr_sorted.values]
bars = plt.barh(range(len(target_corr_sorted)), target_corr_sorted.values, color=colors, alpha=0.7)
plt.yticks(range(len(target_corr_sorted)), target_corr_sorted.index)
plt.xlabel('CorrelaciÃ³n con Churn')
plt.title('Top 15 Correlaciones con Churn', fontsize=14, fontweight='bold')
plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
plt.grid(axis='x', alpha=0.3)

# Agregar valores en las barras
for i, (bar, val) in enumerate(zip(bars, target_corr_sorted.values)):
    plt.text(val + (0.01 if val > 0 else -0.01), i, f'{val:.3f}',
             ha='left' if val > 0 else 'right', va='center', fontweight='bold')

# 3. Heatmap enfocado en variables numÃ©ricas
plt.subplot(2, 2, 3)
numerical_vars_with_target = ['churn'] + numerical_vars
if all(var in df.columns for var in numerical_vars_with_target):
    numerical_corr = df[numerical_vars_with_target].corr()
    sns.heatmap(numerical_corr,
               annot=True,
               cmap='RdBu_r',
               center=0,
               square=True,
               fmt='.3f',
               cbar_kws={"shrink": 0.8})
    plt.title('CorrelaciÃ³n: Variables NumÃ©ricas vs Churn', fontsize=14, fontweight='bold')

# 4. DistribuciÃ³n de correlaciones
plt.subplot(2, 2, 4)
all_correlations = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, 1)]
plt.hist(all_correlations, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Sin correlaciÃ³n')
plt.axvline(x=0.3, color='orange', linestyle='--', alpha=0.7, label='CorrelaciÃ³n moderada')
plt.axvline(x=-0.3, color='orange', linestyle='--', alpha=0.7)
plt.xlabel('Valor de CorrelaciÃ³n')
plt.ylabel('Frecuencia')
plt.title('DistribuciÃ³n de Correlaciones', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

"""## Analisis detallado de features mas importantes"""

def analyze_top_features(target_correlations, correlation_matrix, df):
    """
    Analiza en detalle las features mÃ¡s correlacionadas con churn
    """
    print("ğŸ† ANÃLISIS DETALLADO DE FEATURES MÃS IMPORTANTES")
    print("="*70)

    # Top 5 features por correlaciÃ³n absoluta
    top_features = target_correlations.head(5)

    for i, (feature, corr) in enumerate(top_features.items(), 1):
        print(f"\n{i}. ğŸ“Š {feature.upper()}:")
        print(f"   â€¢ CorrelaciÃ³n con churn: {corr:.4f}")

        # EstadÃ­sticas descriptivas por grupo de churn
        feature_stats = df.groupby('churn')[feature].agg(['count', 'mean', 'std', 'min', 'max'])

        print(f"   â€¢ EstadÃ­sticas por grupo:")
        print(f"     - No Churn (0): Media={feature_stats.loc[0, 'mean']:.2f}, Std={feature_stats.loc[0, 'std']:.2f}")
        print(f"     - Churn (1): Media={feature_stats.loc[1, 'mean']:.2f}, Std={feature_stats.loc[1, 'std']:.2f}")

        # Diferencia de medias
        mean_diff = feature_stats.loc[1, 'mean'] - feature_stats.loc[0, 'mean']
        print(f"     - Diferencia de medias: {mean_diff:.2f}")

        # InterpretaciÃ³n del negocio
        if feature in numerical_vars:
            if corr > 0:
                interpretation = f"Mayor {feature} â†’ Mayor probabilidad de churn"
            else:
                interpretation = f"Mayor {feature} â†’ Menor probabilidad de churn"
        else:
            if corr > 0:
                interpretation = f"Presencia de {feature} â†’ Mayor probabilidad de churn"
            else:
                interpretation = f"Presencia de {feature} â†’ Menor probabilidad de churn"

        print(f"   â€¢ InterpretaciÃ³n: {interpretation}")

    # Crear grÃ¡fico de distribuciÃ³n para variables top
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()

    for i, (feature, corr) in enumerate(top_features.head(6).items()):
        if i < 6:
            if feature in numerical_vars:
                # Boxplot para variables numÃ©ricas
                churn_groups = [df[df['churn'] == 0][feature], df[df['churn'] == 1][feature]]
                axes[i].boxplot(churn_groups, tick_labels=['No Churn', 'Churn'])
                axes[i].set_ylabel(feature)
            else:
                # Barplot para variables categÃ³ricas
                cross_tab = pd.crosstab(df[feature], df['churn'], normalize='columns') * 100
                cross_tab.plot(kind='bar', ax=axes[i], color=['skyblue', 'salmon'])
                axes[i].set_ylabel('Porcentaje (%)')
                axes[i].legend(['No Churn', 'Churn'])
                axes[i].tick_params(axis='x', rotation=45)

            axes[i].set_title(f'{feature}\n(CorrelaciÃ³n: {corr:.3f})', fontweight='bold')
            axes[i].grid(True, alpha=0.3)

    # Ocultar subplots no utilizados
    for i in range(len(top_features.head(6)), 6):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.show()

    print("="*70)

# Ejecutar anÃ¡lisis detallado
analyze_top_features(target_correlations, correlation_matrix, df)

"""## Recomendaciones para seleccion de features"""

print("ğŸ“‹ RECOMENDACIONES FINALES PARA FEATURE SELECTION")
print("="*70)

# Clasificar features por fuerza de correlaciÃ³n
strong_features = target_correlations[abs(target_correlations) > 0.2]
moderate_features = target_correlations[(abs(target_correlations) > 0.1) & (abs(target_correlations) <= 0.2)]
weak_features = target_correlations[abs(target_correlations) <= 0.1]

print(f"\nğŸ”´ FEATURES IMPRESCINDIBLES (CorrelaciÃ³n fuerte |r| > 0.2):")
for i, (feat, corr) in enumerate(strong_features.items(), 1):
    direction = "â†—ï¸" if corr > 0 else "â†˜ï¸"
    print(f"   {i:2d}. {feat:30} | {corr:6.3f} | {direction}")

print(f"\nğŸŸ¡ FEATURES IMPORTANTES (CorrelaciÃ³n moderada 0.1 < |r| â‰¤ 0.2):")
for i, (feat, corr) in enumerate(moderate_features.items(), 1):
    direction = "â†—ï¸" if corr > 0 else "â†˜ï¸"
    print(f"   {i:2d}. {feat:30} | {corr:6.3f} | {direction}")

print(f"\nğŸŸ¢ FEATURES OPCIONALES (CorrelaciÃ³n dÃ©bil |r| â‰¤ 0.1): {len(weak_features)}")
print("   â€¢ Pueden eliminarse para simplificar el modelo sin perder mucha informaciÃ³n")

# Problemas de multicolinealidad
print(f"\nâš ï¸ PROBLEMAS DE MULTICOLINEALIDAD DETECTADOS:")
print(f"   â€¢ {len(high_corr_pairs)} pares de variables con correlaciÃ³n > 0.7")
print("   â€¢ Variables 'No internet service' son perfectamente correlacionadas")
print("   â€¢ charges_monthly y charges_daily son idÃ©nticas (r=1.0)")
print("   â€¢ tenure y charges_total tienen alta correlaciÃ³n (r=0.826)")

print(f"\nğŸ’¡ ESTRATEGIAS RECOMENDADAS:")
print("   1. ğŸ¯ FEATURE SELECTION:")
print("      â€¢ Incluir todas las features fuertes (11 variables)")
print("      â€¢ Evaluar features moderadas segÃºn performance del modelo")
print("      â€¢ Considerar eliminar features dÃ©biles")

print("\n   2. ğŸ”§ MANEJO DE MULTICOLINEALIDAD:")
print("      â€¢ Eliminar charges_daily (idÃ©ntica a charges_monthly)")
print("      â€¢ Mantener solo una variable 'No internet service' representativa")
print("      â€¢ Considerar PCA para variables altamente correlacionadas")

print("\n   3. ğŸ“Š INTERPRETACIÃ“N DE NEGOCIO:")
print("      â€¢ PREDICTORES POSITIVOS DE CHURN:")
print("        - Fiber optic internet service")
print("        - Electronic check payment method")
print("        - Charges mensuales altos")
print("      â€¢ PREDICTORES NEGATIVOS DE CHURN:")
print("        - Mayor tenure (antigÃ¼edad)")
print("        - Contratos de Two year")
print("        - No tener internet service")

print("="*70)

"""## 6. AnÃ¡lisis EspecÃ­fico: Tiempo de Contrato y Gasto Total vs CancelaciÃ³n

En esta secciÃ³n se profundiza en la investigaciÃ³n de las relaciones especÃ­ficas entre:
1. **Tiempo de contrato (tenure)** Ã— CancelaciÃ³n
2. **Gasto total (charges_total)** Ã— CancelaciÃ³n  
3. **Patrones de comportamiento** relacionados con el churn

Este anÃ¡lisis utiliza visualizaciones avanzadas como boxplots, scatter plots y grÃ¡ficos de barras segmentados para identificar tendencias crÃ­ticas y generar recomendaciones de negocio especÃ­ficas.
"""

# Importaciones adicionales para el anÃ¡lisis especÃ­fico
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

def analyze_contract_time_vs_churn(df):
    """
    AnÃ¡lisis detallado de la relaciÃ³n entre tiempo de contrato (tenure) y churn
    """
    print("="*80)
    print("â° ANÃLISIS: TIEMPO DE CONTRATO vs CANCELACIÃ“N")
    print("="*80)

    # EstadÃ­sticas descriptivas por grupo
    tenure_stats = df.groupby('churn')['tenure'].agg([
        'count', 'mean', 'median', 'std', 'min', 'max',
        lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)
    ]).round(2)
    tenure_stats.columns = ['Count', 'Mean', 'Median', 'Std', 'Min', 'Max', 'Q1', 'Q3']

    print("\nğŸ“Š ESTADÃSTICAS DE TENURE POR GRUPO:")
    print("-" * 60)
    print("Grupo Churn:")
    print(f"   â€¢ No Churn (0): Media = {tenure_stats.loc[0, 'Mean']:.1f} meses")
    print(f"                   Mediana = {tenure_stats.loc[0, 'Median']:.1f} meses")
    print(f"                   Std = {tenure_stats.loc[0, 'Std']:.1f} meses")
    print(f"                   Rango = {tenure_stats.loc[0, 'Min']:.0f} - {tenure_stats.loc[0, 'Max']:.0f} meses")

    print(f"\n   â€¢ Churn (1):    Media = {tenure_stats.loc[1, 'Mean']:.1f} meses")
    print(f"                   Mediana = {tenure_stats.loc[1, 'Median']:.1f} meses")
    print(f"                   Std = {tenure_stats.loc[1, 'Std']:.1f} meses")
    print(f"                   Rango = {tenure_stats.loc[1, 'Min']:.0f} - {tenure_stats.loc[1, 'Max']:.0f} meses")

    # Diferencias significativas
    mean_diff = tenure_stats.loc[1, 'Mean'] - tenure_stats.loc[0, 'Mean']
    median_diff = tenure_stats.loc[1, 'Median'] - tenure_stats.loc[0, 'Median']

    print(f"\nğŸ“ˆ DIFERENCIAS:")
    print(f"   â€¢ Diferencia de medias: {mean_diff:.1f} meses")
    print(f"   â€¢ Diferencia de medianas: {median_diff:.1f} meses")
    print(f"   â€¢ Los clientes que cancelan tienen {'MENOR' if mean_diff < 0 else 'MAYOR'} tiempo de contrato promedio")

    # Test estadÃ­stico
    no_churn_tenure = df[df['churn'] == 0]['tenure']
    churn_tenure = df[df['churn'] == 1]['tenure']
    t_stat, p_value = stats.ttest_ind(no_churn_tenure, churn_tenure)

    print(f"\nğŸ”¬ TEST ESTADÃSTICO (t-test):")
    print(f"   â€¢ t-statistic: {t_stat:.4f}")
    print(f"   â€¢ p-value: {p_value:.6f}")
    print(f"   â€¢ Significativo: {'SÃ' if p_value < 0.05 else 'NO'} (Î± = 0.05)")

    # SegmentaciÃ³n por perÃ­odos de tenure
    print(f"\nğŸ¯ ANÃLISIS POR SEGMENTOS DE TENURE:")
    print("-" * 60)

    # Crear segmentos
    df['tenure_segment'] = pd.cut(df['tenure'],
                                 bins=[0, 12, 24, 36, 48, float('inf')],
                                 labels=['0-12 meses', '13-24 meses', '25-36 meses', '37-48 meses', '48+ meses'],
                                 include_lowest=True)

    # Calcular tasa de churn por segmento
    churn_by_segment = df.groupby('tenure_segment').agg({
        'churn': ['count', 'sum', 'mean']
    }).round(3)
    churn_by_segment.columns = ['Total_Clients', 'Churned_Clients', 'Churn_Rate']

    for segment in churn_by_segment.index:
        total = churn_by_segment.loc[segment, 'Total_Clients']
        churned = churn_by_segment.loc[segment, 'Churned_Clients']
        rate = churn_by_segment.loc[segment, 'Churn_Rate']
        print(f"   â€¢ {segment:12}: {churned:4.0f}/{total:4.0f} clientes ({rate*100:5.1f}% churn)")

    # CorrelaciÃ³n con churn
    correlation = df['tenure'].corr(df['churn'])
    print(f"\nğŸ”— CORRELACIÃ“N:")
    print(f"   â€¢ CorrelaciÃ³n tenure-churn: {correlation:.4f}")

    strength = "Fuerte" if abs(correlation) > 0.3 else "Moderada" if abs(correlation) > 0.1 else "DÃ©bil"
    direction = "negativa" if correlation < 0 else "positiva"
    print(f"   â€¢ InterpretaciÃ³n: CorrelaciÃ³n {strength} {direction}")

    print("="*80)
    return tenure_stats, churn_by_segment

# Ejecutar anÃ¡lisis de tiempo de contrato
tenure_stats, churn_by_segment = analyze_contract_time_vs_churn(df_cleaned)

def analyze_total_charges_vs_churn(df):
    """
    AnÃ¡lisis detallado de la relaciÃ³n entre gasto total y churn
    """
    print("="*80)
    print("ğŸ’° ANÃLISIS: GASTO TOTAL vs CANCELACIÃ“N")
    print("="*80)

    # EstadÃ­sticas descriptivas por grupo
    charges_stats = df.groupby('churn')['charges_total'].agg([
        'count', 'mean', 'median', 'std', 'min', 'max',
        lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)
    ]).round(2)
    charges_stats.columns = ['Count', 'Mean', 'Median', 'Std', 'Min', 'Max', 'Q1', 'Q3']

    print("\nğŸ“Š ESTADÃSTICAS DE CHARGES_TOTAL POR GRUPO:")
    print("-" * 60)
    print("Grupo Churn:")
    print(f"   â€¢ No Churn (0): Media = ${charges_stats.loc[0, 'Mean']:,.2f}")
    print(f"                   Mediana = ${charges_stats.loc[0, 'Median']:,.2f}")
    print(f"                   Std = ${charges_stats.loc[0, 'Std']:,.2f}")
    print(f"                   Rango = ${charges_stats.loc[0, 'Min']:,.2f} - ${charges_stats.loc[0, 'Max']:,.2f}")

    print(f"\n   â€¢ Churn (1):    Media = ${charges_stats.loc[1, 'Mean']:,.2f}")
    print(f"                   Mediana = ${charges_stats.loc[1, 'Median']:,.2f}")
    print(f"                   Std = ${charges_stats.loc[1, 'Std']:,.2f}")
    print(f"                   Rango = ${charges_stats.loc[1, 'Min']:,.2f} - ${charges_stats.loc[1, 'Max']:,.2f}")

    # Diferencias significativas
    mean_diff = charges_stats.loc[1, 'Mean'] - charges_stats.loc[0, 'Mean']
    median_diff = charges_stats.loc[1, 'Median'] - charges_stats.loc[0, 'Median']

    print(f"\nğŸ“ˆ DIFERENCIAS:")
    print(f"   â€¢ Diferencia de medias: ${mean_diff:,.2f}")
    print(f"   â€¢ Diferencia de medianas: ${median_diff:,.2f}")
    print(f"   â€¢ Los clientes que cancelan gastan {'MENOS' if mean_diff < 0 else 'MÃS'} en promedio")

    # Test estadÃ­stico
    no_churn_charges = df[df['churn'] == 0]['charges_total']
    churn_charges = df[df['churn'] == 1]['charges_total']
    t_stat, p_value = stats.ttest_ind(no_churn_charges, churn_charges)

    print(f"\nğŸ”¬ TEST ESTADÃSTICO (t-test):")
    print(f"   â€¢ t-statistic: {t_stat:.4f}")
    print(f"   â€¢ p-value: {p_value:.6f}")
    print(f"   â€¢ Significativo: {'SÃ' if p_value < 0.05 else 'NO'} (Î± = 0.05)")

    # SegmentaciÃ³n por niveles de gasto
    print(f"\nğŸ¯ ANÃLISIS POR SEGMENTOS DE GASTO:")
    print("-" * 60)

    # Crear segmentos basados en cuartiles
    df['charges_segment'] = pd.qcut(df['charges_total'],
                                   q=4,
                                   labels=['Bajo', 'Medio-Bajo', 'Medio-Alto', 'Alto'])

    # Calcular tasa de churn por segmento
    churn_by_charges = df.groupby('charges_segment').agg({
        'churn': ['count', 'sum', 'mean'],
        'charges_total': ['mean', 'min', 'max']
    }).round(3)

    for segment in churn_by_charges.index:
        total = churn_by_charges.loc[segment, ('churn', 'count')]
        churned = churn_by_charges.loc[segment, ('churn', 'sum')]
        rate = churn_by_charges.loc[segment, ('churn', 'mean')]
        avg_charges = churn_by_charges.loc[segment, ('charges_total', 'mean')]
        print(f"   â€¢ {segment:10}: {churned:4.0f}/{total:4.0f} clientes ({rate*100:5.1f}% churn) - Promedio: ${avg_charges:,.2f}")

    # CorrelaciÃ³n con churn
    correlation = df['charges_total'].corr(df['churn'])
    print(f"\nğŸ”— CORRELACIÃ“N:")
    print(f"   â€¢ CorrelaciÃ³n charges_total-churn: {correlation:.4f}")

    strength = "Fuerte" if abs(correlation) > 0.3 else "Moderada" if abs(correlation) > 0.1 else "DÃ©bil"
    direction = "negativa" if correlation < 0 else "positiva"
    print(f"   â€¢ InterpretaciÃ³n: CorrelaciÃ³n {strength} {direction}")

    print("="*80)
    return charges_stats, churn_by_charges

# Ejecutar anÃ¡lisis de gasto total
charges_stats, churn_by_charges = analyze_total_charges_vs_churn(df_cleaned)

# Visualizaciones especÃ­ficas para Tiempo de Contrato vs Churn
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('ğŸ“Š ANÃLISIS ESPECÃFICO: TIEMPO DE CONTRATO vs CHURN', fontsize=16, fontweight='bold')

# 1. Boxplot: Tenure vs Churn
ax1 = axes[0, 0]
tenure_data = [df_cleaned[df_cleaned['churn']==0]['tenure'], df_cleaned[df_cleaned['churn']==1]['tenure']]
bp1 = ax1.boxplot(tenure_data, labels=['No Churn', 'Churn'], patch_artist=True)
bp1['boxes'][0].set_facecolor('#3498db')
bp1['boxes'][1].set_facecolor('#e74c3c')
ax1.set_title('â° DistribuciÃ³n de Tiempo de Contrato por Churn\n(Boxplot)', fontweight='bold')
ax1.set_ylabel('Tiempo de Contrato (meses)')
ax1.grid(True, alpha=0.3)

# Agregar estadÃ­sticas
no_churn_median = df_cleaned[df_cleaned['churn']==0]['tenure'].median()
churn_median = df_cleaned[df_cleaned['churn']==1]['tenure'].median()
ax1.text(1, no_churn_median + 2, f'Mediana: {no_churn_median:.1f}', ha='center', fontweight='bold',
         bbox=dict(boxstyle="round,pad=0.3", facecolor="#3498db", alpha=0.7))
ax1.text(2, churn_median + 2, f'Mediana: {churn_median:.1f}', ha='center', fontweight='bold',
         bbox=dict(boxstyle="round,pad=0.3", facecolor="#e74c3c", alpha=0.7))

# 2. Histogram: Tenure vs Churn (overlapped)
ax2 = axes[0, 1]
ax2.hist(df_cleaned[df_cleaned['churn']==0]['tenure'], bins=30, alpha=0.7, label='No Churn', color='#3498db', density=True)
ax2.hist(df_cleaned[df_cleaned['churn']==1]['tenure'], bins=30, alpha=0.7, label='Churn', color='#e74c3c', density=True)
ax2.set_title('ğŸ“ˆ DistribuciÃ³n de Densidad: Tiempo de Contrato\n(Histograma Superpuesto)', fontweight='bold')
ax2.set_xlabel('Tiempo de Contrato (meses)')
ax2.set_ylabel('Densidad')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. Bar Chart: Churn Rate por Segmentos de Tenure
ax3 = axes[1, 0]
churn_by_tenure_viz = df_cleaned.groupby('tenure_segment')['churn'].agg(['count', 'mean']).reset_index()
bars1 = ax3.bar(churn_by_tenure_viz['tenure_segment'], churn_by_tenure_viz['mean'] * 100,
                color=['#85C1E9', '#F8C471', '#F1948A', '#D98880', '#CD6155'], alpha=0.8)
ax3.set_title('ğŸ“Š Tasa de Churn por Segmentos de Tiempo de Contrato', fontweight='bold')
ax3.set_xlabel('Segmentos de Tiempo de Contrato')
ax3.set_ylabel('Tasa de Churn (%)')
ax3.tick_params(axis='x', rotation=45)
ax3.grid(True, alpha=0.3, axis='y')

# Agregar valores en las barras
for bar, rate in zip(bars1, churn_by_tenure_viz['mean'] * 100):
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
            f'{rate:.1f}%', ha='center', fontweight='bold')

# 4. Scatter Plot: Tenure vs Charges_Total (colored by churn)
ax4 = axes[1, 1]
no_churn_data = df_cleaned[df_cleaned['churn']==0]
churn_data = df_cleaned[df_cleaned['churn']==1]

ax4.scatter(no_churn_data['tenure'], no_churn_data['charges_total'],
           c='#3498db', alpha=0.6, s=20, label='No Churn')
ax4.scatter(churn_data['tenure'], churn_data['charges_total'],
           c='#e74c3c', alpha=0.8, s=20, label='Churn')
ax4.set_title('ğŸ” RelaciÃ³n: Tiempo de Contrato vs Gasto Total', fontweight='bold')
ax4.set_xlabel('Tiempo de Contrato (meses)')
ax4.set_ylabel('Gasto Total ($)')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Visualizaciones especÃ­ficas para Gasto Total vs Churn
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('ğŸ’° ANÃLISIS ESPECÃFICO: GASTO TOTAL vs CHURN', fontsize=16, fontweight='bold')

# 1. Boxplot: Charges Total vs Churn
ax1 = axes[0, 0]
charges_data = [df_cleaned[df_cleaned['churn']==0]['charges_total'], df_cleaned[df_cleaned['churn']==1]['charges_total']]
bp2 = ax1.boxplot(charges_data, labels=['No Churn', 'Churn'], patch_artist=True)
bp2['boxes'][0].set_facecolor('#2ECC71')
bp2['boxes'][1].set_facecolor('#E67E22')
ax1.set_title('ğŸ’° DistribuciÃ³n de Gasto Total por Churn\n(Boxplot)', fontweight='bold')
ax1.set_ylabel('Gasto Total ($)')
ax1.grid(True, alpha=0.3)

# Agregar estadÃ­sticas
no_churn_median_charges = df_cleaned[df_cleaned['churn']==0]['charges_total'].median()
churn_median_charges = df_cleaned[df_cleaned['churn']==1]['charges_total'].median()
ax1.text(1, no_churn_median_charges + 500, f'Mediana: ${no_churn_median_charges:,.0f}',
         ha='center', fontweight='bold', bbox=dict(boxstyle="round,pad=0.3", facecolor="#2ECC71", alpha=0.7))
ax1.text(2, churn_median_charges + 500, f'Mediana: ${churn_median_charges:,.0f}',
         ha='center', fontweight='bold', bbox=dict(boxstyle="round,pad=0.3", facecolor="#E67E22", alpha=0.7))

# 2. Histogram: Charges Total vs Churn (overlapped)
ax2 = axes[0, 1]
ax2.hist(df_cleaned[df_cleaned['churn']==0]['charges_total'], bins=30, alpha=0.7, label='No Churn',
         color='#2ECC71', density=True)
ax2.hist(df_cleaned[df_cleaned['churn']==1]['charges_total'], bins=30, alpha=0.7, label='Churn',
         color='#E67E22', density=True)
ax2.set_title('ğŸ“ˆ DistribuciÃ³n de Densidad: Gasto Total\n(Histograma Superpuesto)', fontweight='bold')
ax2.set_xlabel('Gasto Total ($)')
ax2.set_ylabel('Densidad')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. Bar Chart: Churn Rate por Segmentos de Charges
ax3 = axes[1, 0]
churn_by_charges_viz = df_cleaned.groupby('charges_segment')['churn'].agg(['count', 'mean']).reset_index()
bars2 = ax3.bar(churn_by_charges_viz['charges_segment'], churn_by_charges_viz['mean'] * 100,
                color=['#A9DFBF', '#F9E79F', '#F5B7B1', '#EC7063'], alpha=0.8)
ax3.set_title('ğŸ’° Tasa de Churn por Segmentos de Gasto Total', fontweight='bold')
ax3.set_xlabel('Segmentos de Gasto Total')
ax3.set_ylabel('Tasa de Churn (%)')
ax3.tick_params(axis='x', rotation=45)
ax3.grid(True, alpha=0.3, axis='y')

# Agregar valores en las barras
for bar, rate in zip(bars2, churn_by_charges_viz['mean'] * 100):
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
            f'{rate:.1f}%', ha='center', fontweight='bold')

# 4. Violin Plot: DistribuciÃ³n mÃ¡s detallada de Charges por Churn
ax4 = axes[1, 1]
violin_data = [df_cleaned[df_cleaned['churn']==0]['charges_total'],
               df_cleaned[df_cleaned['churn']==1]['charges_total']]
parts = ax4.violinplot(violin_data, positions=[0, 1], showmeans=True, showmedians=True)
ax4.set_xticks([0, 1])
ax4.set_xticklabels(['No Churn', 'Churn'])
ax4.set_title('ğŸ» DistribuciÃ³n Detallada: Gasto Total por Churn\n(Violin Plot)', fontweight='bold')
ax4.set_ylabel('Gasto Total ($)')
ax4.grid(True, alpha=0.3)

# Colorear violin plots
for pc, color in zip(parts['bodies'], ['#2ECC71', '#E67E22']):
    pc.set_facecolor(color)
    pc.set_alpha(0.7)

plt.tight_layout()
plt.show()

# Matriz de CorrelaciÃ³n - Variables Clave vs Churn
key_variables = ['churn', 'tenure', 'charges_total', 'charges_monthly']
available_vars = [var for var in key_variables if var in df_cleaned.columns]

if len(available_vars) > 2:
    # Crear matriz de correlaciÃ³n
    corr_matrix = df_cleaned[available_vars].corr()

    # Crear figura
    plt.figure(figsize=(10, 8))

    # Crear heatmap
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix,
                mask=mask,
                annot=True,
                cmap='RdBu_r',
                center=0,
                square=True,
                fmt='.3f',
                cbar_kws={"shrink": 0.8},
                linewidths=0.5)

    plt.title('ğŸŒ¡ï¸ Matriz de CorrelaciÃ³n: Variables Clave vs Churn\n',
              fontsize=16, fontweight='bold')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    print("ğŸ“Š CORRELACIONES DESTACADAS:")
    print("="*50)
    for var in available_vars:
        if var != 'churn':
            corr_value = corr_matrix.loc[var, 'churn']
            strength = "Fuerte" if abs(corr_value) > 0.3 else "Moderada" if abs(corr_value) > 0.1 else "DÃ©bil"
            direction = "Negativa" if corr_value < 0 else "Positiva"
            print(f"â€¢ {var:15} â†’ Churn: {corr_value:6.3f} ({strength} {direction})")
else:
    print("âš ï¸ No hay suficientes variables para crear el heatmap")

"""### ğŸ“‹ Resumen de Insights Clave del AnÃ¡lisis EspecÃ­fico

#### â° **TIEMPO DE CONTRATO (TENURE)**
- **Diferencia crÃ­tica**: Los clientes SIN churn tienen aproximadamente **20 meses MÃS** de contrato en promedio
- **CorrelaciÃ³n fuerte negativa (-0.35)**: A menor tiempo de contrato, mayor probabilidad de churn
- **Segmento crÃ­tico**: Clientes de 0-12 meses presentan **47.7% de churn** (casi 1 de cada 2 cancela)
- **FidelizaciÃ³n**: DespuÃ©s de 48 meses, solo **9.5% de churn** (10x menor riesgo)

#### ğŸ’° **GASTO TOTAL (CHARGES_TOTAL)**
- **PatrÃ³n sorprendente**: Los clientes que cancelan gastan **$1,024 MENOS** en promedio
- **CorrelaciÃ³n moderada negativa (-0.20)**: Clientes de menor gasto = mayor riesgo de churn
- **Segmento de alto riesgo**: Gasto bajo presenta **43.5% de churn**
- **Clientes premium**: Alto gasto solo **14.5% de churn** (3x menor riesgo)

#### ğŸ” **TENDENCIAS IDENTIFICADAS**
1. **El primer aÃ±o es determinante**: 47.7% vs 9.5% despuÃ©s de 48 meses
2. **Paradoja del gasto**: Menor gasto â†’ Mayor probabilidad de cancelaciÃ³n
3. **CombinaciÃ³n crÃ­tica**: Clientes nuevos + bajo gasto = mÃ¡ximo riesgo
4. **Punto de inflexiÃ³n**: DespuÃ©s del primer aÃ±o, la retenciÃ³n mejora significativamente

### ğŸ’¡ **Recomendaciones EstratÃ©gicas de Negocio**

#### ğŸ¯ **ACCIONES INMEDIATAS - ALTA PRIORIDAD**

**1. Programa Intensivo de Onboarding (0-12 meses)**
- Implementar seguimiento proactivo semanal para clientes nuevos
- Crear programa de bienvenida con beneficios exclusivos primeros 6 meses
- Asignar representante de cuenta dedicado para perÃ­odo crÃ­tico inicial
- **Meta**: Reducir churn de 47.7% a 30% en primer aÃ±o

**2. Estrategia para Clientes de Bajo Gasto**
- DiseÃ±ar planes escalables y opciones de upgrade gradual
- Ofertas personalizadas para incrementar valor percibido
- Programas de fidelidad especÃ­ficos para segmento bajo gasto
- **Meta**: Reducir churn de 43.5% a 25% en segmento bajo gasto

#### ğŸ“Š **MÃ‰TRICAS DE MONITOREO CRÃTICAS**

```python
# MÃ©tricas clave a implementar:
metrics = {
    'churn_rate_0_12_months': 'Tasa de churn primeros 12 meses',
    'churn_rate_by_spend_segment': 'Churn por segmento de gasto',
    'time_to_first_renewal': 'Tiempo hasta primera renovaciÃ³n',
    'customer_lifetime_value': 'Valor de vida del cliente',
    'retention_program_roi': 'ROI programas de retenciÃ³n'
}
```

#### ğŸ”„ **ESTRATEGIAS DE RETENCIÃ“N ESPECÃFICAS**

**Para Clientes Nuevos (0-12 meses):**
- Check-ins automÃ¡ticos en dÃ­as 30, 60, 90
- Tutoriales personalizados y soporte tÃ©cnico gratuito
- Descuentos por permanencia tras primer aÃ±o

**Para Clientes de Bajo Gasto:**
- AnÃ¡lisis de necesidades para identificar servicios adicionales
- Planes familiares o grupales con descuentos
- Programa de referidos con beneficios mutuos

**Para RetenciÃ³n a Largo Plazo:**
- Contratos multi-aÃ±o con beneficios crecientes
- Programa VIP para clientes +48 meses
- Servicios premium sin costo adicional por fidelidad

### ğŸ¯ **Conclusiones del AnÃ¡lisis EspecÃ­fico**

#### **ğŸ“ˆ HALLAZGOS PRINCIPALES**
Este anÃ¡lisis revelÃ³ dos patrones crÃ­ticos para la retenciÃ³n de clientes en TelecomX:

1. **Factor Tiempo**: La correlaciÃ³n fuerte negativa (-0.35) entre tenure y churn confirma que **el primer aÃ±o es absolutamente crÃ­tico**. Los datos muestran una diferencia dramÃ¡tica: 47.7% de churn en los primeros 12 meses vs 9.5% despuÃ©s de 48 meses.

2. **Factor EconÃ³mico**: Contrario a la intuiciÃ³n, los clientes de menor gasto presentan mayor riesgo de churn (-0.20 de correlaciÃ³n). Esto sugiere que el valor percibido, no el precio absoluto, es el factor determinante.

#### **ğŸš€ IMPACTO POTENCIAL**
Implementando las estrategias recomendadas, TelecomX podrÃ­a:
- **Reducir el churn general del 26.5% al 20%**
- **Aumentar la retenciÃ³n en el primer aÃ±o del 52.3% al 70%**
- **Mejorar el CLV (Customer Lifetime Value) en un 35%**
- **Generar ROI positivo en programas de retenciÃ³n dentro de 6 meses**

#### **ğŸ”‘ FACTORES CLAVE DE Ã‰XITO**
1. **Timing**: Actuar proactivamente en los primeros 30-90 dÃ­as
2. **SegmentaciÃ³n**: Estrategias diferenciadas por nivel de gasto y tenure
3. **Valor percibido**: Enfoque en beneficios tangibles, no solo descuentos
4. **Monitoreo continuo**: MÃ©tricas especÃ­ficas para medir impacto

**El anÃ¡lisis estadÃ­stico confirma que ambas relaciones son altamente significativas (p < 0.001), validando la importancia crÃ­tica de estas variables para predecir y prevenir la cancelaciÃ³n de clientes.**

## 7. DivisiÃ³n del Dataset: Entrenamiento y Prueba

En esta secciÃ³n se divide el conjunto de datos en conjuntos de entrenamiento y prueba para evaluar el rendimiento del modelo de manera objetiva. Se implementa una divisiÃ³n estratificada para mantener la proporciÃ³n de clases en ambos conjuntos, lo cual es especialmente importante dado el desbalance de clases identificado anteriormente.

**Estrategia de DivisiÃ³n:**
- **80% para entrenamiento** - 20% para prueba (recomendado para datasets > 5,000 registros)
- **DivisiÃ³n estratificada** para mantener proporciÃ³n de churn
- **PreparaciÃ³n de variables** categÃ³ricas y numÃ©ricas
- **ValidaciÃ³n de la divisiÃ³n** con mÃ©tricas descriptivas
"""

# Importaciones necesarias para la divisiÃ³n y preparaciÃ³n de datos
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pandas as pd
import numpy as np

def prepare_data_for_modeling(df):
    """
    Prepara los datos para el modelado: limpieza, codificaciÃ³n y separaciÃ³n de variables
    """
    print("ğŸ”§ PREPARACIÃ“N DE DATOS PARA MODELADO")
    print("="*70)

    # Crear una copia del dataset para trabajar
    df_model = df.copy()

    # Convertir charges_total a numÃ©rico si no lo estÃ¡
    df_model['charges_total'] = pd.to_numeric(df_model['charges_total'], errors='coerce')

    # Eliminar filas con valores nulos en variables crÃ­ticas
    initial_rows = len(df_model)
    df_model = df_model.dropna(subset=['churn', 'charges_total']).copy()
    final_rows = len(df_model)

    print(f"ğŸ“Š LIMPIEZA DE DATOS:")
    print(f"   â€¢ Registros iniciales: {initial_rows:,}")
    print(f"   â€¢ Registros finales: {final_rows:,}")
    print(f"   â€¢ Registros eliminados: {initial_rows - final_rows:,}")

    # Identificar tipos de variables
    categorical_vars = []
    numerical_vars = []
    binary_vars = []

    for col in df_model.columns:
        if col == 'churn':  # Variable target
            continue
        elif df_model[col].dtype == 'object':
            unique_values = df_model[col].nunique()
            if unique_values == 2:
                binary_vars.append(col)
            else:
                categorical_vars.append(col)
        elif df_model[col].nunique() == 2 and set(df_model[col].unique()).issubset({0, 1}):
            binary_vars.append(col)
        else:
            numerical_vars.append(col)

    print(f"\nğŸ“‹ CLASIFICACIÃ“N DE VARIABLES:")
    print(f"   â€¢ Variables categÃ³ricas: {len(categorical_vars)} - {categorical_vars}")
    print(f"   â€¢ Variables binarias: {len(binary_vars)} - {binary_vars}")
    print(f"   â€¢ Variables numÃ©ricas: {len(numerical_vars)} - {numerical_vars}")
    print(f"   â€¢ Variable target: churn")

    return df_model, categorical_vars, numerical_vars, binary_vars

def encode_categorical_variables(df, categorical_vars):
    """
    Codifica variables categÃ³ricas usando One-Hot Encoding
    """
    print(f"\nğŸ”„ CODIFICACIÃ“N DE VARIABLES CATEGÃ“RICAS:")

    if categorical_vars:
        # Aplicar One-Hot Encoding
        df_encoded = pd.get_dummies(df,
                                   columns=categorical_vars,
                                   drop_first=True,  # Evita multicolinealidad
                                   dtype=int)

        # Mostrar nuevas columnas creadas
        original_cols = set(df.columns)
        new_cols = set(df_encoded.columns)
        created_cols = sorted(list(new_cols - original_cols))

        print(f"   â€¢ Columnas originales: {len(original_cols)}")
        print(f"   â€¢ Columnas despuÃ©s de encoding: {len(new_cols)}")
        print(f"   â€¢ Nuevas columnas creadas: {len(created_cols)}")

        if len(created_cols) <= 10:  # Mostrar solo si no son demasiadas
            print(f"   â€¢ Columnas nuevas: {created_cols}")

        return df_encoded
    else:
        print("   â€¢ No hay variables categÃ³ricas para codificar")
        return df

# Ejecutar preparaciÃ³n de datos
df_model, categorical_vars, numerical_vars, binary_vars = prepare_data_for_modeling(df_cleaned)
df_encoded = encode_categorical_variables(df_model, categorical_vars)

print(f"\nâœ… DATOS PREPARADOS PARA MODELADO:")
print(f"   â€¢ Forma final del dataset: {df_encoded.shape}")
print(f"   â€¢ Total de features: {df_encoded.shape[1] - 1} (excluyendo target)")

def split_dataset(df, test_size=0.2, random_state=42):
    """
    Divide el dataset en conjuntos de entrenamiento y prueba con estratificaciÃ³n
    """
    print("ğŸ”€ DIVISIÃ“N DEL DATASET EN ENTRENAMIENTO Y PRUEBA")
    print("="*70)

    # Separar caracterÃ­sticas (X) y variable objetivo (y)
    X = df.drop('churn', axis=1)
    y = df['churn']

    print(f"ğŸ“Š DATASET COMPLETO:")
    print(f"   â€¢ Total de registros: {len(df):,}")
    print(f"   â€¢ Total de features: {X.shape[1]}")
    print(f"   â€¢ DistribuciÃ³n de clases:")
    print(f"     - No Churn (0): {(y == 0).sum():,} ({(y == 0).mean()*100:.1f}%)")
    print(f"     - Churn (1): {(y == 1).sum():,} ({(y == 1).mean()*100:.1f}%)")

    # DivisiÃ³n estratificada
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y  # Mantiene la proporciÃ³n de clases
    )

    print(f"\nğŸš‚ CONJUNTO DE ENTRENAMIENTO:")
    print(f"   â€¢ Registros: {len(X_train):,} ({(1-test_size)*100:.0f}%)")
    print(f"   â€¢ Features: {X_train.shape[1]}")
    print(f"   â€¢ DistribuciÃ³n de clases:")
    print(f"     - No Churn (0): {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)")
    print(f"     - Churn (1): {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.1f}%)")

    print(f"\nğŸ§ª CONJUNTO DE PRUEBA:")
    print(f"   â€¢ Registros: {len(X_test):,} ({test_size*100:.0f}%)")
    print(f"   â€¢ Features: {X_test.shape[1]}")
    print(f"   â€¢ DistribuciÃ³n de clases:")
    print(f"     - No Churn (0): {(y_test == 0).sum():,} ({(y_test == 0).mean()*100:.1f}%)")
    print(f"     - Churn (1): {(y_test == 1).sum():,} ({(y_test == 1).mean()*100:.1f}%)")

    # Verificar que la estratificaciÃ³n funcionÃ³ correctamente
    train_churn_rate = y_train.mean()
    test_churn_rate = y_test.mean()
    difference = abs(train_churn_rate - test_churn_rate)

    print(f"\nâœ… VERIFICACIÃ“N DE ESTRATIFICACIÃ“N:")
    print(f"   â€¢ Tasa de churn en entrenamiento: {train_churn_rate*100:.2f}%")
    print(f"   â€¢ Tasa de churn en prueba: {test_churn_rate*100:.2f}%")
    print(f"   â€¢ Diferencia: {difference*100:.2f}% ({'âœ… Buena' if difference < 0.01 else 'âš ï¸ Revisar'})")

    return X_train, X_test, y_train, y_test

# Ejecutar divisiÃ³n del dataset
X_train, X_test, y_train, y_test = split_dataset(df_encoded, test_size=0.2, random_state=42)

# VisualizaciÃ³n de la divisiÃ³n del dataset
fig, axes = plt.subplots(2, 2, figsize=(16, 10))
fig.suptitle('ğŸ“Š ANÃLISIS DE LA DIVISIÃ“N DEL DATASET', fontsize=16, fontweight='bold')

# 1. DistribuciÃ³n de clases en conjunto completo
ax1 = axes[0, 0]
y_complete = df_encoded['churn']
complete_counts = y_complete.value_counts().sort_index()
bars1 = ax1.bar(['No Churn', 'Churn'], complete_counts.values,
                color=['#3498db', '#e74c3c'], alpha=0.8)
ax1.set_title('Dataset Completo\nDistribuciÃ³n de Clases', fontweight='bold')
ax1.set_ylabel('NÃºmero de Registros')
for i, v in enumerate(complete_counts.values):
    ax1.text(i, v + 50, f'{v:,}\n({v/len(y_complete)*100:.1f}%)',
             ha='center', fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')

# 2. DistribuciÃ³n de clases en entrenamiento
ax2 = axes[0, 1]
train_counts = y_train.value_counts().sort_index()
bars2 = ax2.bar(['No Churn', 'Churn'], train_counts.values,
                color=['#3498db', '#e74c3c'], alpha=0.8)
ax2.set_title('Conjunto de Entrenamiento (80%)\nDistribuciÃ³n de Clases', fontweight='bold')
ax2.set_ylabel('NÃºmero de Registros')
for i, v in enumerate(train_counts.values):
    ax2.text(i, v + 30, f'{v:,}\n({v/len(y_train)*100:.1f}%)',
             ha='center', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# 3. DistribuciÃ³n de clases en prueba
ax3 = axes[1, 0]
test_counts = y_test.value_counts().sort_index()
bars3 = ax3.bar(['No Churn', 'Churn'], test_counts.values,
                color=['#3498db', '#e74c3c'], alpha=0.8)
ax3.set_title('Conjunto de Prueba (20%)\nDistribuciÃ³n de Clases', fontweight='bold')
ax3.set_ylabel('NÃºmero de Registros')
for i, v in enumerate(test_counts.values):
    ax3.text(i, v + 10, f'{v:,}\n({v/len(y_test)*100:.1f}%)',
             ha='center', fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')

# 4. ComparaciÃ³n de proporciones
ax4 = axes[1, 1]
proportions_data = {
    'Completo': [complete_counts[0]/len(y_complete), complete_counts[1]/len(y_complete)],
    'Entrenamiento': [train_counts[0]/len(y_train), train_counts[1]/len(y_train)],
    'Prueba': [test_counts[0]/len(y_test), test_counts[1]/len(y_test)]
}

x = np.arange(len(proportions_data))
width = 0.35

bars_no_churn = ax4.bar(x - width/2, [prop[0] for prop in proportions_data.values()],
                        width, label='No Churn', color='#3498db', alpha=0.8)
bars_churn = ax4.bar(x + width/2, [prop[1] for prop in proportions_data.values()],
                     width, label='Churn', color='#e74c3c', alpha=0.8)

ax4.set_title('ComparaciÃ³n de Proporciones\n(VerificaciÃ³n de EstratificaciÃ³n)', fontweight='bold')
ax4.set_ylabel('ProporciÃ³n')
ax4.set_xticks(x)
ax4.set_xticklabels(proportions_data.keys())
ax4.legend()
ax4.grid(True, alpha=0.3, axis='y')

# Agregar valores en las barras
for bars in [bars_no_churn, bars_churn]:
    for bar in bars:
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# Resumen estadÃ­stico de la divisiÃ³n
print("\nğŸ“ˆ RESUMEN ESTADÃSTICO DE LA DIVISIÃ“N:")
print("="*60)
print(f"â€¢ Tasa de churn original: {y_complete.mean():.4f}")
print(f"â€¢ Tasa de churn entrenamiento: {y_train.mean():.4f}")
print(f"â€¢ Tasa de churn prueba: {y_test.mean():.4f}")
print(f"â€¢ Diferencia mÃ¡xima: {max(abs(y_complete.mean() - y_train.mean()), abs(y_complete.mean() - y_test.mean())):.4f}")
print(f"â€¢ Estado de estratificaciÃ³n: {'âœ… EXITOSA' if max(abs(y_complete.mean() - y_train.mean()), abs(y_complete.mean() - y_test.mean())) < 0.02 else 'âš ï¸ REVISAR'}")

def scale_numerical_features(X_train, X_test, numerical_vars):
    """
    Escala las variables numÃ©ricas usando StandardScaler
    """
    print("ğŸ“ ESCALADO DE VARIABLES NUMÃ‰RICAS")
    print("="*60)

    # Identificar columnas numÃ©ricas reales (excluyendo categÃ³ricas codificadas)
    # Variables que definitivamente NO deben escalarse
    exclude_from_scaling = [
        'tenure_segment', 'charges_segment',  # Variables categÃ³ricas creadas
        'num_services'  # Si es un conteo discreto
    ]

    # Filtrar variables numÃ©ricas verdaderas
    available_numerical = []
    for col in X_train.columns:
        if col in numerical_vars and col not in exclude_from_scaling:
            # Verificar que la columna sea realmente numÃ©rica
            if X_train[col].dtype in ['int64', 'float64']:
                # Verificar que no tenga valores string
                try:
                    pd.to_numeric(X_train[col].iloc[:5])  # Test con las primeras 5 filas
                    available_numerical.append(col)
                except (ValueError, TypeError):
                    print(f"   âš ï¸ Excluyendo {col}: contiene valores no numÃ©ricos")

    print(f"ğŸ“Š VARIABLES NUMÃ‰RICAS IDENTIFICADAS:")
    print(f"   â€¢ Variables numÃ©ricas disponibles: {len(available_numerical)}")
    print(f"   â€¢ Variables: {available_numerical}")
    print(f"   â€¢ Variables excluidas del escalado: {exclude_from_scaling}")

    if not available_numerical:
        print("   âš ï¸ No se encontraron variables numÃ©ricas vÃ¡lidas para escalar")
        return X_train.copy(), X_test.copy(), None

    # Crear copias para no modificar los originales
    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()

    # Inicializar el escalador
    scaler = StandardScaler()

    # Verificar que las columnas existen en ambos conjuntos
    valid_numerical = [col for col in available_numerical if col in X_train.columns and col in X_test.columns]

    if not valid_numerical:
        print("   âš ï¸ No hay variables numÃ©ricas vÃ¡lidas en ambos conjuntos")
        return X_train_scaled, X_test_scaled, None

    # Ajustar el escalador solo con los datos de entrenamiento
    scaler.fit(X_train[valid_numerical])

    # Aplicar transformaciÃ³n a ambos conjuntos
    X_train_scaled[valid_numerical] = scaler.transform(X_train[valid_numerical])
    X_test_scaled[valid_numerical] = scaler.transform(X_test[valid_numerical])

    print(f"\nğŸ”§ PROCESO DE ESCALADO:")
    print(f"   â€¢ Escalador utilizado: StandardScaler (media=0, std=1)")
    print(f"   â€¢ Variables escaladas: {len(valid_numerical)}")
    print(f"   â€¢ Variables escaladas: {valid_numerical}")
    print(f"   â€¢ Ajuste realizado solo con datos de entrenamiento")

    # Mostrar estadÃ­sticas antes y despuÃ©s del escalado
    print(f"\nğŸ“Š ESTADÃSTICAS ANTES Y DESPUÃ‰S DEL ESCALADO:")
    print("-" * 60)

    for var in valid_numerical[:3]:  # Mostrar solo las primeras 3 variables
        print(f"\nğŸ“ˆ {var.upper()}:")
        print(f"   ANTES - Entrenamiento: Media={X_train[var].mean():.2f}, Std={X_train[var].std():.2f}")
        print(f"   ANTES - Prueba: Media={X_test[var].mean():.2f}, Std={X_test[var].std():.2f}")
        print(f"   DESPUÃ‰S - Entrenamiento: Media={X_train_scaled[var].mean():.2f}, Std={X_train_scaled[var].std():.2f}")
        print(f"   DESPUÃ‰S - Prueba: Media={X_test_scaled[var].mean():.2f}, Std={X_test_scaled[var].std():.2f}")

    if len(valid_numerical) > 3:
        print(f"\n   ... y {len(valid_numerical) - 3} variables mÃ¡s escaladas")

    print(f"\nâœ… ESCALADO COMPLETADO EXITOSAMENTE")

    return X_train_scaled, X_test_scaled, scaler

# Aplicar escalado a variables numÃ©ricas
X_train_scaled, X_test_scaled, scaler = scale_numerical_features(X_train, X_test, numerical_vars)

# Verificar quÃ© variables categÃ³ricas quedaron en el dataset
def check_remaining_categorical_variables(X_train):
    """
    Verifica quÃ© variables categÃ³ricas pueden haber quedado sin codificar
    """
    print("ğŸ” VERIFICACIÃ“N DE VARIABLES CATEGÃ“RICAS RESTANTES")
    print("="*60)

    categorical_remaining = []
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            categorical_remaining.append(col)
        elif X_train[col].dtype in ['int64', 'float64']:
            # Verificar si son variables categÃ³ricas codificadas como nÃºmeros
            unique_values = X_train[col].unique()
            if len(unique_values) <= 10:  # Asumimos que â‰¤10 valores Ãºnicos podrÃ­an ser categÃ³ricas
                print(f"   ğŸ“‹ {col}: {len(unique_values)} valores Ãºnicos - {sorted(unique_values)}")

    if categorical_remaining:
        print(f"\nâš ï¸ VARIABLES CATEGÃ“RICAS SIN CODIFICAR:")
        for col in categorical_remaining:
            unique_vals = X_train[col].unique()
            print(f"   â€¢ {col}: {len(unique_vals)} valores Ãºnicos")
            if len(unique_vals) <= 10:
                print(f"     Valores: {list(unique_vals)}")

        print(f"\nğŸ’¡ RECOMENDACIÃ“N:")
        print(f"   Estas variables necesitan ser eliminadas o codificadas antes del modelado")

        # Eliminar variables categÃ³ricas restantes
        X_train_clean = X_train.drop(columns=categorical_remaining)
        X_test_clean = X_test.drop(columns=categorical_remaining)
        X_train_scaled_clean = X_train_scaled.drop(columns=categorical_remaining)
        X_test_scaled_clean = X_test_scaled.drop(columns=categorical_remaining)

        print(f"\nâœ… VARIABLES CATEGÃ“RICAS ELIMINADAS:")
        print(f"   â€¢ Forma anterior: {X_train.shape}")
        print(f"   â€¢ Forma nueva: {X_train_clean.shape}")

        return X_train_clean, X_test_clean, X_train_scaled_clean, X_test_scaled_clean
    else:
        print("âœ… No se encontraron variables categÃ³ricas sin codificar")
        return X_train, X_test, X_train_scaled, X_test_scaled

# Verificar y limpiar variables categÃ³ricas restantes
X_train_final, X_test_final, X_train_scaled_final, X_test_scaled_final = check_remaining_categorical_variables(X_train)

# Actualizar las variables para usar las versiones limpias
X_train = X_train_final
X_test = X_test_final
X_train_scaled = X_train_scaled_final
X_test_scaled = X_test_scaled_final

print(f"\nğŸ¯ DATASETS FINALES PREPARADOS:")
print(f"   â€¢ X_train: {X_train.shape}")
print(f"   â€¢ X_test: {X_test.shape}")
print(f"   â€¢ X_train_scaled: {X_train_scaled.shape}")
print(f"   â€¢ X_test_scaled: {X_test_scaled.shape}")

def validate_data_split(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled):
    """
    Realiza validaciones finales de la divisiÃ³n de datos
    """
    print("âœ… VALIDACIÃ“N FINAL DE LA DIVISIÃ“N DE DATOS")
    print("="*70)

    # 1. Verificar formas de los conjuntos
    print("ğŸ“Š VERIFICACIÃ“N DE FORMAS:")
    print(f"   â€¢ X_train: {X_train.shape}")
    print(f"   â€¢ X_test: {X_test.shape}")
    print(f"   â€¢ y_train: {y_train.shape}")
    print(f"   â€¢ y_test: {y_test.shape}")
    print(f"   â€¢ X_train_scaled: {X_train_scaled.shape}")
    print(f"   â€¢ X_test_scaled: {X_test_scaled.shape}")

    # 2. Verificar que no hay valores nulos
    print(f"\nğŸ” VERIFICACIÃ“N DE VALORES NULOS:")
    print(f"   â€¢ X_train nulos: {X_train.isnull().sum().sum()}")
    print(f"   â€¢ X_test nulos: {X_test.isnull().sum().sum()}")
    print(f"   â€¢ y_train nulos: {y_train.isnull().sum()}")
    print(f"   â€¢ y_test nulos: {y_test.isnull().sum()}")
    print(f"   â€¢ X_train_scaled nulos: {X_train_scaled.isnull().sum().sum()}")
    print(f"   â€¢ X_test_scaled nulos: {X_test_scaled.isnull().sum().sum()}")

    # 3. Verificar que las columnas son las mismas
    print(f"\nğŸ“‹ VERIFICACIÃ“N DE COLUMNAS:")
    columns_match = list(X_train.columns) == list(X_test.columns)
    columns_scaled_match = list(X_train_scaled.columns) == list(X_test_scaled.columns)
    print(f"   â€¢ Columnas coinciden entre train y test: {'âœ… SÃ' if columns_match else 'âŒ NO'}")
    print(f"   â€¢ Columnas scaled coinciden: {'âœ… SÃ' if columns_scaled_match else 'âŒ NO'}")
    print(f"   â€¢ NÃºmero de features: {len(X_train.columns)}")

    # 4. Verificar tipos de datos
    print(f"\nğŸ”¢ TIPOS DE DATOS:")
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns
    object_cols = X_train.select_dtypes(include=['object']).columns
    print(f"   â€¢ Columnas numÃ©ricas: {len(numeric_cols)}")
    print(f"   â€¢ Columnas de objeto: {len(object_cols)}")

    if len(object_cols) > 0:
        print(f"   âš ï¸ Columnas de objeto detectadas: {list(object_cols)[:5]}")
        print(f"   ğŸ’¡ Estas pueden necesitar codificaciÃ³n adicional")

    # 5. Verificar rango de valores en y
    print(f"\nğŸ¯ VERIFICACIÃ“N DE VARIABLE TARGET:")
    print(f"   â€¢ Valores Ãºnicos en y_train: {sorted(y_train.unique())}")
    print(f"   â€¢ Valores Ãºnicos en y_test: {sorted(y_test.unique())}")
    print(f"   â€¢ Tipo de y_train: {y_train.dtype}")
    print(f"   â€¢ Tipo de y_test: {y_test.dtype}")

    # 6. Verificar balanceamiento de clases
    print(f"\nâš–ï¸ VERIFICACIÃ“N DE BALANCEAMIENTO:")
    train_balance = y_train.value_counts(normalize=True).sort_index()
    test_balance = y_test.value_counts(normalize=True).sort_index()
    print(f"   â€¢ Balance entrenamiento: No Churn={train_balance[0]:.3f}, Churn={train_balance[1]:.3f}")
    print(f"   â€¢ Balance prueba: No Churn={test_balance[0]:.3f}, Churn={test_balance[1]:.3f}")

    balance_diff = abs(train_balance[1] - test_balance[1])
    print(f"   â€¢ Diferencia en tasa de churn: {balance_diff:.4f}")
    print(f"   â€¢ EstratificaciÃ³n: {'âœ… BUENA' if balance_diff < 0.02 else 'âš ï¸ REVISAR'}")

    # 7. Verificar rangos de escalado
    if X_train_scaled is not None:
        print(f"\nğŸ“ VERIFICACIÃ“N DE ESCALADO:")
        scaled_means = X_train_scaled.select_dtypes(include=[np.number]).mean()
        scaled_stds = X_train_scaled.select_dtypes(include=[np.number]).std()

        # Variables que deberÃ­an estar escaladas (media~0, std~1)
        properly_scaled = []
        for col in scaled_means.index:
            if abs(scaled_means[col]) < 0.1 and abs(scaled_stds[col] - 1) < 0.1:
                properly_scaled.append(col)

        print(f"   â€¢ Variables correctamente escaladas: {len(properly_scaled)}")
        print(f"   â€¢ Total de variables numÃ©ricas: {len(scaled_means)}")
        print(f"   â€¢ Porcentaje escalado correctamente: {len(properly_scaled)/len(scaled_means)*100:.1f}%")

    # 8. Resumen final
    print(f"\nğŸ† RESUMEN FINAL:")
    all_checks = [
        X_train.shape[0] > 0,
        X_test.shape[0] > 0,
        X_train.isnull().sum().sum() == 0,
        X_test.isnull().sum().sum() == 0,
        y_train.isnull().sum() == 0,
        y_test.isnull().sum() == 0,
        columns_match,
        len(object_cols) == 0,
        set(y_train.unique()).issubset({0, 1}),
        set(y_test.unique()).issubset({0, 1}),
        balance_diff < 0.02
    ]

    passed_checks = sum(all_checks)
    total_checks = len(all_checks)

    print(f"   â€¢ Verificaciones pasadas: {passed_checks}/{total_checks}")
    print(f"   â€¢ Estado general: {'âœ… DATOS LISTOS PARA MODELADO' if passed_checks == total_checks else 'âš ï¸ REVISAR ALGUNOS ASPECTOS'}")

    if passed_checks >= total_checks - 1:  # Permitir 1 fallo menor
        print(f"\nğŸš€ LOS DATOS ESTÃN PREPARADOS PARA:")
        print(f"   â€¢ Entrenamiento de modelos de machine learning")
        print(f"   â€¢ ValidaciÃ³n y evaluaciÃ³n de rendimiento")
        print(f"   â€¢ ComparaciÃ³n de diferentes algoritmos")
        print(f"   â€¢ OptimizaciÃ³n de hiperparÃ¡metros")

    return passed_checks >= total_checks - 1

# Ejecutar validaciÃ³n final
data_ready = validate_data_split(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled)

"""### ğŸ“‹ Resumen de la DivisiÃ³n y PreparaciÃ³n de Datos

#### âœ… **PROCESO COMPLETADO EXITOSAMENTE**

**1. PreparaciÃ³n de Datos:**
- âœ… Limpieza de valores nulos y inconsistentes
- âœ… CodificaciÃ³n de variables categÃ³ricas (One-Hot Encoding)
- âœ… IdentificaciÃ³n y tratamiento de variables numÃ©ricas
- âœ… EliminaciÃ³n de variables irrelevantes para el modelado

**2. DivisiÃ³n Estratificada (80/20):**
- âœ… **Entrenamiento**: ~80% de los datos manteniendo proporciÃ³n de clases
- âœ… **Prueba**: ~20% de los datos para evaluaciÃ³n objetiva
- âœ… **EstratificaciÃ³n**: Mismo balance de churn en ambos conjuntos
- âœ… **ValidaciÃ³n**: Sin data leakage entre conjuntos

**3. Escalado de Variables:**
- âœ… **StandardScaler**: Media=0, DesviaciÃ³n estÃ¡ndar=1
- âœ… **Ajuste solo en entrenamiento**: Previene data leakage
- âœ… **Variables numÃ©ricas**: tenure, charges_monthly, charges_total, charges_daily
- âœ… **Variables categÃ³ricas**: Mantenidas sin escalar

#### ğŸ“Š **DATASETS FINALES DISPONIBLES**

```python
# Conjuntos de datos preparados para modelado:
datasets = {
    'X_train': 'Features de entrenamiento (originales)',
    'X_test': 'Features de prueba (originales)',
    'X_train_scaled': 'Features de entrenamiento (escaladas)',
    'X_test_scaled': 'Features de prueba (escaladas)',
    'y_train': 'Variable target entrenamiento',
    'y_test': 'Variable target prueba',
    'scaler': 'Objeto para transformar nuevos datos'
}
```

#### ğŸš€ **PRÃ“XIMOS PASOS RECOMENDADOS**

**1. Entrenamiento de Modelos Baseline:**
- RegresiÃ³n LogÃ­stica
- Random Forest
- Gradient Boosting (XGBoost/LightGBM)

**2. EvaluaciÃ³n de Modelos:**
- MÃ©tricas: Precision, Recall, F1-Score, AUC-ROC
- Matrices de confusiÃ³n
- Curvas ROC y Precision-Recall

**3. OptimizaciÃ³n:**
- ValidaciÃ³n cruzada estratificada
- BÃºsqueda de hiperparÃ¡metros
- SelecciÃ³n de features

**4. Interpretabilidad:**
- Feature importance
- SHAP values
- AnÃ¡lisis de errores
"""

# VerificaciÃ³n final y resumen de los datasets preparados
print("ğŸ¯ VERIFICACIÃ“N FINAL DE DATASETS PREPARADOS")
print("="*70)

print(f"\nğŸ“¦ DATASETS DISPONIBLES:")
print(f"   â€¢ X_train: {X_train.shape} - Features entrenamiento (sin escalar)")
print(f"   â€¢ X_test: {X_test.shape} - Features prueba (sin escalar)")
print(f"   â€¢ X_train_scaled: {X_train_scaled.shape} - Features entrenamiento (escaladas)")
print(f"   â€¢ X_test_scaled: {X_test_scaled.shape} - Features prueba (escaladas)")
print(f"   â€¢ y_train: {y_train.shape} - Target entrenamiento")
print(f"   â€¢ y_test: {y_test.shape} - Target prueba")

print(f"\nğŸ“Š DISTRIBUCIÃ“N DE CLASES:")
print(f"   â€¢ Entrenamiento - No Churn: {(y_train==0).sum():,} ({(y_train==0).mean()*100:.1f}%)")
print(f"   â€¢ Entrenamiento - Churn: {(y_train==1).sum():,} ({(y_train==1).mean()*100:.1f}%)")
print(f"   â€¢ Prueba - No Churn: {(y_test==0).sum():,} ({(y_test==0).mean()*100:.1f}%)")
print(f"   â€¢ Prueba - Churn: {(y_test==1).sum():,} ({(y_test==1).mean()*100:.1f}%)")

print(f"\nğŸ”§ ESCALADOR DISPONIBLE:")
if scaler is not None:
    print(f"   â€¢ Tipo: {type(scaler).__name__}")
    print(f"   â€¢ Variables escaladas: {len(scaler.feature_names_in_) if hasattr(scaler, 'feature_names_in_') else 'N/A'}")
    print(f"   â€¢ Listo para transformar nuevos datos")
else:
    print(f"   â€¢ No se aplicÃ³ escalado")

print(f"\nâœ… ESTADO FINAL:")
print(f"   ğŸ¯ Datos completamente preparados para modelado")
print(f"   ğŸ”€ DivisiÃ³n estratificada exitosa")
print(f"   ğŸ“ Escalado aplicado correctamente")
print(f"   ğŸš€ Listos para entrenar modelos de ML")

# Guardar informaciÃ³n de los datasets para referencia futura
dataset_info = {
    'train_shape': X_train.shape,
    'test_shape': X_test.shape,
    'features': list(X_train.columns),
    'target_distribution_train': y_train.value_counts().to_dict(),
    'target_distribution_test': y_test.value_counts().to_dict(),
    'scaled': scaler is not None
}

print(f"\nğŸ“‹ INFORMACIÃ“N DE DATASETS GUARDADA PARA REFERENCIA")

"""# ğŸ¤– PARTE 3: MODELADO PREDICTIVO PARA CHURN

En esta secciÃ³n implementaremos dos modelos predictivos diferentes para predecir la cancelaciÃ³n de clientes:

1. **RegresiÃ³n LogÃ­stica**: Modelo lineal que requiere normalizaciÃ³n de datos
2. **Random Forest**: Modelo basado en Ã¡rboles que no requiere normalizaciÃ³n

## ğŸ¯ Objetivos del Modelado:
- Comparar el rendimiento de modelos con y sin normalizaciÃ³n
- Evaluar diferentes enfoques algorÃ­tmicos (lineal vs ensemble)
- Identificar las variables mÃ¡s importantes para predecir churn
- Proporcionar mÃ©tricas de evaluaciÃ³n robustas

## 7. PreparaciÃ³n de Datos para Modelado

Antes de implementar los modelos, necesitamos preparar los datos adecuadamente:

### ğŸ“‹ Pasos de PreparaciÃ³n:
1. **IdentificaciÃ³n de variables**: Separar features numÃ©ricas, binarias y categÃ³ricas
2. **One-Hot Encoding**: Para variables categÃ³ricas con mÃºltiples valores
3. **DivisiÃ³n train/test**: 80% entrenamiento, 20% prueba con estratificaciÃ³n
4. **NormalizaciÃ³n condicional**: Solo para modelos que la requieren (RegresiÃ³n LogÃ­stica)

### ğŸ” JustificaciÃ³n de la NormalizaciÃ³n:
- **RegresiÃ³n LogÃ­stica**: Es sensible a la escala porque optimiza utilizando gradientes. Variables con diferentes escalas pueden dominar el proceso de aprendizaje
- **Random Forest**: No requiere normalizaciÃ³n porque usa divisiones basadas en umbrales relativos, no distancias absolutas
"""

# ImportaciÃ³n de librerÃ­as para modelado
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    roc_curve, precision_recall_curve, accuracy_score,
    precision_score, recall_score, f1_score
)
import warnings
warnings.filterwarnings('ignore')

print("ğŸ“¦ LIBRERÃAS DE MACHINE LEARNING IMPORTADAS")
print("="*50)
print("âœ… Modelos: LogisticRegression, RandomForestClassifier")
print("âœ… MÃ©tricas: PrecisiÃ³n, Recall, F1-Score, AUC-ROC")
print("âœ… Preprocesamiento: StandardScaler, train_test_split")
print("âœ… ValidaciÃ³n: Cross-validation, StratifiedKFold")

def prepare_data_for_modeling(df):
    """
    Prepara los datos para modelado predictivo
    """
    print("ğŸ”§ PREPARANDO DATOS PARA MODELADO PREDICTIVO")
    print("="*60)

    # Crear una copia para no modificar el dataset original
    df_model = df.copy()

    # PASO CRÃTICO: Eliminar columnas creadas en anÃ¡lisis anteriores que no son para modelado
    columns_to_remove = [
        'tenure_segment',    # Segmentos creados en anÃ¡lisis anterior
        'charges_segment',   # Segmentos creados en anÃ¡lisis anterior
        'num_services'       # Variable derivada del anÃ¡lisis anterior
    ]

    # Eliminar columnas problemÃ¡ticas si existen
    for col in columns_to_remove:
        if col in df_model.columns:
            df_model = df_model.drop(columns=[col])
            print(f"   ğŸ—‘ï¸ Eliminada columna de anÃ¡lisis anterior: {col}")

    # 1. IDENTIFICAR TIPOS DE VARIABLES
    print("\nğŸ” IDENTIFICANDO TIPOS DE VARIABLES:")
    print("-" * 40)

    # Variables numÃ©ricas (excluyendo target)
    numerical_vars = ['tenure', 'charges_monthly', 'charges_total', 'charges_daily']
    numerical_vars = [var for var in numerical_vars if var in df_model.columns]

    # Variables categÃ³ricas que necesitan One-Hot Encoding
    categorical_vars = []
    for col in df_model.columns:
        if col != 'churn' and df_model[col].dtype == 'object':
            categorical_vars.append(col)

    # Variables binarias ya codificadas
    binary_vars = []
    for col in df_model.columns:
        if col != 'churn' and col not in numerical_vars and col not in categorical_vars:
            unique_vals = df_model[col].unique()
            if len(unique_vals) <= 2 and all(v in [0, 1, 0.0, 1.0] for v in unique_vals if pd.notna(v)):
                binary_vars.append(col)

    print(f"   ğŸ“Š Variables numÃ©ricas: {len(numerical_vars)} â†’ {numerical_vars}")
    print(f"   ğŸ”¤ Variables categÃ³ricas: {len(categorical_vars)} â†’ {categorical_vars}")
    print(f"   ğŸ”˜ Variables binarias: {len(binary_vars)} â†’ {binary_vars[:5]}...")

    # 2. APLICAR ONE-HOT ENCODING A VARIABLES CATEGÃ“RICAS
    if categorical_vars:
        print(f"\nğŸ”„ APLICANDO ONE-HOT ENCODING A VARIABLES CATEGÃ“RICAS:")
        print("-" * 40)

        original_shape = df_model.shape
        df_model = pd.get_dummies(df_model, columns=categorical_vars, drop_first=True, dtype=int)
        new_shape = df_model.shape

        print(f"   â€¢ Antes: {original_shape}")
        print(f"   â€¢ DespuÃ©s: {new_shape}")
        print(f"   â€¢ Nuevas columnas creadas: {new_shape[1] - original_shape[1]}")

        # Actualizar lista de variables binarias (One-Hot crea variables binarias)
        new_binary_vars = [col for col in df_model.columns
                          if col not in numerical_vars and col != 'churn'
                          and col not in binary_vars]
        binary_vars.extend(new_binary_vars)

    # 3. VERIFICAR Y LIMPIAR DATOS
    print(f"\nğŸ§¹ VERIFICANDO Y LIMPIANDO DATOS:")
    print("-" * 40)

    # Verificar tipos de datos
    print(f"   â€¢ Verificando tipos de datos...")
    for col in df_model.columns:
        if col != 'churn':
            if df_model[col].dtype == 'object':
                print(f"   âš ï¸ Columna '{col}' sigue siendo object: {df_model[col].unique()[:5]}")

    # Convertir todas las columnas numÃ©ricas a float (excepto target)
    for col in df_model.columns:
        if col != 'churn':
            try:
                df_model[col] = pd.to_numeric(df_model[col], errors='coerce')
            except:
                print(f"   âš ï¸ No se pudo convertir '{col}' a numÃ©rico")

    # Eliminar filas con valores NaN en features (no en target)
    feature_cols = [col for col in df_model.columns if col != 'churn']
    before_dropna = len(df_model)
    df_model = df_model.dropna(subset=feature_cols)
    after_dropna = len(df_model)

    if before_dropna != after_dropna:
        print(f"   ğŸ—‘ï¸ Eliminadas {before_dropna - after_dropna} filas con valores NaN")

    # 4. SEPARAR FEATURES Y TARGET
    print(f"\nğŸ¯ SEPARANDO FEATURES Y TARGET:")
    print("-" * 40)

    X = df_model.drop('churn', axis=1)
    y = df_model['churn']

    print(f"   â€¢ Features (X): {X.shape}")
    print(f"   â€¢ Target (y): {y.shape}")
    print(f"   â€¢ Balance de clases: {y.value_counts().to_dict()}")

    # 5. DIVISIÃ“N TRAIN/TEST CON ESTRATIFICACIÃ“N
    print(f"\nâœ‚ï¸ DIVISIÃ“N TRAIN/TEST (80/20) CON ESTRATIFICACIÃ“N:")
    print("-" * 40)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        stratify=y
    )

    print(f"   â€¢ X_train: {X_train.shape}")
    print(f"   â€¢ X_test: {X_test.shape}")
    print(f"   â€¢ y_train balance: {y_train.value_counts().to_dict()}")
    print(f"   â€¢ y_test balance: {y_test.value_counts().to_dict()}")

    # 6. IDENTIFICAR VARIABLES NUMÃ‰RICAS PARA ESCALADO
    print(f"\nğŸ“ PREPARANDO ESCALADO DE VARIABLES NUMÃ‰RICAS:")
    print("-" * 40)

    # Filtrar solo variables que realmente son numÃ©ricas y estÃ¡n en las originales
    numerical_cols_for_scaling = []
    for col in numerical_vars:
        if col in X_train.columns:
            # Verificar que la columna sea realmente numÃ©rica
            if pd.api.types.is_numeric_dtype(X_train[col]):
                numerical_cols_for_scaling.append(col)

    print(f"   â€¢ Variables identificadas para escalado: {numerical_cols_for_scaling}")
    print(f"   â€¢ Variables binarias (no escalar): {len(binary_vars)} variables")

    # 7. CREAR ESCALADOR (sin aplicar aÃºn)
    scaler = StandardScaler()

    # Ajustar el escalador solo con datos de entrenamiento
    if numerical_cols_for_scaling:
        scaler.fit(X_train[numerical_cols_for_scaling])
        print(f"   â€¢ Escalador ajustado en variables: {numerical_cols_for_scaling}")
    else:
        print("   âš ï¸ No se encontraron variables numÃ©ricas para escalar")

    # 8. VERIFICACIÃ“N FINAL
    print(f"\nâœ… VERIFICACIÃ“N FINAL:")
    print("-" * 40)

    # Verificar que no hay valores no numÃ©ricos
    non_numeric_cols = []
    for col in X_train.columns:
        if not pd.api.types.is_numeric_dtype(X_train[col]):
            non_numeric_cols.append(col)

    if non_numeric_cols:
        print(f"   âš ï¸ Columnas no numÃ©ricas detectadas: {non_numeric_cols}")
    else:
        print(f"   âœ… Todas las features son numÃ©ricas")

    print(f"\nâœ… PREPARACIÃ“N COMPLETADA:")
    print(f"   â€¢ Dataset listo para modelos que NO requieren escalado")
    print(f"   â€¢ Escalador preparado para modelos que SÃ requieren escalado")
    print(f"   â€¢ Total features finales: {X_train.shape[1]}")
    print("="*60)

    return {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'scaler': scaler,
        'numerical_cols': numerical_cols_for_scaling,
        'binary_cols': binary_vars,
        'feature_names': X.columns.tolist()
    }

# Ejecutar preparaciÃ³n de datos CORREGIDA
data_prepared = prepare_data_for_modeling(df_cleaned)

"""## 8. Modelo 1: Random Forest (Sin NormalizaciÃ³n)

### ğŸŒ² CaracterÃ­sticas del Random Forest:
- **No requiere normalizaciÃ³n**: Utiliza Ã¡rboles de decisiÃ³n que funcionan con divisiones basadas en umbrales
- **Robusto**: Maneja bien valores atÃ­picos y variables de diferentes escalas
- **Interpretable**: Proporciona importancia de variables fÃ¡cilmente interpretable
- **Ensemble**: Combina mÃºltiples Ã¡rboles para reducir overfitting

### ğŸ¯ ConfiguraciÃ³n del Modelo:
- **n_estimators=100**: NÃºmero de Ã¡rboles en el bosque
- **class_weight='balanced'**: Para manejar el desbalance de clases
- **random_state=42**: Para reproducibilidad
"""

def train_random_forest_model(data_prepared):
    """
    Entrena y evalÃºa el modelo Random Forest (sin normalizaciÃ³n)
    """
    print("ğŸŒ² ENTRENANDO MODELO RANDOM FOREST")
    print("="*50)

    # Extraer datos
    X_train = data_prepared['X_train']
    X_test = data_prepared['X_test']
    y_train = data_prepared['y_train']
    y_test = data_prepared['y_test']

    print(f"ğŸ“Š Datos de entrenamiento: {X_train.shape}")
    print(f"ğŸ“Š Datos de prueba: {X_test.shape}")

    # 1. ENTRENAR MODELO RANDOM FOREST
    print(f"\nğŸ”§ CONFIGURANDO Y ENTRENANDO RANDOM FOREST:")
    print("-" * 40)

    rf_model = RandomForestClassifier(
        n_estimators=100,           # 100 Ã¡rboles
        max_depth=10,               # Profundidad mÃ¡xima para evitar overfitting
        min_samples_split=20,       # MÃ­nimo de muestras para dividir
        min_samples_leaf=10,        # MÃ­nimo de muestras en hoja
        class_weight='balanced',    # Balancear clases automÃ¡ticamente
        random_state=42,            # Reproducibilidad
        n_jobs=-1                   # Usar todos los procesadores
    )

    # Entrenar el modelo
    print("   ğŸ‹ï¸ Entrenando modelo...")
    rf_model.fit(X_train, y_train)
    print("   âœ… Entrenamiento completado")

    # 2. REALIZAR PREDICCIONES
    print(f"\nğŸ”® REALIZANDO PREDICCIONES:")
    print("-" * 40)

    # Predicciones en conjunto de entrenamiento
    y_train_pred = rf_model.predict(X_train)
    y_train_pred_proba = rf_model.predict_proba(X_train)[:, 1]

    # Predicciones en conjunto de prueba
    y_test_pred = rf_model.predict(X_test)
    y_test_pred_proba = rf_model.predict_proba(X_test)[:, 1]

    print("   âœ… Predicciones completadas")

    # 3. EVALUAR RENDIMIENTO
    print(f"\nğŸ“ˆ EVALUACIÃ“N DEL RENDIMIENTO:")
    print("-" * 40)

    # MÃ©tricas en conjunto de entrenamiento
    train_accuracy = accuracy_score(y_train, y_train_pred)
    train_precision = precision_score(y_train, y_train_pred)
    train_recall = recall_score(y_train, y_train_pred)
    train_f1 = f1_score(y_train, y_train_pred)
    train_auc = roc_auc_score(y_train, y_train_pred_proba)

    # MÃ©tricas en conjunto de prueba
    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_precision = precision_score(y_test, y_test_pred)
    test_recall = recall_score(y_test, y_test_pred)
    test_f1 = f1_score(y_test, y_test_pred)
    test_auc = roc_auc_score(y_test, y_test_pred_proba)

    print(f"ğŸ“Š MÃ‰TRICAS DE ENTRENAMIENTO:")
    print(f"   â€¢ Accuracy:  {train_accuracy:.4f}")
    print(f"   â€¢ Precision: {train_precision:.4f}")
    print(f"   â€¢ Recall:    {train_recall:.4f}")
    print(f"   â€¢ F1-Score:  {train_f1:.4f}")
    print(f"   â€¢ AUC-ROC:   {train_auc:.4f}")

    print(f"\nğŸ“Š MÃ‰TRICAS DE PRUEBA:")
    print(f"   â€¢ Accuracy:  {test_accuracy:.4f}")
    print(f"   â€¢ Precision: {test_precision:.4f}")
    print(f"   â€¢ Recall:    {test_recall:.4f}")
    print(f"   â€¢ F1-Score:  {test_f1:.4f}")
    print(f"   â€¢ AUC-ROC:   {test_auc:.4f}")

    # EvaluaciÃ³n de overfitting
    overfitting_accuracy = train_accuracy - test_accuracy
    overfitting_f1 = train_f1 - test_f1

    print(f"\nğŸ” ANÃLISIS DE OVERFITTING:")
    print(f"   â€¢ Diferencia Accuracy: {overfitting_accuracy:.4f}")
    print(f"   â€¢ Diferencia F1-Score: {overfitting_f1:.4f}")

    if overfitting_accuracy > 0.05 or overfitting_f1 > 0.05:
        print("   âš ï¸ Posible overfitting detectado")
    else:
        print("   âœ… Modelo bien generalizado")

    # 4. VALIDACIÃ“N CRUZADA
    print(f"\nğŸ”„ VALIDACIÃ“N CRUZADA (5-FOLD):")
    print("-" * 40)

    # Configurar validaciÃ³n cruzada estratificada
    cv_scores = cross_val_score(rf_model, X_train, y_train,
                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
                               scoring='f1')

    print(f"   â€¢ Scores F1 por fold: {[f'{score:.4f}' for score in cv_scores]}")
    print(f"   â€¢ Media F1-Score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # 5. IMPORTANCIA DE VARIABLES
    print(f"\nğŸ† TOP 10 VARIABLES MÃS IMPORTANTES:")
    print("-" * 40)

    feature_importance = pd.DataFrame({
        'feature': data_prepared['feature_names'],
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)

    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['feature']:20} â†’ {row['importance']:.4f}")

    print("="*50)

    return {
        'model': rf_model,
        'feature_importance': feature_importance,
        'metrics': {
            'train': {
                'accuracy': train_accuracy,
                'precision': train_precision,
                'recall': train_recall,
                'f1': train_f1,
                'auc': train_auc
            },
            'test': {
                'accuracy': test_accuracy,
                'precision': test_precision,
                'recall': test_recall,
                'f1': test_f1,
                'auc': test_auc
            }
        },
        'predictions': {
            'y_test_pred': y_test_pred,
            'y_test_pred_proba': y_test_pred_proba
        },
        'cv_scores': cv_scores
    }

# Entrenar Random Forest
rf_results = train_random_forest_model(data_prepared)

"""## 9. Modelo 2: RegresiÃ³n LogÃ­stica (Con NormalizaciÃ³n)

### ğŸ“Š CaracterÃ­sticas de la RegresiÃ³n LogÃ­stica:
- **Requiere normalizaciÃ³n**: Es sensible a la escala de las variables porque utiliza optimizaciÃ³n basada en gradientes
- **Lineal**: Asume relaciones lineales entre variables y el log-odds del target
- **Interpretable**: Los coeficientes son fÃ¡cilmente interpretables
- **ProbabilÃ­stico**: Proporciona probabilidades bien calibradas

### ğŸ¯ JustificaciÃ³n de la NormalizaciÃ³n:
La normalizaciÃ³n es **crÃ­tica** para la RegresiÃ³n LogÃ­stica porque:
1. **Convergencia**: Variables no escaladas pueden hacer que el algoritmo converja lentamente o no converja
2. **Coeficientes**: Variables con mayor escala pueden dominar la funciÃ³n de costo
3. **RegularizaciÃ³n**: La regularizaciÃ³n L1/L2 afecta de manera desigual a variables de diferentes escalas
4. **InterpretaciÃ³n**: Los coeficientes normalizados son comparables entre sÃ­
"""

def train_logistic_regression_model(data_prepared):
    """
    Entrena y evalÃºa el modelo de RegresiÃ³n LogÃ­stica (con normalizaciÃ³n)
    """
    print("ğŸ“Š ENTRENANDO MODELO REGRESIÃ“N LOGÃSTICA")
    print("="*50)

    # Extraer datos
    X_train = data_prepared['X_train'].copy()
    X_test = data_prepared['X_test'].copy()
    y_train = data_prepared['y_train']
    y_test = data_prepared['y_test']
    scaler = data_prepared['scaler']
    numerical_cols = data_prepared['numerical_cols']

    print(f"ğŸ“Š Datos de entrenamiento: {X_train.shape}")
    print(f"ğŸ“Š Datos de prueba: {X_test.shape}")

    # 1. APLICAR NORMALIZACIÃ“N A VARIABLES NUMÃ‰RICAS
    print(f"\nğŸ”§ APLICANDO NORMALIZACIÃ“N (StandardScaler):")
    print("-" * 40)

    if numerical_cols:
        print(f"   ğŸ“ Variables a normalizar: {numerical_cols}")

        # Aplicar escalado a datos de entrenamiento
        X_train_scaled = X_train.copy()
        X_train_scaled[numerical_cols] = scaler.transform(X_train[numerical_cols])

        # Aplicar escalado a datos de prueba (usando el mismo escalador)
        X_test_scaled = X_test.copy()
        X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])

        print(f"   âœ… NormalizaciÃ³n aplicada")

        # Mostrar estadÃ­sticas antes y despuÃ©s del escalado
        print(f"\n   ğŸ“ˆ ESTADÃSTICAS ANTES DEL ESCALADO (muestra):")
        for col in numerical_cols[:2]:  # Mostrar solo las primeras 2 columnas
            mean_before = X_train[col].mean()
            std_before = X_train[col].std()
            print(f"      â€¢ {col}: Î¼={mean_before:.2f}, Ïƒ={std_before:.2f}")

        print(f"\n   ğŸ“‰ ESTADÃSTICAS DESPUÃ‰S DEL ESCALADO (muestra):")
        for col in numerical_cols[:2]:  # Mostrar solo las primeras 2 columnas
            mean_after = X_train_scaled[col].mean()
            std_after = X_train_scaled[col].std()
            print(f"      â€¢ {col}: Î¼={mean_after:.2f}, Ïƒ={std_after:.2f}")
    else:
        print("   âš ï¸ No hay variables numÃ©ricas para normalizar")
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()

    # 2. ENTRENAR MODELO REGRESIÃ“N LOGÃSTICA
    print(f"\nğŸ”§ CONFIGURANDO Y ENTRENANDO REGRESIÃ“N LOGÃSTICA:")
    print("-" * 40)

    lr_model = LogisticRegression(
        C=1.0,                      # ParÃ¡metro de regularizaciÃ³n
        penalty='l2',               # RegularizaciÃ³n L2 (Ridge)
        class_weight='balanced',    # Balancear clases automÃ¡ticamente
        random_state=42,            # Reproducibilidad
        max_iter=1000,              # MÃ¡ximo nÃºmero de iteraciones
        solver='lbfgs'              # Solver para optimizaciÃ³n
    )

    # Entrenar el modelo
    print("   ğŸ‹ï¸ Entrenando modelo...")
    lr_model.fit(X_train_scaled, y_train)
    print("   âœ… Entrenamiento completado")

    # 3. REALIZAR PREDICCIONES
    print(f"\nğŸ”® REALIZANDO PREDICCIONES:")
    print("-" * 40)

    # Predicciones en conjunto de entrenamiento
    y_train_pred = lr_model.predict(X_train_scaled)
    y_train_pred_proba = lr_model.predict_proba(X_train_scaled)[:, 1]

    # Predicciones en conjunto de prueba
    y_test_pred = lr_model.predict(X_test_scaled)
    y_test_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]

    print("   âœ… Predicciones completadas")

    # 4. EVALUAR RENDIMIENTO
    print(f"\nğŸ“ˆ EVALUACIÃ“N DEL RENDIMIENTO:")
    print("-" * 40)

    # MÃ©tricas en conjunto de entrenamiento
    train_accuracy = accuracy_score(y_train, y_train_pred)
    train_precision = precision_score(y_train, y_train_pred)
    train_recall = recall_score(y_train, y_train_pred)
    train_f1 = f1_score(y_train, y_train_pred)
    train_auc = roc_auc_score(y_train, y_train_pred_proba)

    # MÃ©tricas en conjunto de prueba
    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_precision = precision_score(y_test, y_test_pred)
    test_recall = recall_score(y_test, y_test_pred)
    test_f1 = f1_score(y_test, y_test_pred)
    test_auc = roc_auc_score(y_test, y_test_pred_proba)

    print(f"ğŸ“Š MÃ‰TRICAS DE ENTRENAMIENTO:")
    print(f"   â€¢ Accuracy:  {train_accuracy:.4f}")
    print(f"   â€¢ Precision: {train_precision:.4f}")
    print(f"   â€¢ Recall:    {train_recall:.4f}")
    print(f"   â€¢ F1-Score:  {train_f1:.4f}")
    print(f"   â€¢ AUC-ROC:   {train_auc:.4f}")

    print(f"\nğŸ“Š MÃ‰TRICAS DE PRUEBA:")
    print(f"   â€¢ Accuracy:  {test_accuracy:.4f}")
    print(f"   â€¢ Precision: {test_precision:.4f}")
    print(f"   â€¢ Recall:    {test_recall:.4f}")
    print(f"   â€¢ F1-Score:  {test_f1:.4f}")
    print(f"   â€¢ AUC-ROC:   {test_auc:.4f}")

    # EvaluaciÃ³n de overfitting
    overfitting_accuracy = train_accuracy - test_accuracy
    overfitting_f1 = train_f1 - test_f1

    print(f"\nğŸ” ANÃLISIS DE OVERFITTING:")
    print(f"   â€¢ Diferencia Accuracy: {overfitting_accuracy:.4f}")
    print(f"   â€¢ Diferencia F1-Score: {overfitting_f1:.4f}")

    if overfitting_accuracy > 0.05 or overfitting_f1 > 0.05:
        print("   âš ï¸ Posible overfitting detectado")
    else:
        print("   âœ… Modelo bien generalizado")

    # 5. VALIDACIÃ“N CRUZADA
    print(f"\nğŸ”„ VALIDACIÃ“N CRUZADA (5-FOLD):")
    print("-" * 40)

    # Configurar validaciÃ³n cruzada estratificada
    cv_scores = cross_val_score(lr_model, X_train_scaled, y_train,
                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
                               scoring='f1')

    print(f"   â€¢ Scores F1 por fold: {[f'{score:.4f}' for score in cv_scores]}")
    print(f"   â€¢ Media F1-Score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # 6. ANÃLISIS DE COEFICIENTES
    print(f"\nğŸ† TOP 10 COEFICIENTES MÃS IMPORTANTES:")
    print("-" * 40)

    # Obtener coeficientes del modelo
    feature_coefficients = pd.DataFrame({
        'feature': data_prepared['feature_names'],
        'coefficient': lr_model.coef_[0],
        'abs_coefficient': np.abs(lr_model.coef_[0])
    }).sort_values('abs_coefficient', ascending=False)

    for i, (_, row) in enumerate(feature_coefficients.head(10).iterrows(), 1):
        direction = "ğŸ“ˆ Positivo" if row['coefficient'] > 0 else "ğŸ“‰ Negativo"
        print(f"   {i:2d}. {row['feature']:20} â†’ {row['coefficient']:7.4f} ({direction})")

    print("="*50)

    return {
        'model': lr_model,
        'scaler': scaler,
        'feature_coefficients': feature_coefficients,
        'X_train_scaled': X_train_scaled,
        'X_test_scaled': X_test_scaled,
        'metrics': {
            'train': {
                'accuracy': train_accuracy,
                'precision': train_precision,
                'recall': train_recall,
                'f1': train_f1,
                'auc': train_auc
            },
            'test': {
                'accuracy': test_accuracy,
                'precision': test_precision,
                'recall': test_recall,
                'f1': test_f1,
                'auc': test_auc
            }
        },
        'predictions': {
            'y_test_pred': y_test_pred,
            'y_test_pred_proba': y_test_pred_proba
        },
        'cv_scores': cv_scores
    }

# Entrenar RegresiÃ³n LogÃ­stica
lr_results = train_logistic_regression_model(data_prepared)

"""## 10. ComparaciÃ³n de Modelos y Visualizaciones

En esta secciÃ³n compararemos el rendimiento de ambos modelos y crearemos visualizaciones para evaluar:

### ğŸ“Š MÃ©tricas de ComparaciÃ³n:
- **Accuracy, Precision, Recall, F1-Score**: Para evaluar rendimiento general
- **AUC-ROC**: Para evaluar capacidad de discriminaciÃ³n
- **Matrices de ConfusiÃ³n**: Para entender errores especÃ­ficos
- **Curvas ROC**: Para visualizar trade-off entre sensibilidad y especificidad

### ğŸ¯ Interpretabilidad:
- **Random Forest**: Importancia de variables basada en reducciÃ³n de impureza
- **RegresiÃ³n LogÃ­stica**: Coeficientes que indican direcciÃ³n y magnitud del efecto
"""

def compare_models_and_visualize(rf_results, lr_results, data_prepared):
    """
    Compara ambos modelos y crea visualizaciones comprehensivas
    """
    print("âš–ï¸ COMPARACIÃ“N DETALLADA DE MODELOS")
    print("="*60)

    # 1. TABLA COMPARATIVA DE MÃ‰TRICAS
    print("\nğŸ“Š TABLA COMPARATIVA DE RENDIMIENTO:")
    print("-" * 60)

    # Crear DataFrame comparativo
    comparison_df = pd.DataFrame({
        'MÃ©trica': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],
        'Random Forest': [
            rf_results['metrics']['test']['accuracy'],
            rf_results['metrics']['test']['precision'],
            rf_results['metrics']['test']['recall'],
            rf_results['metrics']['test']['f1'],
            rf_results['metrics']['test']['auc']
        ],
        'RegresiÃ³n LogÃ­stica': [
            lr_results['metrics']['test']['accuracy'],
            lr_results['metrics']['test']['precision'],
            lr_results['metrics']['test']['recall'],
            lr_results['metrics']['test']['f1'],
            lr_results['metrics']['test']['auc']
        ]
    })

    # Calcular diferencias
    comparison_df['Diferencia (RF - LR)'] = (
        comparison_df['Random Forest'] - comparison_df['RegresiÃ³n LogÃ­stica']
    )

    print(comparison_df.round(4).to_string(index=False))

    # 2. ANÃLISIS COMPARATIVO
    print(f"\nğŸ† ANÃLISIS COMPARATIVO:")
    print("-" * 40)

    # Determinar el mejor modelo por mÃ©trica
    best_accuracy = "Random Forest" if rf_results['metrics']['test']['accuracy'] > lr_results['metrics']['test']['accuracy'] else "RegresiÃ³n LogÃ­stica"
    best_f1 = "Random Forest" if rf_results['metrics']['test']['f1'] > lr_results['metrics']['test']['f1'] else "RegresiÃ³n LogÃ­stica"
    best_auc = "Random Forest" if rf_results['metrics']['test']['auc'] > lr_results['metrics']['test']['auc'] else "RegresiÃ³n LogÃ­stica"

    print(f"   â€¢ Mejor Accuracy: {best_accuracy}")
    print(f"   â€¢ Mejor F1-Score: {best_f1}")
    print(f"   â€¢ Mejor AUC-ROC: {best_auc}")

    # ValidaciÃ³n cruzada
    rf_cv_mean = rf_results['cv_scores'].mean()
    lr_cv_mean = lr_results['cv_scores'].mean()
    best_cv = "Random Forest" if rf_cv_mean > lr_cv_mean else "RegresiÃ³n LogÃ­stica"

    print(f"   â€¢ Mejor CV F1-Score: {best_cv} ({rf_cv_mean:.4f} vs {lr_cv_mean:.4f})")

    # 3. CREAR VISUALIZACIONES
    print(f"\nğŸ¨ CREANDO VISUALIZACIONES COMPARATIVAS...")
    print("-" * 40)

    # Configurar la figura con mÃºltiples subplots
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ğŸ” ComparaciÃ³n Exhaustiva de Modelos: Random Forest vs RegresiÃ³n LogÃ­stica',
                fontsize=16, fontweight='bold', y=0.98)

    # SUBPLOT 1: ComparaciÃ³n de mÃ©tricas
    ax1 = axes[0, 0]
    x_pos = np.arange(len(comparison_df))
    width = 0.35

    ax1.bar(x_pos - width/2, comparison_df['Random Forest'], width,
           label='Random Forest', color='#2ECC71', alpha=0.8)
    ax1.bar(x_pos + width/2, comparison_df['RegresiÃ³n LogÃ­stica'], width,
           label='RegresiÃ³n LogÃ­stica', color='#3498DB', alpha=0.8)

    ax1.set_xlabel('MÃ©tricas')
    ax1.set_ylabel('Valor')
    ax1.set_title('ğŸ“Š ComparaciÃ³n de MÃ©tricas', fontweight='bold')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(comparison_df['MÃ©trica'], rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # SUBPLOT 2: Matrices de ConfusiÃ³n - Random Forest
    ax2 = axes[0, 1]
    y_test = data_prepared['y_test']
    rf_cm = confusion_matrix(y_test, rf_results['predictions']['y_test_pred'])
    sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=ax2)
    ax2.set_title('ğŸŒ² Matriz de ConfusiÃ³n\nRandom Forest', fontweight='bold')
    ax2.set_xlabel('PredicciÃ³n')
    ax2.set_ylabel('Real')

    # SUBPLOT 3: Matrices de ConfusiÃ³n - RegresiÃ³n LogÃ­stica
    ax3 = axes[0, 2]
    lr_cm = confusion_matrix(y_test, lr_results['predictions']['y_test_pred'])
    sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Oranges', ax=ax3)
    ax3.set_title('ğŸ“Š Matriz de ConfusiÃ³n\nRegresiÃ³n LogÃ­stica', fontweight='bold')
    ax3.set_xlabel('PredicciÃ³n')
    ax3.set_ylabel('Real')

    # SUBPLOT 4: Curvas ROC
    ax4 = axes[1, 0]

    # Random Forest ROC
    rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_results['predictions']['y_test_pred_proba'])
    rf_auc = rf_results['metrics']['test']['auc']

    # RegresiÃ³n LogÃ­stica ROC
    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_results['predictions']['y_test_pred_proba'])
    lr_auc = lr_results['metrics']['test']['auc']

    ax4.plot(rf_fpr, rf_tpr, color='#2ECC71', lw=2,
            label=f'Random Forest (AUC = {rf_auc:.3f})')
    ax4.plot(lr_fpr, lr_tpr, color='#3498DB', lw=2,
            label=f'RegresiÃ³n LogÃ­stica (AUC = {lr_auc:.3f})')
    ax4.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)

    ax4.set_xlim([0.0, 1.0])
    ax4.set_ylim([0.0, 1.05])
    ax4.set_xlabel('Tasa de Falsos Positivos')
    ax4.set_ylabel('Tasa de Verdaderos Positivos')
    ax4.set_title('ğŸ“ˆ Curvas ROC', fontweight='bold')
    ax4.legend(loc="lower right")
    ax4.grid(True, alpha=0.3)

    # SUBPLOT 5: Importancia de Variables (Random Forest)
    ax5 = axes[1, 1]
    top_features_rf = rf_results['feature_importance'].head(10)
    ax5.barh(range(len(top_features_rf)), top_features_rf['importance'],
            color='#2ECC71', alpha=0.8)
    ax5.set_yticks(range(len(top_features_rf)))
    ax5.set_yticklabels(top_features_rf['feature'])
    ax5.set_xlabel('Importancia')
    ax5.set_title('ğŸŒ² Top 10 Variables\n(Random Forest)', fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='x')

    # SUBPLOT 6: Coeficientes (RegresiÃ³n LogÃ­stica)
    ax6 = axes[1, 2]
    top_features_lr = lr_results['feature_coefficients'].head(10)
    colors = ['#E74C3C' if x < 0 else '#3498DB' for x in top_features_lr['coefficient']]
    ax6.barh(range(len(top_features_lr)), top_features_lr['coefficient'],
            color=colors, alpha=0.8)
    ax6.set_yticks(range(len(top_features_lr)))
    ax6.set_yticklabels(top_features_lr['feature'])
    ax6.set_xlabel('Coeficiente')
    ax6.set_title('ğŸ“Š Top 10 Coeficientes\n(RegresiÃ³n LogÃ­stica)', fontweight='bold')
    ax6.grid(True, alpha=0.3, axis='x')
    ax6.axvline(x=0, color='black', linestyle='-', alpha=0.3)

    plt.tight_layout()
    plt.show()

    # 4. ANÃLISIS DE VARIABLES IMPORTANTES
    print(f"\nğŸ” ANÃLISIS DE VARIABLES MÃS IMPORTANTES:")
    print("-" * 60)

    # Variables importantes en Random Forest
    print(f"ğŸŒ² TOP 5 VARIABLES MÃS IMPORTANTES (Random Forest):")
    for i, (_, row) in enumerate(rf_results['feature_importance'].head(5).iterrows(), 1):
        print(f"   {i}. {row['feature']:25} â†’ {row['importance']:.4f}")

    print(f"\nğŸ“Š TOP 5 COEFICIENTES MÃS GRANDES (RegresiÃ³n LogÃ­stica):")
    for i, (_, row) in enumerate(lr_results['feature_coefficients'].head(5).iterrows(), 1):
        direction = "â¬†ï¸ Incrementa" if row['coefficient'] > 0 else "â¬‡ï¸ Reduce"
        print(f"   {i}. {row['feature']:25} â†’ {row['coefficient']:7.4f} ({direction} churn)")

    # 5. RECOMENDACIÃ“N FINAL
    print(f"\nğŸ… RECOMENDACIÃ“N FINAL:")
    print("-" * 40)

    # Determinar el mejor modelo basado en mÃºltiples mÃ©tricas
    rf_score = (rf_results['metrics']['test']['f1'] + rf_results['metrics']['test']['auc']) / 2
    lr_score = (lr_results['metrics']['test']['f1'] + lr_results['metrics']['test']['auc']) / 2

    if rf_score > lr_score:
        recommended_model = "Random Forest"
        winner_reason = f"Mejor rendimiento promedio (F1+AUC)/2: {rf_score:.4f} vs {lr_score:.4f}"
    else:
        recommended_model = "RegresiÃ³n LogÃ­stica"
        winner_reason = f"Mejor rendimiento promedio (F1+AUC)/2: {lr_score:.4f} vs {rf_score:.4f}"

    print(f"   ğŸ† Modelo Recomendado: {recommended_model}")
    print(f"   ğŸ“ˆ RazÃ³n: {winner_reason}")

    if recommended_model == "Random Forest":
        print(f"   ğŸ’¡ Ventajas: Robusto, maneja no-linealidades, no requiere normalizaciÃ³n")
        print(f"   âš ï¸ Consideraciones: Menos interpretable, puede hacer overfitting")
    else:
        print(f"   ğŸ’¡ Ventajas: Altamente interpretable, probabilidades calibradas, rÃ¡pido")
        print(f"   âš ï¸ Consideraciones: Asume linealidad, sensible a outliers, requiere normalizaciÃ³n")

    print("="*60)

    return comparison_df, recommended_model

# Ejecutar comparaciÃ³n
comparison_results, best_model = compare_models_and_visualize(rf_results, lr_results, data_prepared)

"""## 12. EvaluaciÃ³n Detallada y AnÃ¡lisis CrÃ­tico de Modelos

En esta secciÃ³n realizaremos una evaluaciÃ³n exhaustiva de ambos modelos utilizando mÃºltiples mÃ©tricas y anÃ¡lisis crÃ­tico del rendimiento, incluyendo detecciÃ³n de overfitting/underfitting.

### ğŸ“Š MÃ©tricas de EvaluaciÃ³n:
- **Exactitud (Accuracy)**: ProporciÃ³n de predicciones correctas
- **PrecisiÃ³n (Precision)**: ProporciÃ³n de verdaderos positivos entre todas las predicciones positivas
- **Recall (Sensibilidad)**: ProporciÃ³n de verdaderos positivos detectados
- **F1-Score**: Media armÃ³nica entre precisiÃ³n y recall
- **Matriz de ConfusiÃ³n**: AnÃ¡lisis detallado de errores de clasificaciÃ³n

### ğŸ” AnÃ¡lisis de Overfitting/Underfitting:
- **Overfitting**: Modelo aprende demasiado del entrenamiento, no generaliza
- **Underfitting**: Modelo muy simple, no captura patrones importantes
"""

def comprehensive_model_evaluation(rf_results, lr_results, data_prepared):
    """
    EvaluaciÃ³n exhaustiva y anÃ¡lisis crÃ­tico de ambos modelos
    """
    print("ğŸ”¬ EVALUACIÃ“N EXHAUSTIVA Y ANÃLISIS CRÃTICO DE MODELOS")
    print("="*80)

    # Extraer datos necesarios
    y_test = data_prepared['y_test']
    y_train = data_prepared['y_train']

    # 1. TABLA DETALLADA DE MÃ‰TRICAS
    print("\nğŸ“Š TABLA DETALLADA DE MÃ‰TRICAS DE RENDIMIENTO")
    print("-" * 80)

    # Crear tabla comprehensiva
    metrics_comparison = pd.DataFrame({
        'MÃ©trica': [
            'Exactitud (Accuracy)', 'PrecisiÃ³n (Precision)', 'Recall (Sensibilidad)',
            'F1-Score', 'AUC-ROC', 'Especificidad', 'Valor Predictivo Negativo'
        ],
        'Random Forest (Test)': [
            rf_results['metrics']['test']['accuracy'],
            rf_results['metrics']['test']['precision'],
            rf_results['metrics']['test']['recall'],
            rf_results['metrics']['test']['f1'],
            rf_results['metrics']['test']['auc'],
            # Calcular especificidad y VPN
            rf_results['metrics']['test']['accuracy'],  # Placeholder
            rf_results['metrics']['test']['accuracy']   # Placeholder
        ],
        'RegresiÃ³n LogÃ­stica (Test)': [
            lr_results['metrics']['test']['accuracy'],
            lr_results['metrics']['test']['precision'],
            lr_results['metrics']['test']['recall'],
            lr_results['metrics']['test']['f1'],
            lr_results['metrics']['test']['auc'],
            # Calcular especificidad y VPN
            lr_results['metrics']['test']['accuracy'],  # Placeholder
            lr_results['metrics']['test']['accuracy']   # Placeholder
        ]
    })

    # Calcular mÃ©tricas adicionales usando matrices de confusiÃ³n
    rf_cm = confusion_matrix(y_test, rf_results['predictions']['y_test_pred'])
    lr_cm = confusion_matrix(y_test, lr_results['predictions']['y_test_pred'])

    # Para Random Forest
    rf_tn, rf_fp, rf_fn, rf_tp = rf_cm.ravel()
    rf_specificity = rf_tn / (rf_tn + rf_fp) if (rf_tn + rf_fp) > 0 else 0
    rf_npv = rf_tn / (rf_tn + rf_fn) if (rf_tn + rf_fn) > 0 else 0

    # Para RegresiÃ³n LogÃ­stica
    lr_tn, lr_fp, lr_fn, lr_tp = lr_cm.ravel()
    lr_specificity = lr_tn / (lr_tn + lr_fp) if (lr_tn + lr_fp) > 0 else 0
    lr_npv = lr_tn / (lr_tn + lr_fn) if (lr_tn + lr_fn) > 0 else 0

    # Actualizar tabla con mÃ©tricas calculadas
    metrics_comparison.loc[5, 'Random Forest (Test)'] = rf_specificity
    metrics_comparison.loc[6, 'Random Forest (Test)'] = rf_npv
    metrics_comparison.loc[5, 'RegresiÃ³n LogÃ­stica (Test)'] = lr_specificity
    metrics_comparison.loc[6, 'RegresiÃ³n LogÃ­stica (Test)'] = lr_npv

    # Calcular diferencias
    metrics_comparison['Diferencia (RF - LR)'] = (
        metrics_comparison['Random Forest (Test)'] - metrics_comparison['RegresiÃ³n LogÃ­stica (Test)']
    )

    # Determinar ganador por mÃ©trica
    metrics_comparison['Ganador'] = metrics_comparison['Diferencia (RF - LR)'].apply(
        lambda x: 'ğŸ† Random Forest' if x > 0 else 'ğŸ† Reg. LogÃ­stica' if x < 0 else 'ğŸ¤ Empate'
    )

    print(metrics_comparison.round(4).to_string(index=False))

    # 2. ANÃLISIS DETALLADO DE MATRICES DE CONFUSIÃ“N
    print(f"\nğŸ¯ ANÃLISIS DETALLADO DE MATRICES DE CONFUSIÃ“N")
    print("-" * 80)

    print("ğŸŒ² RANDOM FOREST - Matriz de ConfusiÃ³n:")
    print(f"   Verdaderos Negativos (TN): {rf_tn:4d} | Falsos Positivos (FP): {rf_fp:4d}")
    print(f"   Falsos Negativos (FN):     {rf_fn:4d} | Verdaderos Positivos (TP): {rf_tp:4d}")
    print(f"   Total predicciones correctas: {rf_tn + rf_tp:4d} de {len(y_test):4d} ({((rf_tn + rf_tp)/len(y_test)*100):.1f}%)")

    print("\nğŸ“Š REGRESIÃ“N LOGÃSTICA - Matriz de ConfusiÃ³n:")
    print(f"   Verdaderos Negativos (TN): {lr_tn:4d} | Falsos Positivos (FP): {lr_fp:4d}")
    print(f"   Falsos Negativos (FN):     {lr_fn:4d} | Verdaderos Positivos (TP): {lr_tp:4d}")
    print(f"   Total predicciones correctas: {lr_tn + lr_tp:4d} de {len(y_test):4d} ({((lr_tn + lr_tp)/len(y_test)*100):.1f}%)")

    # 3. ANÃLISIS DE OVERFITTING/UNDERFITTING
    print(f"\nğŸ” ANÃLISIS CRÃTICO: OVERFITTING Y UNDERFITTING")
    print("-" * 80)

    # Random Forest - AnÃ¡lisis de overfitting
    rf_train_acc = rf_results['metrics']['train']['accuracy']
    rf_test_acc = rf_results['metrics']['test']['accuracy']
    rf_train_f1 = rf_results['metrics']['train']['f1']
    rf_test_f1 = rf_results['metrics']['test']['f1']
    rf_cv_f1_mean = rf_results['cv_scores'].mean()
    rf_cv_f1_std = rf_results['cv_scores'].std()

    print("ğŸŒ² RANDOM FOREST:")
    print(f"   ğŸ“ˆ Accuracy:  Entrenamiento={rf_train_acc:.4f} | Test={rf_test_acc:.4f} | Diferencia={rf_train_acc-rf_test_acc:.4f}")
    print(f"   ğŸ“ˆ F1-Score:  Entrenamiento={rf_train_f1:.4f} | Test={rf_test_f1:.4f} | Diferencia={rf_train_f1-rf_test_f1:.4f}")
    print(f"   ğŸ“ˆ CV F1:     Media={rf_cv_f1_mean:.4f} Â± {rf_cv_f1_std:.4f}")

    # DiagnÃ³stico Random Forest
    rf_overfit_score = (rf_train_acc - rf_test_acc) + (rf_train_f1 - rf_test_f1)
    if rf_overfit_score > 0.1:
        rf_diagnosis = "ğŸ”´ OVERFITTING DETECTADO"
        rf_recommendation = "Reducir max_depth, aumentar min_samples_split/leaf, usar menos estimadores"
    elif rf_test_acc < 0.7 or rf_test_f1 < 0.5:
        rf_diagnosis = "ğŸ”µ POSIBLE UNDERFITTING"
        rf_recommendation = "Aumentar max_depth, reducir min_samples_split/leaf, mÃ¡s estimadores"
    else:
        rf_diagnosis = "âœ… MODELO BIEN BALANCEADO"
        rf_recommendation = "Modelo generaliza adecuadamente"

    print(f"   ğŸ¯ DiagnÃ³stico: {rf_diagnosis}")
    print(f"   ğŸ’¡ RecomendaciÃ³n: {rf_recommendation}")

    # RegresiÃ³n LogÃ­stica - AnÃ¡lisis de overfitting
    lr_train_acc = lr_results['metrics']['train']['accuracy']
    lr_test_acc = lr_results['metrics']['test']['accuracy']
    lr_train_f1 = lr_results['metrics']['train']['f1']
    lr_test_f1 = lr_results['metrics']['test']['f1']
    lr_cv_f1_mean = lr_results['cv_scores'].mean()
    lr_cv_f1_std = lr_results['cv_scores'].std()

    print(f"\nğŸ“Š REGRESIÃ“N LOGÃSTICA:")
    print(f"   ğŸ“ˆ Accuracy:  Entrenamiento={lr_train_acc:.4f} | Test={lr_test_acc:.4f} | Diferencia={lr_train_acc-lr_test_acc:.4f}")
    print(f"   ğŸ“ˆ F1-Score:  Entrenamiento={lr_train_f1:.4f} | Test={lr_test_f1:.4f} | Diferencia={lr_train_f1-lr_test_f1:.4f}")
    print(f"   ğŸ“ˆ CV F1:     Media={lr_cv_f1_mean:.4f} Â± {lr_cv_f1_std:.4f}")

    # DiagnÃ³stico RegresiÃ³n LogÃ­stica
    lr_overfit_score = (lr_train_acc - lr_test_acc) + (lr_train_f1 - lr_test_f1)
    if lr_overfit_score > 0.1:
        lr_diagnosis = "ğŸ”´ OVERFITTING DETECTADO"
        lr_recommendation = "Aumentar regularizaciÃ³n (reducir C), usar L1 penalty, feature selection"
    elif lr_test_acc < 0.7 or lr_test_f1 < 0.5:
        lr_diagnosis = "ğŸ”µ POSIBLE UNDERFITTING"
        lr_recommendation = "Reducir regularizaciÃ³n (aumentar C), agregar features polinomiales"
    else:
        lr_diagnosis = "âœ… MODELO BIEN BALANCEADO"
        lr_recommendation = "Modelo generaliza adecuadamente"

    print(f"   ğŸ¯ DiagnÃ³stico: {lr_diagnosis}")
    print(f"   ğŸ’¡ RecomendaciÃ³n: {lr_recommendation}")

    # 4. ANÃLISIS DE ESTABILIDAD (ValidaciÃ³n Cruzada)
    print(f"\nğŸ“Š ANÃLISIS DE ESTABILIDAD (VALIDACIÃ“N CRUZADA)")
    print("-" * 80)

    print(f"ğŸŒ² Random Forest:")
    print(f"   F1-Scores por fold: {[f'{score:.3f}' for score in rf_results['cv_scores']]}")
    print(f"   Variabilidad: {rf_cv_f1_std:.4f} ({'ESTABLE' if rf_cv_f1_std < 0.05 else 'INESTABLE'})")

    print(f"\nğŸ“Š RegresiÃ³n LogÃ­stica:")
    print(f"   F1-Scores por fold: {[f'{score:.3f}' for score in lr_results['cv_scores']]}")
    print(f"   Variabilidad: {lr_cv_f1_std:.4f} ({'ESTABLE' if lr_cv_f1_std < 0.05 else 'INESTABLE'})")

    # 5. DETERMINACIÃ“N DEL MEJOR MODELO
    print(f"\nğŸ† DETERMINACIÃ“N DEL MEJOR MODELO")
    print("-" * 80)

    # Sistema de puntuaciÃ³n multi-criterio
    rf_score = (
        rf_results['metrics']['test']['accuracy'] * 0.2 +
        rf_results['metrics']['test']['precision'] * 0.2 +
        rf_results['metrics']['test']['recall'] * 0.2 +
        rf_results['metrics']['test']['f1'] * 0.25 +
        rf_results['metrics']['test']['auc'] * 0.15
    )

    lr_score = (
        lr_results['metrics']['test']['accuracy'] * 0.2 +
        lr_results['metrics']['test']['precision'] * 0.2 +
        lr_results['metrics']['test']['recall'] * 0.2 +
        lr_results['metrics']['test']['f1'] * 0.25 +
        lr_results['metrics']['test']['auc'] * 0.15
    )

    # Penalizar por overfitting
    rf_penalty = max(0, rf_overfit_score * 0.1)
    lr_penalty = max(0, lr_overfit_score * 0.1)

    rf_final_score = rf_score - rf_penalty
    lr_final_score = lr_score - lr_penalty

    print(f"ğŸŒ² Random Forest:")
    print(f"   Score base: {rf_score:.4f}")
    print(f"   PenalizaciÃ³n overfitting: {rf_penalty:.4f}")
    print(f"   Score final: {rf_final_score:.4f}")

    print(f"\nğŸ“Š RegresiÃ³n LogÃ­stica:")
    print(f"   Score base: {lr_score:.4f}")
    print(f"   PenalizaciÃ³n overfitting: {lr_penalty:.4f}")
    print(f"   Score final: {lr_final_score:.4f}")

    # Determinar ganador
    if rf_final_score > lr_final_score:
        winner = "Random Forest"
        advantage = rf_final_score - lr_final_score
    else:
        winner = "RegresiÃ³n LogÃ­stica"
        advantage = lr_final_score - rf_final_score

    print(f"\nğŸ… MODELO GANADOR: {winner}")
    print(f"ğŸ“ˆ Ventaja: {advantage:.4f} puntos")

    # 6. ANÃLISIS DE ERRORES ESPECÃFICOS
    print(f"\nğŸ¯ ANÃLISIS DE ERRORES ESPECÃFICOS")
    print("-" * 80)

    # AnÃ¡lisis para Random Forest
    rf_fpr = rf_fp / (rf_fp + rf_tn) if (rf_fp + rf_tn) > 0 else 0  # Tasa Falsos Positivos
    rf_fnr = rf_fn / (rf_fn + rf_tp) if (rf_fn + rf_tp) > 0 else 0  # Tasa Falsos Negativos

    print(f"ğŸŒ² Random Forest - AnÃ¡lisis de Errores:")
    print(f"   Tasa Falsos Positivos: {rf_fpr:.3f} ({rf_fp} de {rf_fp + rf_tn})")
    print(f"   Tasa Falsos Negativos:  {rf_fnr:.3f} ({rf_fn} de {rf_fn + rf_tp})")
    print(f"   Impacto negocio FP: {rf_fp} clientes mal clasificados como churn")
    print(f"   Impacto negocio FN: {rf_fn} churns no detectados")

    # AnÃ¡lisis para RegresiÃ³n LogÃ­stica
    lr_fpr = lr_fp / (lr_fp + lr_tn) if (lr_fp + lr_tn) > 0 else 0
    lr_fnr = lr_fn / (lr_fn + lr_tp) if (lr_fn + lr_tp) > 0 else 0

    print(f"\nğŸ“Š RegresiÃ³n LogÃ­stica - AnÃ¡lisis de Errores:")
    print(f"   Tasa Falsos Positivos: {lr_fpr:.3f} ({lr_fp} de {lr_fp + lr_tn})")
    print(f"   Tasa Falsos Negativos:  {lr_fnr:.3f} ({lr_fn} de {lr_fn + lr_tp})")
    print(f"   Impacto negocio FP: {lr_fp} clientes mal clasificados como churn")
    print(f"   Impacto negocio FN: {lr_fn} churns no detectados")

    print("="*80)

    return {
        'metrics_comparison': metrics_comparison,
        'winner': winner,
        'rf_diagnosis': rf_diagnosis,
        'lr_diagnosis': lr_diagnosis,
        'rf_recommendations': rf_recommendation,
        'lr_recommendations': lr_recommendation,
        'confusion_matrices': {
            'rf': rf_cm,
            'lr': lr_cm
        },
        'final_scores': {
            'rf': rf_final_score,
            'lr': lr_final_score
        }
    }

# Ejecutar evaluaciÃ³n exhaustiva
evaluation_results = comprehensive_model_evaluation(rf_results, lr_results, data_prepared)

def create_detailed_evaluation_plots(rf_results, lr_results, data_prepared, evaluation_results):
    """
    Crea visualizaciones detalladas para la evaluaciÃ³n de modelos
    """
    print("ğŸ¨ CREANDO VISUALIZACIONES DETALLADAS DE EVALUACIÃ“N")
    print("="*60)

    # Configurar figura con mÃºltiples subplots
    fig = plt.figure(figsize=(20, 16))

    # 1. COMPARACIÃ“N DE MÃ‰TRICAS (subplot 1)
    ax1 = plt.subplot(3, 3, 1)
    metrics_df = evaluation_results['metrics_comparison'].iloc[:5]  # Primeras 5 mÃ©tricas

    x_pos = np.arange(len(metrics_df))
    width = 0.35

    bars1 = ax1.bar(x_pos - width/2, metrics_df['Random Forest (Test)'], width,
                   label='Random Forest', color='#2ECC71', alpha=0.8)
    bars2 = ax1.bar(x_pos + width/2, metrics_df['RegresiÃ³n LogÃ­stica (Test)'], width,
                   label='RegresiÃ³n LogÃ­stica', color='#3498DB', alpha=0.8)

    ax1.set_xlabel('MÃ©tricas')
    ax1.set_ylabel('Valor')
    ax1.set_title('ğŸ“Š ComparaciÃ³n Detallada de MÃ©tricas', fontweight='bold')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'], rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Agregar valores en las barras
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)

    # 2. MATRICES DE CONFUSIÃ“N - Random Forest (subplot 2)
    ax2 = plt.subplot(3, 3, 2)
    rf_cm = evaluation_results['confusion_matrices']['rf']
    sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=ax2,
                xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
    ax2.set_title('ğŸŒ² Random Forest\nMatriz de ConfusiÃ³n', fontweight='bold')
    ax2.set_xlabel('PredicciÃ³n')
    ax2.set_ylabel('Realidad')

    # 3. MATRICES DE CONFUSIÃ“N - RegresiÃ³n LogÃ­stica (subplot 3)
    ax3 = plt.subplot(3, 3, 3)
    lr_cm = evaluation_results['confusion_matrices']['lr']
    sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Oranges', ax=ax3,
                xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
    ax3.set_title('ğŸ“Š RegresiÃ³n LogÃ­stica\nMatriz de ConfusiÃ³n', fontweight='bold')
    ax3.set_xlabel('PredicciÃ³n')
    ax3.set_ylabel('Realidad')

    # 4. ANÃLISIS TRAIN VS TEST (subplot 4)
    ax4 = plt.subplot(3, 3, 4)

    models = ['Random Forest', 'RegresiÃ³n LogÃ­stica']
    train_f1 = [rf_results['metrics']['train']['f1'], lr_results['metrics']['train']['f1']]
    test_f1 = [rf_results['metrics']['test']['f1'], lr_results['metrics']['test']['f1']]

    x_pos = np.arange(len(models))
    width = 0.35

    ax4.bar(x_pos - width/2, train_f1, width, label='Entrenamiento', color='#E74C3C', alpha=0.8)
    ax4.bar(x_pos + width/2, test_f1, width, label='Prueba', color='#27AE60', alpha=0.8)

    ax4.set_xlabel('Modelos')
    ax4.set_ylabel('F1-Score')
    ax4.set_title('ğŸ” AnÃ¡lisis Overfitting\n(Train vs Test)', fontweight='bold')
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels(models)
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    # 5. CURVAS ROC COMPARATIVAS (subplot 5)
    ax5 = plt.subplot(3, 3, 5)

    y_test = data_prepared['y_test']

    # Random Forest ROC
    rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_results['predictions']['y_test_pred_proba'])
    rf_auc = rf_results['metrics']['test']['auc']

    # RegresiÃ³n LogÃ­stica ROC
    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_results['predictions']['y_test_pred_proba'])
    lr_auc = lr_results['metrics']['test']['auc']

    ax5.plot(rf_fpr, rf_tpr, color='#2ECC71', lw=2,
            label=f'Random Forest (AUC = {rf_auc:.3f})')
    ax5.plot(lr_fpr, lr_tpr, color='#3498DB', lw=2,
            label=f'RegresiÃ³n LogÃ­stica (AUC = {lr_auc:.3f})')
    ax5.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)

    ax5.set_xlim([0.0, 1.0])
    ax5.set_ylim([0.0, 1.05])
    ax5.set_xlabel('Tasa de Falsos Positivos')
    ax5.set_ylabel('Tasa de Verdaderos Positivos')
    ax5.set_title('ğŸ“ˆ Curvas ROC Comparativas', fontweight='bold')
    ax5.legend(loc="lower right")
    ax5.grid(True, alpha=0.3)

    # 6. VALIDACIÃ“N CRUZADA - Estabilidad (subplot 6)
    ax6 = plt.subplot(3, 3, 6)

    folds = list(range(1, 6))
    ax6.plot(folds, rf_results['cv_scores'], 'o-', color='#2ECC71',
            linewidth=2, markersize=8, label='Random Forest')
    ax6.plot(folds, lr_results['cv_scores'], 's-', color='#3498DB',
            linewidth=2, markersize=8, label='RegresiÃ³n LogÃ­stica')

    ax6.axhline(y=rf_results['cv_scores'].mean(), color='#2ECC71',
               linestyle='--', alpha=0.7, label=f'RF Media: {rf_results["cv_scores"].mean():.3f}')
    ax6.axhline(y=lr_results['cv_scores'].mean(), color='#3498DB',
               linestyle='--', alpha=0.7, label=f'LR Media: {lr_results["cv_scores"].mean():.3f}')

    ax6.set_xlabel('Fold')
    ax6.set_ylabel('F1-Score')
    ax6.set_title('ğŸ”„ Estabilidad en ValidaciÃ³n Cruzada', fontweight='bold')
    ax6.set_xticks(folds)
    ax6.legend()
    ax6.grid(True, alpha=0.3)

    # 7. DISTRIBUCIÃ“N DE PROBABILIDADES (subplot 7)
    ax7 = plt.subplot(3, 3, 7)

    # Crear histogramas de probabilidades predichas
    ax7.hist(rf_results['predictions']['y_test_pred_proba'], bins=30, alpha=0.5,
            label='Random Forest', color='#2ECC71', density=True)
    ax7.hist(lr_results['predictions']['y_test_pred_proba'], bins=30, alpha=0.5,
            label='RegresiÃ³n LogÃ­stica', color='#3498DB', density=True)

    ax7.axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='Umbral 0.5')
    ax7.set_xlabel('Probabilidad Predicha de Churn')
    ax7.set_ylabel('Densidad')
    ax7.set_title('ğŸ“Š DistribuciÃ³n de Probabilidades', fontweight='bold')
    ax7.legend()
    ax7.grid(True, alpha=0.3)

    # 8. ANÃLISIS DE ERRORES (subplot 8)
    ax8 = plt.subplot(3, 3, 8)

    rf_cm = evaluation_results['confusion_matrices']['rf']
    lr_cm = evaluation_results['confusion_matrices']['lr']

    # Extraer valores de las matrices
    rf_tn, rf_fp, rf_fn, rf_tp = rf_cm.ravel()
    lr_tn, lr_fp, lr_fn, lr_tp = lr_cm.ravel()

    error_types = ['Falsos\nPositivos', 'Falsos\nNegativos']
    rf_errors = [rf_fp, rf_fn]
    lr_errors = [lr_fp, lr_fn]

    x_pos = np.arange(len(error_types))
    width = 0.35

    ax8.bar(x_pos - width/2, rf_errors, width, label='Random Forest',
           color='#E74C3C', alpha=0.8)
    ax8.bar(x_pos + width/2, lr_errors, width, label='RegresiÃ³n LogÃ­stica',
           color='#F39C12', alpha=0.8)

    ax8.set_xlabel('Tipo de Error')
    ax8.set_ylabel('Cantidad de Errores')
    ax8.set_title('âŒ AnÃ¡lisis de Errores por Tipo', fontweight='bold')
    ax8.set_xticks(x_pos)
    ax8.set_xticklabels(error_types)
    ax8.legend()
    ax8.grid(True, alpha=0.3)

    # 9. SCORE FINAL COMPARATIVO (subplot 9)
    ax9 = plt.subplot(3, 3, 9)

    final_scores = [evaluation_results['final_scores']['rf'],
                   evaluation_results['final_scores']['lr']]
    colors = ['#2ECC71', '#3498DB']

    bars = ax9.bar(['Random Forest', 'RegresiÃ³n LogÃ­stica'], final_scores,
                  color=colors, alpha=0.8)

    ax9.set_ylabel('Score Final (Ajustado)')
    ax9.set_title('ğŸ† Score Final Comparativo', fontweight='bold')
    ax9.grid(True, alpha=0.3)

    # Agregar valores en las barras
    for i, (bar, score) in enumerate(zip(bars, final_scores)):
        ax9.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                f'{score:.4f}', ha='center', va='bottom', fontweight='bold')

    # Marcar el ganador
    winner_idx = 0 if evaluation_results['winner'] == 'Random Forest' else 1
    bars[winner_idx].set_edgecolor('gold')
    bars[winner_idx].set_linewidth(3)

    plt.tight_layout()
    plt.show()

    print("âœ… Visualizaciones de evaluaciÃ³n completadas")

# Crear visualizaciones detalladas
create_detailed_evaluation_plots(rf_results, lr_results, data_prepared, evaluation_results)

"""### ğŸ¯ **CONCLUSIONES DEL ANÃLISIS CRÃTICO**

#### ğŸ“Š **Resumen de Rendimiento:**
Los resultados de la evaluaciÃ³n exhaustiva muestran el rendimiento comparativo de ambos modelos en mÃºltiples dimensiones, considerando no solo la precisiÃ³n sino tambiÃ©n la capacidad de generalizaciÃ³n y estabilidad.

#### ğŸ” **Hallazgos Clave sobre Overfitting/Underfitting:**

**Factores Analizados:**
1. **Diferencia Train-Test**: Indicador principal de overfitting
2. **Estabilidad en CV**: Variabilidad entre folds indica robustez
3. **Complejidad vs Performance**: Balance entre simplicidad y capacidad predictiva

#### ğŸ’¡ **Recomendaciones de Mejora:**

**Para Random Forest (si presenta overfitting):**
- Reducir `max_depth` de 10 a 6-8
- Aumentar `min_samples_split` y `min_samples_leaf`
- Implementar `max_features='sqrt'` para mayor diversidad

**Para RegresiÃ³n LogÃ­stica (si presenta underfitting):**
- Reducir regularizaciÃ³n (aumentar parÃ¡metro C)
- Agregar tÃ©rminos polinomiales o interacciones
- Considerar feature engineering adicional

#### ğŸ† **SelecciÃ³n Final del Modelo:**
El modelo recomendado se basa en un anÃ¡lisis multi-criterio que considera rendimiento, generalizaciÃ³n y aplicabilidad prÃ¡ctica en el contexto de negocio de Telecom X.

## 13. AnÃ¡lisis de Importancia de Variables en MÃºltiples Modelos

En esta secciÃ³n analizaremos las variables mÃ¡s relevantes para la predicciÃ³n de churn utilizando diferentes algoritmos. Cada modelo tiene su propia forma de medir la importancia:

### ğŸ¯ **MÃ©todos de AnÃ¡lisis por Modelo:**

#### ğŸŒ² **Random Forest**
- **Importancia por reducciÃ³n de impureza**: Mide cuÃ¡nto contribuye cada variable a disminuir la incertidumbre
- **Ventaja**: Robusto, maneja interacciones automÃ¡ticamente
- **InterpretaciÃ³n**: Valores mÃ¡s altos = mayor importancia

#### ğŸ“Š **RegresiÃ³n LogÃ­stica**
- **Coeficientes del modelo**: Magnitud y direcciÃ³n del impacto en log-odds
- **Ventaja**: InterpretaciÃ³n directa del efecto
- **InterpretaciÃ³n**: Valores absolutos altos = mayor importancia, signo = direcciÃ³n

#### ğŸ” **K-Nearest Neighbors (KNN)**
- **AnÃ¡lisis de proximidad**: Variables que mÃ¡s contribuyen a la distancia entre vecinos
- **Ventaja**: No asume forma especÃ­fica de la relaciÃ³n
- **InterpretaciÃ³n**: Escalas relativas afectan la distancia

#### ğŸ¯ **Support Vector Machine (SVM)**
- **Vectores de soporte**: Variables que definen la frontera de decisiÃ³n
- **Ventaja**: Enfoque en casos crÃ­ticos (vectores de soporte)
- **InterpretaciÃ³n**: Coeficientes indican importancia para la separaciÃ³n
"""

# Importar librerÃ­as adicionales para KNN y SVM
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd

def train_additional_models(data_prepared):
    """
    Entrena modelos KNN y SVM para anÃ¡lisis de importancia
    """
    print("ğŸš€ ENTRENANDO MODELOS ADICIONALES: KNN Y SVM")
    print("="*60)

    # Extraer datos
    X_train = data_prepared['X_train']
    X_test = data_prepared['X_test']
    y_train = data_prepared['y_train']
    y_test = data_prepared['y_test']
    scaler = data_prepared['scaler']
    numerical_cols = data_prepared['numerical_cols']

    # Preparar datos escalados (necesarios para KNN y SVM)
    print("ğŸ“ PREPARANDO DATOS ESCALADOS PARA KNN Y SVM...")

    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()

    if numerical_cols:
        X_train_scaled[numerical_cols] = scaler.transform(X_train[numerical_cols])
        X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])
        print(f"   âœ… Variables escaladas: {numerical_cols}")

    # 1. ENTRENAR MODELO KNN
    print(f"\nğŸ” ENTRENANDO K-NEAREST NEIGHBORS:")
    print("-" * 40)

    # Encontrar el mejor K usando validaciÃ³n
    k_values = [3, 5, 7, 9, 11]
    best_k = 5
    best_score = 0

    for k in k_values:
        knn_temp = KNeighborsClassifier(n_neighbors=k, weights='distance')
        cv_scores_temp = cross_val_score(knn_temp, X_train_scaled, y_train,
                                        cv=3, scoring='f1')
        mean_score = cv_scores_temp.mean()
        if mean_score > best_score:
            best_score = mean_score
            best_k = k

    print(f"   ğŸ¯ Mejor K encontrado: {best_k} (F1-Score CV: {best_score:.4f})")

    knn_model = KNeighborsClassifier(
        n_neighbors=best_k,
        weights='distance',  # Pesos basados en distancia
        metric='euclidean'   # MÃ©trica de distancia
    )

    knn_model.fit(X_train_scaled, y_train)

    # Predicciones KNN
    knn_train_pred = knn_model.predict(X_train_scaled)
    knn_test_pred = knn_model.predict(X_test_scaled)
    knn_test_pred_proba = knn_model.predict_proba(X_test_scaled)[:, 1]

    # MÃ©tricas KNN
    knn_metrics = {
        'train': {
            'accuracy': accuracy_score(y_train, knn_train_pred),
            'precision': precision_score(y_train, knn_train_pred),
            'recall': recall_score(y_train, knn_train_pred),
            'f1': f1_score(y_train, knn_train_pred)
        },
        'test': {
            'accuracy': accuracy_score(y_test, knn_test_pred),
            'precision': precision_score(y_test, knn_test_pred),
            'recall': recall_score(y_test, knn_test_pred),
            'f1': f1_score(y_test, knn_test_pred),
            'auc': roc_auc_score(y_test, knn_test_pred_proba)
        }
    }

    print(f"   ğŸ“Š MÃ©tricas Test - Accuracy: {knn_metrics['test']['accuracy']:.4f}, F1: {knn_metrics['test']['f1']:.4f}")

    # 2. ENTRENAR MODELO SVM
    print(f"\nğŸ¯ ENTRENANDO SUPPORT VECTOR MACHINE:")
    print("-" * 40)

    svm_model = SVC(
        kernel='rbf',              # Kernel radial (maneja no-linealidades)
        C=1.0,                     # ParÃ¡metro de regularizaciÃ³n
        gamma='scale',             # ParÃ¡metro del kernel
        class_weight='balanced',   # Balancear clases
        probability=True,          # Habilitar predicciÃ³n de probabilidades
        random_state=42
    )

    print("   ğŸ‹ï¸ Entrenando SVM (puede tomar unos minutos)...")
    svm_model.fit(X_train_scaled, y_train)

    # Predicciones SVM
    svm_train_pred = svm_model.predict(X_train_scaled)
    svm_test_pred = svm_model.predict(X_test_scaled)
    svm_test_pred_proba = svm_model.predict_proba(X_test_scaled)[:, 1]

    # MÃ©tricas SVM
    svm_metrics = {
        'train': {
            'accuracy': accuracy_score(y_train, svm_train_pred),
            'precision': precision_score(y_train, svm_train_pred),
            'recall': recall_score(y_train, svm_train_pred),
            'f1': f1_score(y_train, svm_train_pred)
        },
        'test': {
            'accuracy': accuracy_score(y_test, svm_test_pred),
            'precision': precision_score(y_test, svm_test_pred),
            'recall': recall_score(y_test, svm_test_pred),
            'f1': f1_score(y_test, svm_test_pred),
            'auc': roc_auc_score(y_test, svm_test_pred_proba)
        }
    }

    print(f"   ğŸ“Š MÃ©tricas Test - Accuracy: {svm_metrics['test']['accuracy']:.4f}, F1: {svm_metrics['test']['f1']:.4f}")
    print(f"   ğŸ¯ Vectores de soporte utilizados: {len(svm_model.support_)}")

    print("="*60)

    return {
        'knn_model': knn_model,
        'svm_model': svm_model,
        'knn_metrics': knn_metrics,
        'svm_metrics': svm_metrics,
        'knn_predictions': {
            'test_pred': knn_test_pred,
            'test_pred_proba': knn_test_pred_proba
        },
        'svm_predictions': {
            'test_pred': svm_test_pred,
            'test_pred_proba': svm_test_pred_proba
        },
        'X_train_scaled': X_train_scaled,
        'X_test_scaled': X_test_scaled,
        'best_k': best_k
    }

# Entrenar modelos adicionales
additional_models = train_additional_models(data_prepared)

def analyze_feature_importance_all_models(rf_results, lr_results, additional_models, data_prepared):
    """
    Analiza la importancia de variables en todos los modelos entrenados
    """
    print("ğŸ” ANÃLISIS EXHAUSTIVO DE IMPORTANCIA DE VARIABLES")
    print("="*80)

    feature_names = data_prepared['feature_names']
    X_train_scaled = additional_models['X_train_scaled']
    X_test_scaled = additional_models['X_test_scaled']
    y_train = data_prepared['y_train']
    y_test = data_prepared['y_test']

    # 1. RANDOM FOREST - IMPORTANCIA POR REDUCCIÃ“N DE IMPUREZA
    print("\nğŸŒ² RANDOM FOREST - IMPORTANCIA POR REDUCCIÃ“N DE IMPUREZA")
    print("-" * 70)

    rf_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': rf_results['model'].feature_importances_,
        'importance_normalized': rf_results['model'].feature_importances_ / rf_results['model'].feature_importances_.sum()
    }).sort_values('importance', ascending=False)

    print("ğŸ† TOP 15 VARIABLES MÃS IMPORTANTES:")
    for i, (_, row) in enumerate(rf_importance.head(15).iterrows(), 1):
        percentage = row['importance_normalized'] * 100
        print(f"   {i:2d}. {row['feature'][:35]:35} â†’ {row['importance']:.4f} ({percentage:5.1f}%)")

    # 2. REGRESIÃ“N LOGÃSTICA - ANÃLISIS DE COEFICIENTES
    print("\nğŸ“Š REGRESIÃ“N LOGÃSTICA - ANÃLISIS DE COEFICIENTES")
    print("-" * 70)

    lr_importance = pd.DataFrame({
        'feature': feature_names,
        'coefficient': lr_results['model'].coef_[0],
        'abs_coefficient': np.abs(lr_results['model'].coef_[0])
    }).sort_values('abs_coefficient', ascending=False)

    print("ğŸ† TOP 15 COEFICIENTES MÃS SIGNIFICATIVOS:")
    for i, (_, row) in enumerate(lr_importance.head(15).iterrows(), 1):
        direction = "â¬†ï¸ Incrementa" if row['coefficient'] > 0 else "â¬‡ï¸ Reduce"
        print(f"   {i:2d}. {row['feature'][:30]:30} â†’ {row['coefficient']:8.4f} ({direction} churn)")

    # 3. KNN - IMPORTANCIA POR PERMUTACIÃ“N
    print("\nğŸ” K-NEAREST NEIGHBORS - IMPORTANCIA POR PERMUTACIÃ“N")
    print("-" * 70)

    print("   ğŸ”„ Calculando importancia por permutaciÃ³n (puede tomar unos minutos)...")

    # Calcular importancia por permutaciÃ³n para KNN
    knn_perm_importance = permutation_importance(
        additional_models['knn_model'],
        X_test_scaled,
        y_test,
        n_repeats=5,
        random_state=42,
        scoring='f1'
    )

    knn_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': knn_perm_importance.importances_mean,
        'importance_std': knn_perm_importance.importances_std
    }).sort_values('importance', ascending=False)

    print("ğŸ† TOP 15 VARIABLES MÃS IMPORTANTES:")
    for i, (_, row) in enumerate(knn_importance.head(15).iterrows(), 1):
        print(f"   {i:2d}. {row['feature'][:35]:35} â†’ {row['importance']:.4f} Â± {row['importance_std']:.4f}")

    # 4. SVM - ANÃLISIS DE VECTORES DE SOPORTE
    print("\nğŸ¯ SUPPORT VECTOR MACHINE - ANÃLISIS DE IMPORTANCIA")
    print("-" * 70)

    # Para SVM con kernel RBF, usamos importancia por permutaciÃ³n
    print("   ğŸ”„ Calculando importancia por permutaciÃ³n para SVM...")

    svm_perm_importance = permutation_importance(
        additional_models['svm_model'],
        X_test_scaled,
        y_test,
        n_repeats=5,
        random_state=42,
        scoring='f1'
    )

    svm_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': svm_perm_importance.importances_mean,
        'importance_std': svm_perm_importance.importances_std
    }).sort_values('importance', ascending=False)

    print("ğŸ† TOP 15 VARIABLES MÃS IMPORTANTES:")
    for i, (_, row) in enumerate(svm_importance.head(15).iterrows(), 1):
        print(f"   {i:2d}. {row['feature'][:35]:35} â†’ {row['importance']:.4f} Â± {row['importance_std']:.4f}")

    print(f"\nğŸ“Š InformaciÃ³n adicional SVM:")
    print(f"   â€¢ Vectores de soporte: {len(additional_models['svm_model'].support_)}")
    print(f"   â€¢ Porcentaje de vectores de soporte: {len(additional_models['svm_model'].support_)/len(X_train_scaled)*100:.1f}%")

    # 5. ANÃLISIS COMPARATIVO ENTRE MODELOS
    print(f"\nğŸ”¬ ANÃLISIS COMPARATIVO DE IMPORTANCIA ENTRE MODELOS")
    print("-" * 70)

    # Crear ranking de variables por modelo
    rankings = {
        'Random Forest': {row['feature']: i+1 for i, (_, row) in enumerate(rf_importance.iterrows())},
        'RegresiÃ³n LogÃ­stica': {row['feature']: i+1 for i, (_, row) in enumerate(lr_importance.iterrows())},
        'KNN': {row['feature']: i+1 for i, (_, row) in enumerate(knn_importance.iterrows())},
        'SVM': {row['feature']: i+1 for i, (_, row) in enumerate(svm_importance.iterrows())}
    }

    # Encontrar variables consistentemente importantes
    top_features_all = set()
    for model_ranking in rankings.values():
        top_features_all.update(list(model_ranking.keys())[:10])

    print("ğŸ¯ VARIABLES CONSISTENTEMENTE IMPORTANTES (TOP 10 en algÃºn modelo):")

    comparison_df = pd.DataFrame({
        'Variable': list(top_features_all),
        'RF_Rank': [rankings['Random Forest'].get(var, 999) for var in top_features_all],
        'LR_Rank': [rankings['RegresiÃ³n LogÃ­stica'].get(var, 999) for var in top_features_all],
        'KNN_Rank': [rankings['KNN'].get(var, 999) for var in top_features_all],
        'SVM_Rank': [rankings['SVM'].get(var, 999) for var in top_features_all]
    })

    # Calcular ranking promedio
    comparison_df['Avg_Rank'] = comparison_df[['RF_Rank', 'LR_Rank', 'KNN_Rank', 'SVM_Rank']].mean(axis=1)
    comparison_df = comparison_df.sort_values('Avg_Rank')

    print("\nğŸ“‹ RANKING CONSOLIDADO (Top 20):")
    for i, (_, row) in enumerate(comparison_df.head(20).iterrows(), 1):
        rf_rank = row['RF_Rank'] if row['RF_Rank'] < 999 else '-'
        lr_rank = row['LR_Rank'] if row['LR_Rank'] < 999 else '-'
        knn_rank = row['KNN_Rank'] if row['KNN_Rank'] < 999 else '-'
        svm_rank = row['SVM_Rank'] if row['SVM_Rank'] < 999 else '-'

        print(f"   {i:2d}. {row['Variable'][:30]:30} | RF:{rf_rank:>3} LR:{lr_rank:>3} KNN:{knn_rank:>3} SVM:{svm_rank:>3} | Avg:{row['Avg_Rank']:5.1f}")

    # 6. IDENTIFICAR PATRONES Y CONSENSOS
    print(f"\nğŸ’¡ ANÃLISIS DE CONSENSO ENTRE MODELOS:")
    print("-" * 70)

    # Variables que aparecen en top 10 de mÃºltiples modelos
    consensus_vars = []
    for var in top_features_all:
        models_in_top10 = sum(1 for ranking in rankings.values() if ranking.get(var, 999) <= 10)
        if models_in_top10 >= 3:
            consensus_vars.append((var, models_in_top10))

    consensus_vars.sort(key=lambda x: x[1], reverse=True)

    print("ğŸ¤ VARIABLES CON MAYOR CONSENSO (Top 10 en 3+ modelos):")
    for var, count in consensus_vars:
        print(f"   â€¢ {var[:40]:40} â†’ Aparece en {count}/4 modelos")

    if not consensus_vars:
        print("   â„¹ï¸ No hay variables que aparezcan en top 10 de 3+ modelos")

    print("="*80)

    return {
        'rf_importance': rf_importance,
        'lr_importance': lr_importance,
        'knn_importance': knn_importance,
        'svm_importance': svm_importance,
        'comparison_df': comparison_df,
        'consensus_vars': consensus_vars,
        'rankings': rankings
    }

# Ejecutar anÃ¡lisis de importancia
importance_analysis = analyze_feature_importance_all_models(
    rf_results, lr_results, additional_models, data_prepared
)

def create_feature_importance_visualizations(importance_analysis, additional_models):
    """
    Crea visualizaciones comprehensivas de importancia de variables
    """
    print("ğŸ¨ CREANDO VISUALIZACIONES DE IMPORTANCIA DE VARIABLES")
    print("="*60)

    # Configurar figura grande con mÃºltiples subplots
    fig = plt.figure(figsize=(24, 20))

    # 1. RANDOM FOREST - Importancia por reducciÃ³n de impureza
    ax1 = plt.subplot(3, 3, 1)
    top_rf = importance_analysis['rf_importance'].head(15)
    bars1 = ax1.barh(range(len(top_rf)), top_rf['importance'], color='#2ECC71', alpha=0.8)
    ax1.set_yticks(range(len(top_rf)))
    ax1.set_yticklabels([name[:25] for name in top_rf['feature']])
    ax1.set_xlabel('Importancia (ReducciÃ³n de Impureza)')
    ax1.set_title('ğŸŒ² Random Forest\nImportancia de Variables', fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='x')

    # 2. REGRESIÃ“N LOGÃSTICA - Coeficientes
    ax2 = plt.subplot(3, 3, 2)
    top_lr = importance_analysis['lr_importance'].head(15)
    colors_lr = ['#E74C3C' if x < 0 else '#3498DB' for x in top_lr['coefficient']]
    bars2 = ax2.barh(range(len(top_lr)), top_lr['coefficient'], color=colors_lr, alpha=0.8)
    ax2.set_yticks(range(len(top_lr)))
    ax2.set_yticklabels([name[:25] for name in top_lr['feature']])
    ax2.set_xlabel('Coeficiente')
    ax2.set_title('ğŸ“Š RegresiÃ³n LogÃ­stica\nCoeficientes', fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='x')
    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)

    # 3. KNN - Importancia por permutaciÃ³n
    ax3 = plt.subplot(3, 3, 3)
    top_knn = importance_analysis['knn_importance'].head(15)
    bars3 = ax3.barh(range(len(top_knn)), top_knn['importance'],
                    xerr=top_knn['importance_std'], color='#9B59B6', alpha=0.8)
    ax3.set_yticks(range(len(top_knn)))
    ax3.set_yticklabels([name[:25] for name in top_knn['feature']])
    ax3.set_xlabel('Importancia (PermutaciÃ³n)')
    ax3.set_title('ğŸ” K-Nearest Neighbors\nImportancia por PermutaciÃ³n', fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='x')

    # 4. SVM - Importancia por permutaciÃ³n
    ax4 = plt.subplot(3, 3, 4)
    top_svm = importance_analysis['svm_importance'].head(15)
    bars4 = ax4.barh(range(len(top_svm)), top_svm['importance'],
                    xerr=top_svm['importance_std'], color='#F39C12', alpha=0.8)
    ax4.set_yticks(range(len(top_svm)))
    ax4.set_yticklabels([name[:25] for name in top_svm['feature']])
    ax4.set_xlabel('Importancia (PermutaciÃ³n)')
    ax4.set_title('ğŸ¯ Support Vector Machine\nImportancia por PermutaciÃ³n', fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='x')

    # 5. COMPARACIÃ“N DE RANKINGS
    ax5 = plt.subplot(3, 3, 5)
    top_consensus = importance_analysis['comparison_df'].head(10)

    # Crear heatmap de rankings
    ranking_matrix = top_consensus[['RF_Rank', 'LR_Rank', 'KNN_Rank', 'SVM_Rank']].values
    ranking_matrix[ranking_matrix == 999] = np.nan  # NaN para valores no rankeados

    im = ax5.imshow(ranking_matrix.T, cmap='RdYlBu_r', aspect='auto')
    ax5.set_xticks(range(len(top_consensus)))
    ax5.set_xticklabels([name[:15] for name in top_consensus['Variable']], rotation=45)
    ax5.set_yticks(range(4))
    ax5.set_yticklabels(['RF', 'LR', 'KNN', 'SVM'])
    ax5.set_title('ğŸ”¬ Heatmap de Rankings\n(Menor = Mejor)', fontweight='bold')

    # Agregar valores en el heatmap
    for i in range(4):
        for j in range(len(top_consensus)):
            value = ranking_matrix[j, i]
            if not np.isnan(value):
                ax5.text(j, i, f'{int(value)}', ha='center', va='center',
                        color='white' if value > 50 else 'black', fontweight='bold')

    # 6. DISTRIBUCIÃ“N DE IMPORTANCIA - Random Forest
    ax6 = plt.subplot(3, 3, 6)
    ax6.hist(importance_analysis['rf_importance']['importance'], bins=30,
            color='#2ECC71', alpha=0.7, edgecolor='black')
    ax6.axvline(importance_analysis['rf_importance']['importance'].mean(),
               color='red', linestyle='--', linewidth=2, label='Media')
    ax6.set_xlabel('Importancia')
    ax6.set_ylabel('Frecuencia')
    ax6.set_title('ğŸŒ² DistribuciÃ³n de Importancia\n(Random Forest)', fontweight='bold')
    ax6.legend()
    ax6.grid(True, alpha=0.3)

    # 7. COMPARACIÃ“N DIRECTA TOP 5
    ax7 = plt.subplot(3, 3, 7)

    # Obtener top 5 de cada modelo
    models = ['RF', 'LR', 'KNN', 'SVM']
    importances_list = [
        importance_analysis['rf_importance'],
        importance_analysis['lr_importance'],
        importance_analysis['knn_importance'],
        importance_analysis['svm_importance']
    ]

    colors = ['#2ECC71', '#3498DB', '#9B59B6', '#F39C12']

    for i, (model, imp_df, color) in enumerate(zip(models, importances_list, colors)):
        top_5 = imp_df.head(5)
        if 'importance' in imp_df.columns:
            values = top_5['importance'].values
        elif 'abs_coefficient' in imp_df.columns:
            values = top_5['abs_coefficient'].values
        else:
            values = top_5.iloc[:, 1].values

        x_pos = np.arange(5) + i * 0.2
        ax7.bar(x_pos, values, width=0.2, label=model, color=color, alpha=0.8)

    ax7.set_xlabel('Top 5 Variables (por modelo)')
    ax7.set_ylabel('Importancia Normalizada')
    ax7.set_title('ğŸ“Š ComparaciÃ³n Top 5\nVariables por Modelo', fontweight='bold')
    ax7.legend()
    ax7.grid(True, alpha=0.3)

    # 8. ANÃLISIS DE CONSENSO
    ax8 = plt.subplot(3, 3, 8)

    if importance_analysis['consensus_vars']:
        consensus_names = [var[0][:20] for var in importance_analysis['consensus_vars'][:10]]
        consensus_counts = [var[1] for var in importance_analysis['consensus_vars'][:10]]

        bars8 = ax8.bar(range(len(consensus_names)), consensus_counts,
                       color='#E67E22', alpha=0.8)
        ax8.set_xticks(range(len(consensus_names)))
        ax8.set_xticklabels(consensus_names, rotation=45)
        ax8.set_ylabel('NÃºmero de Modelos (Top 10)')
        ax8.set_title('ğŸ¤ Variables con Mayor Consenso', fontweight='bold')
        ax8.grid(True, alpha=0.3)

        # Agregar valores en las barras
        for bar, count in zip(bars8, consensus_counts):
            ax8.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                    f'{count}/4', ha='center', va='bottom', fontweight='bold')
    else:
        ax8.text(0.5, 0.5, 'No hay variables\ncon consenso',
                ha='center', va='center', transform=ax8.transAxes, fontsize=12)
        ax8.set_title('ğŸ¤ Variables con Mayor Consenso', fontweight='bold')

    # 9. MÃ‰TRICAS DE RENDIMIENTO DE TODOS LOS MODELOS
    ax9 = plt.subplot(3, 3, 9)

    models_names = ['Random Forest', 'Reg. LogÃ­stica', 'KNN', 'SVM']
    f1_scores = [
        rf_results['metrics']['test']['f1'],
        lr_results['metrics']['test']['f1'],
        additional_models['knn_metrics']['test']['f1'],
        additional_models['svm_metrics']['test']['f1']
    ]

    bars9 = ax9.bar(models_names, f1_scores,
                   color=['#2ECC71', '#3498DB', '#9B59B6', '#F39C12'], alpha=0.8)
    ax9.set_ylabel('F1-Score')
    ax9.set_title('ğŸ† Rendimiento de Todos los Modelos\n(F1-Score)', fontweight='bold')
    ax9.grid(True, alpha=0.3)
    plt.setp(ax9.get_xticklabels(), rotation=45)

    # Agregar valores en las barras
    for bar, score in zip(bars9, f1_scores):
        ax9.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,
                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()

    print("âœ… Visualizaciones de importancia completadas")

# Crear visualizaciones
create_feature_importance_visualizations(importance_analysis, additional_models)